# python/main_train.py

import logging
import random
from tqdm import tqdm
import numpy as np
from typing import Dict, Any, List, Tuple

# Imports tá»« cÃ¡c module Ä‘Ã£ hoÃ n thiá»‡n
from python.utils.db_connector import MongoConnector
from python.utils.state_builder import StateBuilder
from python.env.satellite_simulator import SatelliteEnv
from python.rl_agent.trainer import DQNAgent, TARGET_UPDATE_INTERVAL
from python.rl_agent.policy import get_epsilon # Cáº§n cho logging

# --- Cáº¤U HÃŒNH VÃ€ Háº°NG Sá» ---
NUM_EPISODES = 1000
MAX_HOPS_PER_EPISODE = 50 # Giá»›i háº¡n vÃ²ng láº·p mÃ´ phá»ng
CHECKPOINT_PATH = "models/checkpoints/dqn_checkpoint_fullpath.pth"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ----------------- MOCK PACKET GENERATOR -----------------

def generate_packet(node_list: List[str]) -> Dict[str, Any]:
    """Táº¡o packet tá»« 1 node ngáº«u nhiÃªn Ä‘áº¿n 1 destination khÃ¡c."""
    
    if len(node_list) < 2:
        raise ValueError("Cannot generate packet: Need at least two nodes.")
        
    src = random.choice(node_list)
    dest = random.choice([n for n in node_list if n != src])
    
    packet = {
        "currentHoldingNodeId": src,
        "stationDest": dest,
        "accumulatedDelayMs": 0.0,
        "ttl": random.randint(35, 45),
        "serviceQoS": {
            "serviceType": random.choice(["VIDEO_STREAM", "AUDIO_CALL", "FILE_TRANSFER"]),
            "maxLatencyMs": random.uniform(100.0, 300.0),
            "minBandwidthMbps": random.uniform(2.0, 10.0),
            "maxLossRate": random.uniform(0.01, 0.05)
        },
        "dropped": False,
        "path": [src]
    }
    return packet

def simulate_full_path(
    env: SatelliteEnv,
    agent: DQNAgent,
    state_builder: StateBuilder,
    packet: Dict[str, Any],
    max_hops: int = MAX_HOPS_PER_EPISODE
) -> Tuple[List[Tuple], float]:
    """
    MÃ´ phá»ng hÃ nh trÃ¬nh cá»§a 1 packet qua nhiá»u hop Ä‘áº¿n Ä‘Ã­ch.
    Tráº£ vá» (transitions, total_reward)
    """
    state = env.reset(packet)
    transitions = []
    total_reward = 0.0
    hops = 0

    current_packet = packet.copy()

    while True:
        current_node_id = current_packet["currentHoldingNodeId"]
        dest_node_id = current_packet["stationDest"]

        # ---- Äiá»u kiá»‡n káº¿t thÃºc ----
        if (
            current_packet.get("dropped")
            or current_packet.get("ttl", 0) <= 0
            or current_node_id == dest_node_id
            or hops >= max_hops
        ):
            done = True
            # Pháº¡t nháº¹ náº¿u TTL cáº¡n hoáº·c bá»‹ drop
            if current_packet.get("dropped", False):
                total_reward += -150.0
            elif current_packet.get("ttl", 0) <= 0:
                total_reward += -50.0
            elif current_node_id == dest_node_id:
                total_reward += 200.0  # ThÆ°á»Ÿng Ä‘áº¿n Ä‘Ã­ch
            break

        # ---- Láº¥y neighbors hiá»‡n táº¡i ----
        node_data = state_builder.db.get_node(current_node_id, projection={"neighbors": 1})
        neighbor_ids = node_data.get("neighbors", []) if node_data else []

        if not neighbor_ids:
            current_packet["dropped"] = True
            continue

        # ---- Agent chá»n hÃ nh Ä‘á»™ng ----
        action_index = agent.select_action(state)

        # Xá»­ lÃ½ náº¿u action_index vÆ°á»£t quÃ¡ sá»‘ neighbor thá»±c táº¿
        if action_index < len(neighbor_ids):
            next_hop_id = neighbor_ids[action_index]
        else:
            next_hop_id = random.choice(neighbor_ids)

        # ---- MÃ´ phá»ng chuyá»ƒn tiáº¿p ----
        next_packet = current_packet.copy()
        next_packet["currentHoldingNodeId"] = next_hop_id
        next_packet["ttl"] = max(current_packet.get("ttl", 10) - 1, 0)
        next_packet["accumulatedDelayMs"] += random.uniform(5.0, 20.0)
        next_packet["path"] = current_packet["path"] + [next_hop_id]

        # ---- Step trong mÃ´i trÆ°á»ng ----
        next_state, reward, done = env.step(action_index, next_hop_id, next_packet)

        # ---- Ghi nháº­n ----
        total_reward += reward
        transitions.append((state, action_index, reward, next_state, done))

        # ---- Cáº­p nháº­t cho vÃ²ng tiáº¿p theo ----
        state = next_state
        current_packet = next_packet
        hops += 1

        if done:
            break

    # ---- Logging chi tiáº¿t ----
    logger.info(
        f"[Episode Path] {packet['path'][0]} â†’ {current_packet['currentHoldingNodeId']} "
        f"| TotalReward={total_reward:.2f} | Hops={hops} | TTL={current_packet.get('ttl',0)}"
    )

    return transitions, total_reward

# ----------------- TRAINING LOOP -----------------

def train_agent():
    logger.info("=== KHá»I Táº O Há»† THá»NG DQN ROUTER FULLPATH ===")
    mongo_conn = MongoConnector(uri="mongodb://user:password123@localhost:27017/sagsin_network?authSource=admin")
    state_builder = StateBuilder(mongo_conn)

    # Weights cho Reward (ÄÃ£ thÃªm hop_cost Ä‘á»ƒ giáº£i quyáº¿t lá»—i lang thang)
    reward_weights = {
        'goal': 200.0,
        'drop': -150.0,
        'latency': -10.0,
        'latency_violation': -50.0,
        'utilization': 2.0,
        'bandwidth': 1.0,
        'reliability': 3.0,
        'fspl': -0.1,
        'hop_cost': -1.0 # ğŸ’¡ PHáº T Má»šI
    }
    env = SatelliteEnv(state_builder, weights=reward_weights)
    agent = DQNAgent(env)
    
    # Sá»¬A Lá»–I: Láº¥y táº¥t cáº£ Node ID cho generator
    all_nodes_data = state_builder.db.get_all_nodes(projection={"nodeId": 1})
    all_nodes = [n["nodeId"] for n in all_nodes_data]

    if len(all_nodes) < 2:
        logger.error("KhÃ´ng Ä‘á»§ Node Ä‘á»ƒ huáº¥n luyá»‡n. Vui lÃ²ng kiá»ƒm tra MongoDB.")
        return

    pbar = tqdm(range(NUM_EPISODES), desc="DQN Fullpath Training")
    for episode in pbar:
        packet = generate_packet(all_nodes)
        
        # Simulate full path and get all transitions
        transitions, episode_reward = simulate_full_path(env, agent, state_builder, packet)

        # LÆ°u transitions vÃ o replay buffer vÃ  tá»‘i Æ°u hÃ³a
        for s, a, r, s_next, done in transitions:
            if a is not None:
                agent.memory.push(s, a, r, s_next, done)
                agent.optimize_model()

        # Cáº­p nháº­t Target Network
        if episode % TARGET_UPDATE_INTERVAL == 0:
            agent.update_target_network()

        # Logging vÃ  Checkpoint
        epsilon = get_epsilon(agent.steps_done)
        pbar.set_postfix({'Reward': f"{episode_reward:.2f}", 'Hops': len(transitions), 'Epsilon': f"{epsilon:.4f}"})

        if (episode + 1) % 100 == 0:
            agent.save_checkpoint(CHECKPOINT_PATH.replace(".pth", f"_ep{episode+1}.pth"))

    agent.save_checkpoint(CHECKPOINT_PATH.replace(".pth", "_final.pth"))
    logger.info("=== HUáº¤N LUYá»†N DQN FULLPATH HOÃ€N Táº¤T ===")

if __name__ == "__main__":
    train_agent()
# python/main_train.py

import logging
import random
from tqdm import tqdm
import numpy as np
from typing import Dict, Any, List, Tuple

# Imports tá»« cÃ¡c module Ä‘Ã£ hoÃ n thiá»‡n
from python.utils.db_connector import MongoConnector
from python.utils.state_builder import StateBuilder
from python.env.satellite_simulator import SatelliteEnv
from python.rl_agent.trainer import DQNAgent, TARGET_UPDATE
from python.rl_agent.policy import get_epsilon # Cáº§n cho logging

# --- Cáº¤U HÃŒNH VÃ€ Háº°NG Sá» ---
NUM_EPISODES = 1000
MAX_HOPS_PER_EPISODE = 50 # Giá»›i háº¡n vÃ²ng láº·p mÃ´ phá»ng
CHECKPOINT_PATH = "models/checkpoints/dqn_checkpoint_fullpath.pth"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ----------------- MOCK PACKET GENERATOR -----------------

def generate_packet(node_list: List[str]) -> Dict[str, Any]:
    """Táº¡o packet tá»« 1 node ngáº«u nhiÃªn Ä‘áº¿n 1 destination khÃ¡c."""
    
    if len(node_list) < 2:
        raise ValueError("Cannot generate packet: Need at least two nodes.")
        
    src = random.choice(node_list)
    dest = random.choice([n for n in node_list if n != src])
    
    packet = {
        "currentHoldingNodeId": src,
        "stationDest": dest,
        "accumulatedDelayMs": 0.0,
        "ttl": random.randint(15, 25),
        "serviceQoS": {
            "serviceType": random.choice(["VIDEO_STREAM", "AUDIO_CALL", "FILE_TRANSFER"]),
            "maxLatencyMs": random.uniform(100.0, 300.0),
            "minBandwidthMbps": random.uniform(2.0, 10.0),
            "maxLossRate": random.uniform(0.01, 0.05)
        },
        "dropped": False,
        "path": [src]
    }
    return packet

def simulate_full_path(env: SatelliteEnv, agent: DQNAgent, state_builder: StateBuilder, packet: Dict[str, Any]) -> Tuple[List[Tuple], float]:
    """
    MÃ´ phá»ng hÃ nh trÃ¬nh packet Ä‘áº§y Ä‘á»§ (nhiá»u hop) cho má»™t Episode.
    Tráº£ vá» danh sÃ¡ch transitions vÃ  tá»•ng reward.
    """
    state = env.reset(packet)
    transitions = []
    total_reward = 0
    hops = 0
    current_packet = packet.copy()

    while hops < MAX_HOPS_PER_EPISODE:
        current_node_id = current_packet["currentHoldingNodeId"]
        
        # Láº¥y thÃ´ng tin neighbors cho Agent (dÃ¹ Agent dÃ¹ng Q-Network) vÃ  cho mÃ´ phá»ng
        current_node_data = state_builder.db.get_node(current_node_id, projection={"neighbors": 1})
        neighbor_ids = current_node_data.get("neighbors", []) if current_node_data else []

        # Kiá»ƒm tra Ä‘iá»u kiá»‡n káº¿t thÃºc sá»›m
        if current_packet.get("dropped") or current_packet.get("ttl", 0) <= 0 or current_node_id == current_packet.get("stationDest"):
            # LÆ°u transition cuá»‘i (náº¿u cÃ³) vÃ  káº¿t thÃºc
            if hops > 0:
                 transitions.append((state, None, 0.0, state, True)) # Transition cuá»‘i giáº£ Ä‘á»‹nh
            break

        # 1. Chá»n HÃ nh Ä‘á»™ng (Action Selection)
        action_index = agent.select_action(state)
        
        # Xá»­ lÃ½ trÆ°á»ng há»£p khÃ´ng cÃ³ neighbors
        if not neighbor_ids:
            current_packet["dropped"] = True
            continue # Láº·p láº¡i vÃ  sáº½ bá»‹ báº¯t á»Ÿ kiá»ƒm tra Ä‘iá»u kiá»‡n káº¿t thÃºc

        # 2. Ãnh xáº¡ HÃ nh Ä‘á»™ng sang Node ID
        if action_index < len(neighbor_ids):
            next_hop_id = neighbor_ids[action_index]
        else:
            # Náº¿u Agent chá»n index ngoÃ i pháº¡m vi (padding), chá»n ngáº«u nhiÃªn neighbor há»£p lá»‡
            next_hop_id = random.choice(neighbor_ids) 
            
        # 3. Cáº­p nháº­t packet mÃ´ phá»ng hop tiáº¿p theo (MÃ´i trÆ°á»ng)
        next_packet = current_packet.copy()
        next_packet["currentHoldingNodeId"] = next_hop_id
        next_packet["ttl"] = max(current_packet.get("ttl", 10) - 1, 0)
        
        # Giáº£ láº­p tÄƒng Ä‘á»™ trá»… (sá»­ dá»¥ng random)
        next_packet["accumulatedDelayMs"] += random.uniform(5.0, 20.0) 
        next_packet["path"] = current_packet["path"] + [next_hop_id]

        # 4. Step trong mÃ´i trÆ°á»ng
        # HÃ m env.step sáº½ tÃ­nh Reward dá»±a trÃªn tráº¡ng thÃ¡i cÅ© (current_packet) vÃ  tráº¡ng thÃ¡i má»›i (next_packet)
        next_state, reward, done = env.step(action_index, next_hop_id, next_packet)
        total_reward += reward
        
        # 5. LÆ°u trá»¯ Transition
        transitions.append((state, action_index, reward, next_state, done))

        current_packet = next_packet
        state = next_state
        hops += 1

        if done:
            break

    return transitions, total_reward

# ----------------- TRAINING LOOP -----------------

def train_agent():
    logger.info("=== KHá»I Táº O Há»† THá»NG DQN ROUTER FULLPATH ===")
    mongo_conn = MongoConnector(uri="mongodb://user:password123@localhost:27017/sagsin_network?authSource=admin")
    state_builder = StateBuilder(mongo_conn)

    # Weights cho Reward (ÄÃ£ thÃªm hop_cost Ä‘á»ƒ giáº£i quyáº¿t lá»—i lang thang)
    reward_weights = {
        'goal': 200.0,
        'drop': -150.0,
        'latency': -10.0,
        'latency_violation': -50.0,
        'utilization': 2.0,
        'bandwidth': 1.0,
        'reliability': 3.0,
        'fspl': -0.1,
        'hop_cost': -1.0 # ğŸ’¡ PHáº T Má»šI
    }
    env = SatelliteEnv(state_builder, weights=reward_weights)
    agent = DQNAgent(env)
    
    # Sá»¬A Lá»–I: Láº¥y táº¥t cáº£ Node ID cho generator
    all_nodes_data = state_builder.db.get_all_nodes(projection={"nodeId": 1})
    all_nodes = [n["nodeId"] for n in all_nodes_data]

    if len(all_nodes) < 2:
        logger.error("KhÃ´ng Ä‘á»§ Node Ä‘á»ƒ huáº¥n luyá»‡n. Vui lÃ²ng kiá»ƒm tra MongoDB.")
        return

    pbar = tqdm(range(NUM_EPISODES), desc="DQN Fullpath Training")
    for episode in pbar:
        packet = generate_packet(all_nodes)
        
        # Simulate full path and get all transitions
        transitions, episode_reward = simulate_full_path(env, agent, state_builder, packet)

        # LÆ°u transitions vÃ o replay buffer vÃ  tá»‘i Æ°u hÃ³a
        for s, a, r, s_next, done in transitions:
            if a is not None:
                agent.memory.push(s, a, r, s_next, done)
                agent.optimize_model()

        # Cáº­p nháº­t Target Network
        if episode % TARGET_UPDATE == 0:
            agent.update_target_network()

        # Logging vÃ  Checkpoint
        epsilon = get_epsilon(agent.steps_done)
        pbar.set_postfix({'Reward': f"{episode_reward:.2f}", 'Hops': len(transitions), 'Epsilon': f"{epsilon:.4f}"})

        if (episode + 1) % 100 == 0:
            agent.save_checkpoint(CHECKPOINT_PATH.replace(".pth", f"_ep{episode+1}.pth"))

    agent.save_checkpoint(CHECKPOINT_PATH.replace(".pth", "_final.pth"))
    logger.info("=== HUáº¤N LUYá»†N DQN FULLPATH HOÃ€N Táº¤T ===")

if __name__ == "__main__":
    train_agent()
# env/RewardCalculator.py
from typing import Dict, Any, List

class RewardCalculator:
    
    """
        Improved Reward Calculator vá»›i path history tracking
        GiÃºp RL trÃ¡nh routing loops vÃ  há»c Ä‘Æ°á»£c behavior tá»‘t hÆ¡n Dijkstra
    """
    def __init__(self):
        # Service-specific QoS profiles
        self.service_profiles = {
            "VIDEO_STREAMING": {
                'priorityWeight': 1.0,
                'maxLatencyMs': 50.0,
                'minBandwidthMbps': 500.0,
                'maxLossRate': 0.02
            },
            "VOICE_CALL": {
                'priorityWeight': 1.5,
                'maxLatencyMs': 30.0,
                'minBandwidthMbps': 0.128,
                'maxLossRate': 0.01
            },
            "MESSAGING": {
                'priorityWeight': 0.7,
                'maxLatencyMs': 2000.0,
                'minBandwidthMbps': 0.01,
                'maxLossRate': 0.05
            },
            "FILE_TRANSFER": {
                'priorityWeight': 0.8,
                'maxLatencyMs': 5000.0,
                'minBandwidthMbps': 100.0,
                'maxLossRate': 0.001
            }
        }
        self.training_phase = "exploration"
    
    def calculate_reward(self, qos: Dict, link_metrics: Dict, source_node: Dict, 
                    next_hop_node: Dict, step: int, total_steps: int, 
                    reached_destination: bool = False, path_history: List[str] = None) -> float:
        """Simplified reward function - FOCUS ON SUCCESS"""
        
        if not self._is_link_usable(link_metrics):
            return -5.0  # GIáº¢M penalty
        
        # 1. ğŸ¯ SUCCESS REWARD - QUAN TRá»ŒNG NHáº¤T
        if reached_destination:
            step_bonus = (total_steps - step) * 10.0  # TÄ‚NG bonus cho Ä‘Æ°á»ng ngáº¯n
            return 300.0 + step_bonus  # TÄ‚NG success reward

        # 2. ğŸ”„ LOOP PENALTY - GIáº¢M
        if path_history and self._is_loop_detected(next_hop_node, path_history):
            return -20.0  # GIáº¢M penalty
        
        reward = 0.0
        
        # 3. ğŸ“ˆ PROGRESS REWARD - ThÆ°á»Ÿng tiáº¿n Ä‘á»™ Ä‘Æ¡n giáº£n
        reward += 5.0  # LuÃ´n thÆ°á»Ÿng cho viá»‡c chuyá»ƒn tiáº¿p
        
        # 4. ğŸ”— LINK QUALITY REWARD - ÄÆ N GIáº¢N HÃ“A
        latency = link_metrics.get('latencyMs', 1000.0)
        loss_rate = link_metrics.get('packetLossRate', 1.0)
        
        # Latency reward
        if latency < 100.0:  # Chá»‰ penalty latency ráº¥t cao
            reward += 10.0 * (100.0 - latency) / 100.0
        else:
            reward -= 5.0
        
        # Packet loss penalty - GIáº¢M
        if loss_rate > 0.1:  # Chá»‰ penalty loss rate cao
            reward -= 10.0 * min(loss_rate, 1.0)
        else:
            reward += 5.0
        
        # 5. ğŸª EXPLORATION BONUS - ThÆ°á»Ÿng khÃ¡m phÃ¡
        if path_history and next_hop_node.get('nodeId') not in path_history:
            reward += 8.0
        
        # 6. â±ï¸ HOP PENALTY - Ráº¤T NHáº¸
        reward -= 0.5  # Giáº£m penalty
        
        return max(-10.0, min(50.0, reward))  # Giá»›i háº¡n reward nhá» hÆ¡n
    
    def _is_loop_detected(self, next_hop_node: Dict, path_history: List[str]) -> bool:
        """PhÃ¡t hiá»‡n vÃ²ng láº·p dá»±a trÃªn path history"""
        if not path_history or not next_hop_node:
            return False
            
        next_node_id = next_hop_node.get('nodeId')
        if not next_node_id:
            return False
            
        # Kiá»ƒm tra náº¿u next node Ä‘Ã£ xuáº¥t hiá»‡n trong path history
        return next_node_id in path_history
    
    def _calculate_progress_reward(self, source_node: Dict, next_hop_node: Dict, 
                                 qos: Dict, path_history: List[str]) -> float:
        """ThÆ°á»Ÿng cho viá»‡c tiáº¿n gáº§n Ä‘áº¿n Ä‘Ã­ch - improved version"""
        try:
            dest_id = qos.get('destinationNodeId')
            if not dest_id:
                return 1.0  # Default progress reward
            
            src_pos = source_node.get('position', {})
            next_pos = next_hop_node.get('position', {})
            dest_node = qos.get('destinationNodeInfo', {})
            dest_pos = dest_node.get('position', {})
            
            # Náº¿u cÃ³ Ä‘á»§ position data, tÃ­nh toÃ¡n khoáº£ng cÃ¡ch
            if src_pos and next_pos and dest_pos:
                # TÃ­nh khoáº£ng cÃ¡ch tá»« source vÃ  next_hop Ä‘áº¿n destination
                src_to_dest = self._calculate_distance_3d(src_pos, dest_pos)
                next_to_dest = self._calculate_distance_3d(next_pos, dest_pos)
                
                if src_to_dest > 0:
                    progress_ratio = (src_to_dest - next_to_dest) / src_to_dest
                    # ThÆ°á»Ÿng 5-15 points cho viá»‡c tiáº¿n gáº§n Ä‘Ã­ch
                    return max(0.0, progress_ratio * 15.0)
            
            # Fallback: heuristic progress reward
            if path_history:
                # Náº¿u path Ä‘ang dÃ i, thÆ°á»Ÿng Ã­t hÆ¡n Ä‘á»ƒ trÃ¡nh Ä‘Æ°á»ng vÃ²ng
                path_length_penalty = min(len(path_history) * 0.5, 5.0)
                return 5.0 - path_length_penalty
            else:
                return 3.0  # Default progress reward
                
        except Exception as e:
            print(f"âš ï¸  Progress reward calculation error: {e}")
            return 2.0
    
    def _calculate_distance_3d(self, pos1: Dict, pos2: Dict) -> float:
        """TÃ­nh khoáº£ng cÃ¡ch 3D giá»¯a hai positions"""
        try:
            lat1, lon1, alt1 = pos1.get('latitude', 0), pos1.get('longitude', 0), pos1.get('altitude', 0)
            lat2, lon2, alt2 = pos2.get('latitude', 0), pos2.get('longitude', 0), pos2.get('altitude', 0)
            
            # Simple Euclidean distance approximation
            distance = ((lat2 - lat1)**2 + (lon2 - lon1)**2 + (alt2 - alt1)**2) ** 0.5
            return max(0.1, distance)  # Avoid division by zero
        except:
            return 1.0  # Default distance
    
    def _calculate_link_quality_reward(self, link_metrics: Dict, service_profile: Dict) -> float:
        """TÃ­nh toÃ¡n thÆ°á»Ÿng dá»±a trÃªn cháº¥t lÆ°á»£ng liÃªn káº¿t - FIX math errors"""
        reward = 0.0
        
        latency = link_metrics.get('latencyMs', 1000.0)
        bandwidth = link_metrics.get('currentAvailableBandwidthMbps', 0.0)
        loss_rate = link_metrics.get('packetLossRate', 1.0)
        link_score = link_metrics.get('linkScore', 0.0)
        
        max_latency = service_profile['maxLatencyMs']
        min_bandwidth = service_profile['minBandwidthMbps']
        max_loss_rate = service_profile['maxLossRate']
        
        # FIX: Avoid division by zero vÃ  negative values
        max_latency = max(1.0, max_latency)
        min_bandwidth = max(0.1, min_bandwidth)
        max_loss_rate = max(0.001, max_loss_rate)
        
        # FIX: Ensure values are reasonable
        latency = max(0.1, latency)
        bandwidth = max(0.0, bandwidth)
        loss_rate = max(0.0, min(1.0, loss_rate))
        
        # 1. ğŸ“ LATENCY REWARD
        if latency <= max_latency:
            latency_quality = (max_latency - latency) / max_latency
            reward += 25.0 * max(0.0, latency_quality)
        else:
            latency_penalty = (latency - max_latency) / max_latency
            reward -= 20.0 * min(max(0.0, latency_penalty), 2.0)
            
        # 2. ğŸ“Š BANDWIDTH REWARD
        if bandwidth >= min_bandwidth:
            bandwidth_surplus = (bandwidth - min_bandwidth) / min_bandwidth
            reward += 20.0 * min(max(0.0, bandwidth_surplus), 2.0)
        else:
            bandwidth_deficit = (min_bandwidth - bandwidth) / min_bandwidth
            reward -= 15.0 * min(max(0.0, bandwidth_deficit), 2.0)
        
        # 3. ğŸ“‰ PACKET LOSS PENALTY
        if loss_rate > max_loss_rate:
            loss_penalty = (loss_rate - max_loss_rate) / max_loss_rate
            reward -= 25.0 * min(max(0.0, loss_penalty), 3.0)
        else:
            loss_quality = (max_loss_rate - loss_rate) / max_loss_rate
            reward += 15.0 * max(0.0, loss_quality)
        
        # 4. ğŸ† LINK SCORE BONUS
        if link_score > 50.0:
            reward += min(link_score * 0.1, 10.0)

        return reward
        
    def _is_link_usable(self, link_metrics: Dict) -> bool:
        """Kiá»ƒm tra link cÃ³ usable khÃ´ng - relaxed conditions"""
        if not link_metrics or not link_metrics.get('isLinkActive', True):
            return False
        if link_metrics.get('currentAvailableBandwidthMbps', 0.0) < 0.1:
            return False
        if link_metrics.get('packetLossRate', 1.0) > 0.9:  # TÄƒng ngÆ°á»¡ng loss
            return False
        return True

    def update_training_phase(self, episode: int, total_episodes: int):
        """Cáº­p nháº­t phase training Ä‘á»ƒ Ä‘iá»u chá»‰nh reward strategy"""
        progress = episode / total_episodes
        
        if progress < 0.3:
            self.training_phase = "exploration"
        elif progress < 0.7:
            self.training_phase = "exploitation" 
        else:
            self.training_phase = "fine_tuning"
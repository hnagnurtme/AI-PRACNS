# main.py
import torch
import os
from typing import Dict, Any, Tuple

# Import to√†n b·ªô logic ƒë√£ x√¢y d·ª±ng
from env.StateProcessor import StateProcessor
from env.ActionMapper import ActionMapper
from agents.DqnAgent import DqnAgent
from agents.InMemoryReplayBuffer import InMemoryReplayBuffer 
from simulator.NetworkSimulator import NetworkSimulator
from training.trainer import train_agent_batch
from scripts.scenario_manager import run_scenario 
from utils.static_data import BASE_NODE_INFO, calculate_link_score_simplified 
import random
import numpy as np

# --- 1. H√ÄM M√î PH·ªéNG DIJKSTRA (Greedy) ---
def find_best_dijkstra_next_hop(raw_state_s: Dict[str, Any]) -> str:
    """M√¥ ph·ªèng quy·∫øt ƒë·ªãnh tham lam d·ª±a tr√™n LinkScore cao nh·∫•t."""
    neighbor_links = raw_state_s.get('neighborLinkMetrics', {})
    if not neighbor_links:
        return raw_state_s.get('sourceNodeId', 'NONE') 

    best_node_id = None
    max_score = -float('inf')
    
    for node_id, link in neighbor_links.items():
        score = link.get('linkScore', 0.0)
        if score > max_score:
            max_score = score
            best_node_id = node_id
            
    return best_node_id if best_node_id is not None else raw_state_s.get('sourceNodeId', 'NONE')

# --- 2. H√ÄM CH·∫†Y SO S√ÅNH (INFERENCE LOOP) ---
def run_comparison_loop(agent: DqnAgent, simulator: NetworkSimulator, source_id: str, dest_id: str, num_queries: int, current_scenario_name: str):
    """Ch·∫°y v√≤ng l·∫∑p gi·∫£ ƒë·ªãnh, so s√°nh quy·∫øt ƒë·ªãnh c·ªßa RL vs Dijkstra."""
    
    agent.epsilon = 0.0 
    drl_wins = 0
    dijkstra_wins = 0
    total_reward_drl = 0.0
    total_reward_dijkstra = 0.0

    print("\n" + "="*80)
    print(f"| B·∫ÆT ƒê·∫¶U SO S√ÅNH INFERENCE ({num_queries} QUERY) - K·ªäCH B·∫¢N: {current_scenario_name} |")
    print("="*80 + "\n")
    
    for i in range(num_queries):
        raw_state_s: Dict[str, Any] = simulator._collect_current_state_data(source_id, dest_id)
        
        # 1. Quy·∫øt ƒë·ªãnh c·ªßa DRL
        state_vector_s = simulator.processor.json_to_state_vector(raw_state_s)
        drl_action_id = agent.select_action(state_vector_s)
        drl_link = raw_state_s['neighborLinkMetrics'].get(drl_action_id, {})
        drl_reward = simulator.reward_calc.calculate_reward(simulator.default_qos, drl_link)

        # 2. Quy·∫øt ƒë·ªãnh c·ªßa Dijkstra (Greedy)
        dijkstra_action_id = find_best_dijkstra_next_hop(raw_state_s)
        dijkstra_link = raw_state_s['neighborLinkMetrics'].get(dijkstra_action_id, {})
        dijkstra_reward = simulator.reward_calc.calculate_reward(simulator.default_qos, dijkstra_link)

        # 3. Log v√† ƒê·∫øm
        total_reward_drl += drl_reward
        total_reward_dijkstra += dijkstra_reward
        
        if drl_reward > dijkstra_reward:
            drl_wins += 1
        elif dijkstra_reward > drl_reward:
            dijkstra_wins += 1

    # Log k·∫øt qu·∫£ cu·ªëi c√πng
    print("\n| K·∫æT QU·∫¢ T·ªîNG H·ª¢P:")
    print("-" * 35)
    print(f"| T·ªïng Reward DRL: {total_reward_drl:.2f}")
    print(f"| T·ªïng Reward Dijkstra: {total_reward_dijkstra:.2f}")
    print(f"| DRL T·ªët h∆°n: {drl_wins} l·∫ßn")
    print(f"| Dijkstra T·ªët h∆°n: {dijkstra_wins} l·∫ßn")
    
    if total_reward_drl > total_reward_dijkstra:
        print("\nüèÜ K·∫æT LU·∫¨N: DRL Agent ƒë√£ v∆∞·ª£t tr·ªôi trong vi·ªác ƒë√°p ·ª©ng QoS.")
    else:
        print("\n‚ö†Ô∏è K·∫æT LU·∫¨N: Dijkstra (Greedy) v·∫´n t·ªët h∆°n. C·∫ßn hu·∫•n luy·ªán th√™m!")
    print("="*80)


# --- C·∫§U H√åNH V√Ä CH·∫†Y CH√çNH ---

MAX_NEIGHBORS = 10 
STATE_SIZE = 6 + (4 * MAX_NEIGHBORS) 

if __name__ == "__main__":
    
    print("--- Kh·ªüi ƒë·ªông DRL Agent ---")
    
    # 1. Kh·ªüi t·∫°o Th√†nh ph·∫ßn C·ªët l√µi
    processor = StateProcessor(max_neighbors=MAX_NEIGHBORS)
    mapper = ActionMapper() 
    mapper.sync_node_list()
    ACTION_SIZE = mapper.get_action_size()
    
    if ACTION_SIZE == 0:
        print("üõë L·ªói: Kh√¥ng t√¨m th·∫•y Node n√†o trong d·ªØ li·ªáu tƒ©nh.")
        exit()
        
    dqn_agent = DqnAgent(state_size=STATE_SIZE, action_size=ACTION_SIZE)
    dqn_agent.action_mapper = mapper
    buffer = InMemoryReplayBuffer()
    simulator = NetworkSimulator(dqn_agent, buffer) 

    # 2. Hu·∫•n luy·ªán (Phase 1)
    TOTAL_TRAINING_STEPS_BASELINE = 10000 
    TOTAL_TRAINING_STEPS_FOCUSED = 10000 # TƒÉng l√™n 10k b∆∞·ªõc
    BATCH_SIZE = 64
    TARGET_UPDATE_FREQ = 200 
    SOURCE_NODE = "NodeA"
    DEST_NODE = "NodeD"
    
    # [B∆Ø·ªöC 1/3] HU·∫§N LUY·ªÜN BASELINE
    print("\n[B∆Ø·ªöC 1/3] B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán BASELINE (10k b∆∞·ªõc)...")
    run_scenario('BASELINE') 
    for step in range(1, TOTAL_TRAINING_STEPS_BASELINE + 1):
        simulator.simulate_one_step(source_id=SOURCE_NODE, dest_id=DEST_NODE)
        train_agent_batch(dqn_agent, buffer, BATCH_SIZE, TARGET_UPDATE_FREQ)
    print(f"HU·∫§N LUY·ªÜN BASELINE HO√ÄN T·∫§T ({TOTAL_TRAINING_STEPS_BASELINE} b∆∞·ªõc).")

    # [B∆Ø·ªöC 2/3] HU·∫§N LUY·ªÜN CHUY√äN S√ÇU (Focused Training)
    print("\n[B∆Ø·ªöC 2/3] Hu·∫•n luy·ªán Chuy√™n s√¢u tr√™n k·ªãch b·∫£n T·∫ÆC NGH·∫ºN ƒë·ªÉ t·ªëi ∆∞u h√≥a n√© tr√°nh...")
    run_scenario('CONGESTION_LINK_AB') # ƒê·∫∑t d·ªØ li·ªáu tƒ©nh v·ªÅ tr·∫°ng th√°i T·∫ÆC NGH·∫ºN
    dqn_agent.epsilon = 0.8 # TƒÉng epsilon ƒë·ªÉ kh√°m ph√° l·∫°i c√°c h√†nh ƒë·ªông r·ªßi ro
    for step in range(1, TOTAL_TRAINING_STEPS_FOCUSED + 1):
        simulator.simulate_one_step(source_id=SOURCE_NODE, dest_id=DEST_NODE)
        train_agent_batch(dqn_agent, buffer, BATCH_SIZE, TARGET_UPDATE_FREQ)
    print(f"HU·∫§N LUY·ªÜN CHUY√äN S√ÇU HO√ÄN T·∫§T ({TOTAL_TRAINING_STEPS_FOCUSED} b∆∞·ªõc).")
    dqn_agent.epsilon = 0.01 # Kh√¥i ph·ª•c epsilon th·∫•p cho inference

    
    # 3. B·∫ÆT ƒê·∫¶U SO S√ÅNH HI·ªÜU SU·∫§T TRONG C√ÅC K·ªäCH B·∫¢N
    
    # --- S-1: M·∫†NG B√åNH TH∆Ø·ªúNG ---
    run_comparison_loop(dqn_agent, simulator, SOURCE_NODE, DEST_NODE, num_queries=200, current_scenario_name='BASELINE')

    # --- S-2: T·∫ÆC NGH·∫ºN (Ki·ªÉm tra xem Agent c√≥ n√© tr√°nh ƒë∆∞·ª£c kh√¥ng) ---
    run_scenario('CONGESTION_LINK_AB') 
    run_comparison_loop(dqn_agent, simulator, SOURCE_NODE, DEST_NODE, num_queries=200, current_scenario_name='CONGESTION_LINK_AB')
    
    # --- S-3: NODE QU√Å T·∫¢I ---
    run_scenario('NODE_C_OVERLOAD') 
    run_comparison_loop(dqn_agent, simulator, SOURCE_NODE, DEST_NODE, num_queries=200, current_scenario_name='NODE_C_OVERLOAD')
    
    # --- S-4: M·∫§T LINK HO√ÄN TO√ÄN ---
    run_scenario('LINK_FAIL_AB') 
    run_comparison_loop(dqn_agent, simulator, SOURCE_NODE, DEST_NODE, num_queries=200, current_scenario_name='LINK_FAIL_AB')
    
    # --- S-5: SUY HAO C·ª∞C ƒê·ªò (M√¥i tr∆∞·ªùng) ---
    run_scenario('EXTREME_ATTENUATION_AC')
    run_comparison_loop(dqn_agent, simulator, SOURCE_NODE, DEST_NODE, num_queries=200, current_scenario_name='EXTREME_ATTENUATION_AC')
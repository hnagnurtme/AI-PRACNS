# SAGIN Routing System - Development Guide

## üìã M·ª•c L·ª•c
1. [H∆∞·ªõng D·∫´n Training](#h∆∞·ªõng-d·∫´n-training)
2. [C·∫•u H√¨nh v√† Tham S·ªë](#c·∫•u-h√¨nh-v√†-tham-s·ªë)
3. [T√≠nh NƒÉng N√¢ng Cao](#t√≠nh-nƒÉng-n√¢ng-cao)
4. [Monitoring v√† ƒê√°nh Gi√°](#monitoring-v√†-ƒë√°nh-gi√°)
5. [Troubleshooting](#troubleshooting)
6. [Best Practices](#best-practices)

---

## H∆∞·ªõng D·∫´n Training

### 1. Training C∆° B·∫£n

#### B∆∞·ªõc 1: Kh·ªüi ƒë·ªông training
```bash
cd Backend
python -m training.train
```

#### B∆∞·ªõc 2: Training v·ªõi s·ªë episodes t√πy ch·ªânh
```bash
python -m training.train --episodes 3000
```

#### B∆∞·ªõc 3: Training v·ªõi config file t√πy ch·ªânh
```bash
python -m training.train --config custom_config.yaml
```

#### B∆∞·ªõc 4: Resume t·ª´ checkpoint
```bash
python -m training.train --resume
```

### 2. Training v·ªõi Enhanced Trainer

Enhanced Trainer bao g·ªìm:
- **Curriculum Learning**: T·ª´ d·ªÖ ƒë·∫øn kh√≥
- **Imitation Learning**: H·ªçc t·ª´ expert (Dijkstra)
- **Multi-objective Optimization**: T·ªëi ∆∞u nhi·ªÅu m·ª•c ti√™u

ƒê·ªÉ s·ª≠ d·ª•ng Enhanced Trainer, set trong config:
```yaml
training:
  use_enhanced_trainer: true
```

### 3. Training Flow Chi Ti·∫øt

```
1. Initialize Environment
   ‚îú‚îÄ‚îÄ Load nodes t·ª´ MongoDB
   ‚îú‚îÄ‚îÄ Load terminals t·ª´ MongoDB
   ‚îî‚îÄ‚îÄ Create RoutingEnvironment

2. Initialize Agent
   ‚îú‚îÄ‚îÄ Create DuelingDQN networks (Q-network + Target network)
   ‚îú‚îÄ‚îÄ Initialize Replay Buffer
   ‚îî‚îÄ‚îÄ Set exploration parameters (epsilon)

3. Training Loop (cho m·ªói episode):
   ‚îú‚îÄ‚îÄ Reset environment
   ‚îÇ   ‚îú‚îÄ‚îÄ Ch·ªçn random source & destination terminals
   ‚îÇ   ‚îî‚îÄ‚îÄ Build initial state
   ‚îÇ
   ‚îú‚îÄ‚îÄ Episode Loop (cho m·ªói step):
   ‚îÇ   ‚îú‚îÄ‚îÄ Select action (epsilon-greedy)
   ‚îÇ   ‚îú‚îÄ‚îÄ Execute action ‚Üí (next_state, reward, done)
   ‚îÇ   ‚îú‚îÄ‚îÄ Store experience v√†o replay buffer
   ‚îÇ   ‚îú‚îÄ‚îÄ Sample batch t·ª´ replay buffer
   ‚îÇ   ‚îú‚îÄ‚îÄ Compute Q-targets
   ‚îÇ   ‚îú‚îÄ‚îÄ Update Q-network (backpropagation)
   ‚îÇ   ‚îú‚îÄ‚îÄ Update target network (m·ªói C steps)
   ‚îÇ   ‚îî‚îÄ‚îÄ Decay epsilon
   ‚îÇ
   ‚îú‚îÄ‚îÄ Evaluation (m·ªói eval_frequency episodes):
   ‚îÇ   ‚îú‚îÄ‚îÄ Run evaluation episodes
   ‚îÇ   ‚îú‚îÄ‚îÄ Compute metrics (success rate, mean reward, etc.)
   ‚îÇ   ‚îî‚îÄ‚îÄ Save best model n·∫øu c·∫£i thi·ªán
   ‚îÇ
   ‚îî‚îÄ‚îÄ Checkpoint (m·ªói save_frequency episodes):
       ‚îî‚îÄ‚îÄ Save model checkpoint

4. Final Evaluation & Save
   ‚îú‚îÄ‚îÄ Run comprehensive evaluation
   ‚îî‚îÄ‚îÄ Save final model
```

---

## C·∫•u H√¨nh v√† Tham S·ªë

### ‚ö†Ô∏è CRITICAL PARAMETERS - C√°c Tham S·ªë Quan Tr·ªçng Nh·∫•t

#### 1. **Learning Rate** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Quy·∫øt ƒë·ªãnh t·ªëc ƒë·ªô h·ªçc v√† stability
- **Gi√° tr·ªã**: `0.0001` (default)
- **Qu√° cao** (> 0.001): Training kh√¥ng ·ªïn ƒë·ªãnh, loss explode, NaN
- **Qu√° th·∫•p** (< 0.00001): H·ªçc qu√° ch·∫≠m, kh√¥ng h·ªôi t·ª•
- **Khuy·∫øn ngh·ªã**: B·∫Øt ƒë·∫ßu v·ªõi `0.0001`, tune trong kho·∫£ng `[0.00005, 0.0005]`

```yaml
rl_agent:
  dqn:
    learning_rate: 0.0001
```

#### 2. **Gamma (Discount Factor)** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Quy·∫øt ƒë·ªãnh agent quan t√¢m rewards xa ƒë·∫øn ƒë√¢u
- **Gi√° tr·ªã**: `0.99` (default)
- **Qu√° cao** (> 0.99): Agent quan t√¢m rewards qu√° xa ‚Üí ch·∫≠m h·ªçc
- **Qu√° th·∫•p** (< 0.9): Agent ch·ªâ quan t√¢m immediate rewards ‚Üí kh√¥ng t·ªëi ∆∞u long-term
- **Khuy·∫øn ngh·ªã**: `0.95 - 0.99` cho episodic tasks nh∆∞ routing

```yaml
rl_agent:
  dqn:
    gamma: 0.99
```

#### 3. **Batch Size** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Stability v√† memory usage
- **Gi√° tr·ªã**: `64` (default)
- **Qu√° nh·ªè** (< 32): Unstable gradients, noisy updates
- **Qu√° l·ªõn** (> 256): T·ªën memory, ch·∫≠m training, c√≥ th·ªÉ overfit
- **Khuy·∫øn ngh·ªã**: `32-128` t√πy v√†o GPU memory

```yaml
rl_agent:
  dqn:
    batch_size: 64
```

#### 4. **Replay Buffer Size** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: ƒêa d·∫°ng c·ªßa training data
- **Gi√° tr·ªã**: `100000` (default)
- **Qu√° nh·ªè** (< 10000): Kh√¥ng ƒë·ªß ƒëa d·∫°ng, overfitting
- **Qu√° l·ªõn** (> 1000000): T·ªën memory, ch·∫≠m sampling
- **Khuy·∫øn ngh·ªã**: `50000-200000` cho m√¥i tr∆∞·ªùng ph·ª©c t·∫°p

```yaml
rl_agent:
  dqn:
    buffer_size: 100000
```

#### 5. **Target Update Frequency** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Stability c·ªßa target Q-values
- **Gi√° tr·ªã**: `1000` steps (default)
- **Qu√° th∆∞·ªùng xuy√™n** (< 100): Target network thay ƒë·ªïi qu√° nhanh ‚Üí unstable
- **Qu√° √≠t** (> 5000): Target network qu√° c≈© ‚Üí ch·∫≠m h·ªçc
- **Khuy·∫øn ngh·ªã**: `500-2000` steps

```yaml
rl_agent:
  dqn:
    target_update_interval: 1000
```

#### 6. **Learning Starts** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: ƒê·∫£m b·∫£o ƒë·ªß experiences tr∆∞·ªõc khi train
- **Gi√° tr·ªã**: `5000` (default)
- **Qu√° th·∫•p** (< 1000): H·ªçc t·ª´ qu√° √≠t samples ‚Üí unstable, overfitting
- **Qu√° cao** (> 10000): T·ªën th·ªùi gian ch·ªù ƒë·ª£i kh√¥ng c·∫ßn thi·∫øt
- **Khuy·∫øn ngh·ªã**: `5000-10000` cho m√¥i tr∆∞·ªùng ph·ª©c t·∫°p

```yaml
rl_agent:
  dqn:
    learning_starts: 5000
```

#### 7. **Reward Scale** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Stability v√† t·ªëc ƒë·ªô h·ªçc
- **Gi√° tr·ªã**: `success_reward: 200.0` (default)
- **Qu√° l·ªõn** (> 1000): Q-values explode, training unstable
- **Qu√° nh·ªè** (< 10): Agent kh√¥ng h·ªçc ƒë∆∞·ª£c (rewards qu√° nh·ªè so v·ªõi noise)
- **Khuy·∫øn ngh·ªã**: Gi·ªØ rewards trong kho·∫£ng `[-100, 500]` ƒë·ªÉ stable

```yaml
reward:
  success_reward: 200.0
  failure_penalty: -10.0
  progress_reward_scale: 100.0
```

### Exploration Parameters

```yaml
rl_agent:
  dqn:
    exploration_initial_eps: 1.0      # B·∫Øt ƒë·∫ßu v·ªõi 100% exploration
    exploration_final_eps: 0.01        # K·∫øt th√∫c v·ªõi 1% exploration
    exploration_decay: 0.9995          # T·ªëc ƒë·ªô decay
```

### Network Architecture

```yaml
rl_agent:
  dqn:
    dueling:
      hidden_dims: [512, 256, 128]    # K√≠ch th∆∞·ªõc c√°c layers
      activation_fn: "elu"             # ELU t·ªët h∆°n ReLU cho DQN
      dropout_rate: 0.1                # Regularization
      use_layer_norm: true            # Training stability
```

### Training Parameters

```yaml
training:
  max_episodes: 2000                  # T·ªïng s·ªë episodes
  max_steps_per_episode: 15           # Max steps m·ªói episode
  eval_frequency: 50                   # Evaluate m·ªói 50 episodes
  eval_episodes: 20                   # S·ªë episodes ƒë·ªÉ evaluate
  save_frequency: 100                 # Save checkpoint m·ªói 100 episodes
  early_stopping_patience: 50          # Early stop n·∫øu kh√¥ng c·∫£i thi·ªán 50 evals
```

---

## T√≠nh NƒÉng N√¢ng Cao

### 1. Curriculum Learning

**M·ª•c ƒë√≠ch**: Train t·ª´ scenarios ƒë∆°n gi·∫£n ƒë·∫øn ph·ª©c t·∫°p

**C√°c Levels:**
- **Level 0 (Beginner)**: G·∫ßn (<1000km), √≠t nodes (5-30)
- **Level 1 (Easy)**: G·∫ßn (<2000km), nhi·ªÅu nodes h∆°n (10-40)
- **Level 2 (Medium)**: Xa (<5000km), nhi·ªÅu nodes (20-60)
- **Level 3 (Hard)**: R·∫•t xa (<10000km), nhi·ªÅu nodes (40-77), c√≥ QoS
- **Level 4 (Expert)**: To√†n c·∫ßu (<20000km), t·∫•t c·∫£ nodes (60-81)
- **Level 5 (Master)**: Kh√¥ng gi·ªõi h·∫°n

**C·∫•u h√¨nh:**
```yaml
curriculum:
  enabled: true
  min_success_rate: 0.7              # Advance khi success rate >= 70%
  min_episodes_at_level: 100         # T·ªëi thi·ªÉu 100 episodes m·ªói level
  adaptive: true                      # Adaptive difficulty
```

### 2. Imitation Learning

**M·ª•c ƒë√≠ch**: H·ªçc t·ª´ expert demonstrations (Dijkstra algorithm)

**Ph∆∞∆°ng ph√°p**: DAGGER (Dataset Aggregation)
- B·∫Øt ƒë·∫ßu v·ªõi 100% expert actions
- Gradually gi·∫£m expert ratio khi agent c·∫£i thi·ªán
- Mix expert actions v·ªõi agent actions

**C·∫•u h√¨nh:**
```yaml
imitation_learning:
  enabled: true
  use_dagger: true
  expert_ratio: 0.3                   # 30% expert actions ban ƒë·∫ßu
  bc_loss_weight: 0.5                # Behavior Cloning loss weight
```

### 3. Multi-Objective Optimization

**M·ª•c ƒë√≠ch**: T·ªëi ∆∞u ƒë·ªìng th·ªùi nhi·ªÅu m·ª•c ti√™u (latency, reliability, energy)

**C·∫•u h√¨nh:**
```yaml
multi_objective:
  enabled: true
  use_pareto: true
  pareto_front_size: 10
  latency_weight: 0.4
  reliability_weight: 0.3
  energy_weight: 0.3
  adaptive_weights: true              # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh weights
```

---

## Monitoring v√† ƒê√°nh Gi√°

### 1. Tensorboard

```bash
tensorboard --logdir=./logs/tensorboard
```

M·ªü browser t·∫°i `http://localhost:6006` ƒë·ªÉ xem:
- Training reward
- Loss curves
- Success rate
- Epsilon decay
- Episode length

### 2. Evaluation Script

```python
from training.trainer import RoutingTrainer
from models.database import db

# Load model
trainer = RoutingTrainer(config)
metrics = trainer.load_and_evaluate(
    model_path='./models/best_models/best_model.pt',
    nodes=nodes,
    terminals=terminals,
    num_episodes=50
)

print(f"Success Rate: {metrics['success_rate']:.2%}")
print(f"Mean Hops: {metrics['mean_hops']:.1f}")
print(f"Mean Latency: {metrics['mean_latency']:.2f}ms")
```

---

## Troubleshooting

### 1. Training Kh√¥ng H·ªôi T·ª•

**Tri·ªáu ch·ª©ng**: Reward kh√¥ng tƒÉng, loss kh√¥ng gi·∫£m

**Gi·∫£i ph√°p**:
- Gi·∫£m learning rate: `0.0001 ‚Üí 0.00005`
- TƒÉng batch size: `64 ‚Üí 128`
- Ki·ªÉm tra reward scale (c√≥ th·ªÉ qu√° l·ªõn/nh·ªè)
- TƒÉng exploration: `exploration_final_eps: 0.05`
- Ki·ªÉm tra state normalization

### 2. Out of Memory

**Tri·ªáu ch·ª©ng**: CUDA out of memory

**Gi·∫£i ph√°p**:
- Gi·∫£m batch size: `64 ‚Üí 32`
- Gi·∫£m buffer size: `100000 ‚Üí 50000`
- Gi·∫£m max_nodes trong state: `30 ‚Üí 20`
- S·ª≠ d·ª•ng CPU thay v√¨ GPU

### 3. Success Rate Th·∫•p

**Tri·ªáu ch·ª©ng**: Agent kh√¥ng t√¨m ƒë∆∞·ª£c ƒë∆∞·ªùng ƒëi

**Gi·∫£i ph√°p**:
- TƒÉng success reward: `200.0 ‚Üí 500.0`
- Gi·∫£m failure penalty: `-10.0 ‚Üí -5.0`
- TƒÉng progress reward scale: `100.0 ‚Üí 200.0`
- S·ª≠ d·ª•ng Curriculum Learning
- S·ª≠ d·ª•ng Imitation Learning ƒë·ªÉ bootstrap

### 4. Training Qu√° Ch·∫≠m

**Tri·ªáu ch·ª©ng**: M·ªói episode m·∫•t qu√° nhi·ªÅu th·ªùi gian

**Gi·∫£i ph√°p**:
- Gi·∫£m max_steps_per_episode: `15 ‚Üí 10`
- Gi·∫£m max_nodes: `30 ‚Üí 20`
- T·∫Øt m·ªôt s·ªë features kh√¥ng c·∫ßn thi·∫øt
- S·ª≠ d·ª•ng GPU n·∫øu c√≥
- TƒÉng batch size ƒë·ªÉ train hi·ªáu qu·∫£ h∆°n

### 5. NaN Loss

**Tri·ªáu ch·ª©ng**: Loss = NaN

**Gi·∫£i ph√°p**:
- Ki·ªÉm tra reward scale (c√≥ th·ªÉ qu√° l·ªõn)
- Th√™m gradient clipping: `gradient_clip: 10.0`
- Ki·ªÉm tra state c√≥ NaN/Inf kh√¥ng
- Normalize rewards: `reward = reward / 100.0`

---

## Best Practices

### 1. Hyperparameter Tuning

- **B·∫Øt ƒë·∫ßu v·ªõi default values** trong config
- **Tune t·ª´ng tham s·ªë m·ªôt** ƒë·ªÉ hi·ªÉu ·∫£nh h∆∞·ªüng
- **S·ª≠ d·ª•ng Tensorboard** ƒë·ªÉ visualize
- **Early stopping** ƒë·ªÉ tr√°nh overfitting

### 2. Reward Engineering

- **Reward scale quan tr·ªçng**: Qu√° l·ªõn ‚Üí unstable, qu√° nh·ªè ‚Üí ch·∫≠m h·ªçc
- **Shaped rewards**: Th√™m intermediate rewards ƒë·ªÉ guide learning
- **Penalty balance**: Kh√¥ng penalty qu√° m·∫°nh ‚Üí agent kh√¥ng d√°m explore

### 3. State Design

- **Normalize features**: T·∫•t c·∫£ features v·ªÅ [0, 1] ho·∫∑c [-1, 1]
- **Feature selection**: Ch·ªâ gi·ªØ features quan tr·ªçng
- **Caching**: Cache c√°c t√≠nh to√°n t·ªën k√©m (distance, quality)

### 4. Training Strategy

- **Warm-up**: Cho agent explore nhi·ªÅu tr∆∞·ªõc khi train (`learning_starts: 5000`)
- **Curriculum**: B·∫Øt ƒë·∫ßu t·ª´ scenarios ƒë∆°n gi·∫£n
- **Evaluation**: Evaluate th∆∞·ªùng xuy√™n nh∆∞ng kh√¥ng qu√° nhi·ªÅu (t·ªën th·ªùi gian)

---

## T√†i Li·ªáu Tham Kh·∫£o

### Papers:
1. **Dueling DQN**: "Dueling Network Architectures for Deep Reinforcement Learning" (Wang et al., 2016)
2. **Double DQN**: "Deep Reinforcement Learning with Double Q-learning" (van Hasselt et al., 2016)
3. **Prioritized Replay**: "Prioritized Experience Replay" (Schaul et al., 2016)
4. **Curriculum Learning**: "Curriculum Learning" (Bengio et al., 2009)
5. **DAGGER**: "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning" (Ross et al., 2011)

### Code Structure:
- `agent/dueling_dqn.py`: Dueling DQN implementation
- `environment/routing_env.py`: Environment implementation
- `environment/state_builder.py`: State representation
- `training/trainer.py`: Standard trainer
- `training/enhanced_trainer.py`: Enhanced trainer v·ªõi advanced features
- `training/curriculum_learning.py`: Curriculum learning
- `training/imitation_learning.py`: Imitation learning
- `training/multi_objective.py`: Multi-objective optimization

---

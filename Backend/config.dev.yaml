# Optimized Backend Configuration for RL Routing
# Enhanced for performance and training efficiency

# MongoDB Connection
mongodb:
  uri: "mongodb://admin:password@localhost:27017/aiprancs?authSource=admin"
  database: "aiprancs"
  connection_timeout: 30
  max_pool_size: 50
  min_pool_size: 10

# RL Agent Configuration - OPTIMIZED
rl_agent:
  # Algorithm
  algorithm: "DuelingDQN"
  architecture: "optimized_dueling"
  
  # Model Paths
  model_path: "./models/rl_agent"
  best_model_path: "./models/best_models"
  checkpoint_path: "./models/checkpoints"
  tensorboard_log_dir: "./logs/tensorboard"
  
  # Optimized DQN Hyperparameters
  dqn:
    learning_rate: 0.0001
    batch_size: 32  # Reduced for faster updates
    buffer_size: 100000
    learning_starts: 256  # ðŸ”§ FIX: Reduced from 1000 for faster learning start
    target_update_interval: 100  # ðŸ”§ FIX: Reduced from 1000 for more stable Q-targets
    gamma: 0.99
    tau: 0.005  # Soft updates for stability
    
    # Advanced Training
    use_double_dqn: true
    use_prioritized_replay: true
    gradient_clip: 10.0
    train_freq: 1  # Train every step
    gradient_steps: 1
    
    # Optimized Exploration
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.05  # ðŸ”§ FIX: Increased from 0.01 for more exploration
    exploration_decay: 0.999  # ðŸ”§ FIX: Slower decay from 0.9995
    epsilon_decay_strategy: "linear"  # ðŸ”§ NEW: Use linear decay instead of exponential
    
    # Optimized Dueling DQN
    dueling:
      enabled: true
      hidden_dims: [512, 256, 128]  # Deeper network
      activation_fn: "elu"  # Better than relu
      dropout_rate: 0.1  # Regularization
      use_layer_norm: true  # Training stability

# Optimized State Builder Configuration
# FIX: Reduced max_nodes for actual filtering and faster training
state_builder:
  # Optimized dimensions - REDUCED for faster training
  max_nodes: 15  # FIX: Reduced from 53 - only keep top 15 candidates
  max_terminals: 2  # Only need source + destination
  
  # Feature dimensions
  node_feature_dim: 12  # FIX: Reduced from 18
  terminal_feature_dim: 6
  global_feature_dim: 8
  
  # Enable Dijkstra-aware features
  include_dijkstra_features: true
  
  # Performance features
  use_caching: true
  cache_size: 1000
  enable_smart_filtering: true
  
  # Dynamic features
  dynamic_positions:
    enabled: true
    update_interval_seconds: 30

# Optimized Training Configuration
training:
  max_episodes: 5000
  max_steps_per_episode: 15
  eval_frequency: 25
  adaptive_max_steps: true
  
  # Advanced Training Features
  target_update_frequency: 100
  gradient_clip: 1.0
  early_stopping_patience: 100  # Phase 2: Increased from 50 to 100
  
  # Evaluation
  eval_episodes: 20  # More episodes for stable metrics
  eval_deterministic: true
  
  # Checkpointing
  save_frequency: 100
  save_best_only: true  # Only save best models
  
  # Early Stopping
  early_stopping:
    enabled: true
    patience: 100
    min_delta: 0.01
    monitor: "mean_reward"
  
  # Logging
  log_frequency: 10
  tensorboard_enabled: true
  console_log_enabled: true
  
  # Dashboard Configuration - Custom training visualization
  dashboard:
    enabled: true
    auto_cleanup_logs: true  # Tá»± Ä‘á»™ng xÃ³a logs sau training
    log_retention_days: 7    # Giá»¯ logs 7 ngÃ y gáº§n nháº¥t
    live_update_frequency: 10  # Update dashboard má»—i 10 episodes
  
  # Performance Monitoring
  enable_performance_metrics: true
  memory_monitoring: true
  
  # Advanced Training Features
  use_enhanced_trainer: true  # Use enhanced trainer with curriculum, imitation, multi-objective

# Curriculum Learning Configuration
curriculum:
  enabled: true
  min_success_rate: 0.7  # Advance to next level when success rate >= 70%
  min_episodes_at_level: 100  # Minimum episodes before advancing
  adaptive: true  # Adaptive difficulty
  difficulty_increment: 0.05

# Imitation Learning Configuration
imitation_learning:
  enabled: true
  use_dagger: true
  expert_ratio: 0.3
  bc_loss_weight: 0.5
  num_demos: 500
  use_weighted_demos: true

# Multi-objective Optimization Configuration
multi_objective:
  enabled: true
  use_pareto: true  # Use Pareto front
  pareto_front_size: 10
  latency_weight: 0.4
  reliability_weight: 0.3
  energy_weight: 0.3
  adaptive_weights: true  # Adapt weights based on performance

# Optimized Reward Function Configuration
# Phase 1 Enhancement: Dijkstra-aligned rewards
reward:
  # Phase 1: Enable Dijkstra-aligned reward calculation
  dijkstra_aligned: true  # NEW: Use Dijkstra's edge weight logic for rewards
  
  # Phase 1: Dijkstra parameters (match calculate_path_dijkstra)
  drop_threshold: 95.0  # Drop nodes with utilization >= 95%
  penalty_threshold: 80.0  # Apply penalty for utilization >= 80%
  penalty_multiplier: 3.0  # Multiplier for penalty (3.0x)
  
  # Primary Rewards (optimized scales)
  success_reward: 500.0
  failure_penalty: -50.0
  step_penalty: -1.0
  hop_penalty: -2.0
  
  # Progressive Rewards - TÄƒng Ä‘á»ƒ khuyáº¿n khÃ­ch progress
  progress_reward_scale: 100.0  # TÄƒng tá»« 15.0 lÃªn 100.0
  distance_reward_scale: 5.0  # Giáº£m tá»« 8.0 xuá»‘ng 5.0
  quality_reward_scale: 5.0
  proximity_bonus_scale: 50.0  # Bonus khi Ä‘áº¿n gáº§n destination
  
  # QoS Compliance
  qos_bonus: 30.0
  qos_penalty: -15.0
  
  # Efficiency Rewards
  efficiency_bonus: 20.0
  inefficiency_penalty: -10.0
  
  # Resource Awareness - Reduced penalties
  high_utilization_penalty: -1.0
  medium_utilization_penalty: -0.5
  loss_rate_penalty_scale: 3.0

# Action Space Configuration
action_space:
  type: "discrete"
  max_actions: 53  # Matches max_nodes (actual operational nodes)
  
  # Action Selection
  use_action_masking: true
  enable_temperature: true
  default_temperature: 1.0

# Network Configuration
network:
  # Node limits
  max_satellite_nodes: 15
  max_aerial_nodes: 10
  max_ground_nodes: 5
  
  # Performance limits
  max_bandwidth_mhz: 1000.0
  max_power_watts: 100.0
  max_latency_ms: 1000

# QoS Configuration
qos:
  # Default QoS profiles
  profiles:
    voice:
      max_latency_ms: 150
      min_bandwidth_mbps: 64
      max_loss_rate: 0.01
      priority: 10
      
    video:
      max_latency_ms: 300
      min_bandwidth_mbps: 512
      max_loss_rate: 0.05
      priority: 8
      
    data:
      max_latency_ms: 1000
      min_bandwidth_mbps: 10
      max_loss_rate: 0.1
      priority: 5
  
  # Validation
  enable_qos_validation: true
  strict_qos_enforcement: false

# Performance Optimization Configuration
performance:
  # Hardware
  num_threads: 4
  use_gpu: true
  device: "cuda"  # Prefer GPU
  
  # Memory
  max_memory_usage_mb: 4096
  enable_memory_monitoring: true
  
  # Caching
  enable_model_caching: true
  cache_size: 100
  cache_ttl_seconds: 3600
  
  # Batch processing
  enable_batch_processing: true
  max_batch_size: 32

# Logging Configuration
logging:
  level: "INFO"
  file: "./logs/rl_training.log"
  max_file_size_mb: 100
  backup_count: 5
  console: true
  tensorboard: true
  
  # Detailed logging
  enable_training_metrics: true
  enable_performance_metrics: true
  enable_qos_metrics: true

# Evaluation Configuration
evaluation:
  # Comprehensive metrics
  metrics:
    - "mean_reward"
    - "success_rate"
    - "mean_hops"
    - "mean_latency"
    - "mean_distance"
    - "qos_compliance_rate"
    - "resource_utilization"
  
  # Comparison benchmarks
  enable_dijkstra_comparison: true
  enable_heuristic_comparison: true
  
  # Reporting
  generate_reports: true
  report_format: "json"
  save_detailed_metrics: true

# Advanced Features
advanced:
  # Curriculum Learning
  curriculum_learning:
    enabled: true
    difficulty_steps: [500, 1000, 1500]  # Episode milestones
    distance_increase_km: [1000, 5000, 10000]
    node_failure_rates: [0.0, 0.1, 0.2]
  
  # Transfer Learning
  transfer_learning:
    enabled: false
    pretrained_model_path: "./models/pretrained"
    freeze_layers: ["shared_layers"]
  
  # Multi-objective Optimization
  multi_objective:
    enabled: true
    objectives: ["latency", "reliability", "resource_usage"]
    weights: [0.4, 0.4, 0.2]
  
  # Ensemble Learning
  ensemble:
    enabled: false
    num_models: 5
    voting_strategy: "soft"

# Service Configuration
service:
  # RL Routing Service
  rl_routing:
    enable_caching: true
    cache_ttl_seconds: 300
    max_concurrent_requests: 100
    timeout_seconds: 30
    
    # Fallback strategies
    enable_heuristic_fallback: true
    enable_dijkstra_fallback: true
    
    # QoS handling
    strict_qos_enforcement: false
    qos_validation_level: "medium"
  
  # Performance
  request_timeout_seconds: 10
  max_retries: 3
  circuit_breaker_enabled: true
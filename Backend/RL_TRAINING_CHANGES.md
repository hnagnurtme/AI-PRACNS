# ğŸ”§ RL Training Optimization - Changelog & Recommendations

## Overview

TÃ i liá»‡u nÃ y mÃ´ táº£ cÃ¡c thay Ä‘á»•i Ä‘Ã£ thá»±c hiá»‡n Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t RL training vÃ  Ä‘á» xuáº¥t hÆ°á»›ng phÃ¡t triá»ƒn tiáº¿p theo.

---

## ğŸ“ Changelog

### 1. Config Changes (`config.dev.yaml`)

| Parameter | Before | After | Reason |
|-----------|--------|-------|--------|
| `learning_starts` | 1000 | **256** | Báº¯t Ä‘áº§u há»c sá»›m hÆ¡n |
| `target_update_interval` | 1000 | **100** | Q-targets á»•n Ä‘á»‹nh hÆ¡n |
| `batch_size` | 64 | **32** | Updates nhanh hÆ¡n |
| `exploration_final_eps` | 0.01 | **0.05** | Exploration nhiá»u hÆ¡n |
| `epsilon_decay_strategy` | N/A | **"linear"** | Decay á»•n Ä‘á»‹nh |
| `max_nodes` | 53 | **15** | State vector nhá» hÆ¡n |
| `node_feature_dim` | 18 | **12** | Features tinh gá»n |

**State dimension**: 994 â†’ **200** (giáº£m ~5x)

---

### 2. Epsilon Decay (`dueling_dqn.py`)

**Before**: Exponential decay quÃ¡ nhanh
```python
epsilon = epsilon_start * (0.9995 ** total_steps)
```

**After**: Linear decay á»•n Ä‘á»‹nh
```python
if strategy == 'linear':
    progress = total_steps / max_steps
    epsilon = start - progress * (start - end)
```

---

### 3. Reward Function (`routing_env.py`)

**Before**: 15+ reward components â†’ rewards -95000 Ä‘áº¿n +2000

**After**: 3 core components â†’ rewards **-52 to +580**

| Case | Components |
|------|------------|
| **Per-step** | Progress ratio Ã— 30, Step penalty -1, Util penalty -20 |
| **Success** | Base 500, Dest GS +50, Efficiency Â±30 |
| **Truncated** | Fixed -50 |

**Key change**: Ratio-based rewards thay vÃ¬ absolute distances:
```python
progress_ratio = progress / initial_distance  # -1 to +1
reward = progress_ratio * 30.0
```

---

## ğŸ¯ Recommendations

### Short-term (1-2 days)

1. **Run longer training**: 5000+ episodes vá»›i config má»›i
2. **Monitor metrics**:
   - Loss nÃªn < 1000
   - Success rate nÃªn > 70%
   - Avg reward nÃªn > 0

### Medium-term (1 week)

1. **Add directional features** to state:
   ```python
   delta_lat = (dest_lat - node_lat) / 180.0
   delta_lon = (dest_lon - node_lon) / 360.0
   ```

2. **Implement curriculum learning**:
   - Start vá»›i easy pairs (gáº§n nhau)
   - Gradually tÄƒng difficulty

3. **Tune reward scales**:
   - Progress: 30 â†’ 50 náº¿u cáº§n stronger signal
   - Step penalty: -1 â†’ -2 náº¿u paths quÃ¡ dÃ i

### Long-term (2-4 weeks)

1. **Imitation learning warmup**:
   - Pre-train vá»›i Dijkstra expert paths
   - Sau Ä‘Ã³ fine-tune vá»›i RL

2. **Multi-objective optimization**:
   - Separate heads cho distance vs utilization
   - Weighted combination

3. **Graph Neural Network**:
   - Thay MLP báº±ng GNN cho tá»‘t hÆ¡n vá»›i graph structure
   - Xem xÃ©t GAT (Graph Attention) hoáº·c GCN

---

## ğŸ“Š Expected Results

| Metric | Before | Expected After |
|--------|--------|----------------|
| Success Rate | 40% | **>70%** |
| Loss | 18000+ | **<1000** |
| Avg Reward | -50000 | **>0** |
| Training Speed | Slow | **~3x faster** |

---

## ğŸ”— Related Files

- [config.dev.yaml](./config.dev.yaml) - Training configuration
- [dueling_dqn.py](./agent/dueling_dqn.py) - Agent implementation
- [routing_env.py](./environment/routing_env.py) - Environment & rewards
- [state_builder.py](./environment/state_builder.py) - State representation

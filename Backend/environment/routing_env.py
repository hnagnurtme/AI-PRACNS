"""
Optimized Routing Environment for SAGIN
Environment ƒë∆∞·ª£c t·ªëi ∆∞u cho training hi·ªáu qu·∫£ v√† performance
"""
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import logging
import math
from collections import deque

from environment.state_builder import RoutingStateBuilder

logger = logging.getLogger(__name__)


def get_terminal_connection_count(node_id: str) -> int:
    """Get number of terminals connected to a node - safe version to avoid circular import"""
    try:
        from models.database import db
        terminals_collection = db.get_collection('terminals')
        count = terminals_collection.count_documents({
            'connectedNodeId': node_id,
            'status': {'$in': ['connected', 'transmitting']}
        })
        return count
    except Exception as e:
        logger.warning(f"Error counting terminals for {node_id}: {e}")
        return 0


class RoutingEnvironment(gym.Env):
    """
    Optimized environment cho routing v·ªõi reward engineering ti√™n ti·∫øn
    Gi·ªØ nguy√™n interface v√† t√™n class
    """
    
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 4}
    
    def __init__(
        self,
        nodes: List[Dict],
        terminals: List[Dict],
        config: Dict = None,
        max_steps: int = 8  # GI·∫¢M: 10 ‚Üí 8 ƒë·ªÉ force shorter paths
    ):
        super().__init__()
        
        self.config = config or {}
        self.nodes = nodes
        self.terminals = terminals
        self.max_steps = max_steps
        
        # State builder
        self.state_builder = RoutingStateBuilder(config)
        
        # Action space
        max_actions = min(len(nodes), self.state_builder.max_nodes)
        self.action_space = spaces.Discrete(max_actions)
        
        # Observation space
        self.observation_space = spaces.Box(
            low=-1.0,  # Thay ƒë·ªïi ƒë·ªÉ ·ªïn ƒë·ªãnh training
            high=2.0,
            shape=(self.state_builder.state_dimension,),
            dtype=np.float32
        )
        
        # Episode state
        self.source_terminal = None
        self.dest_terminal = None
        self.current_node = None
        self.path = []
        self.visited_nodes = set()
        self.step_count = 0
        self.total_distance = 0.0
        self.total_latency = 0.0
        self.service_qos = None
        self.terminated = False  # Track if episode terminated successfully
        
        # Optimized reward configuration - ∆ØU TI√äN GI·∫¢M HOP/DISTANCE/LATENCY
        reward_config = self.config.get('reward', {})
        self.success_reward = reward_config.get('success_reward', 200.0)
        self.failure_penalty = reward_config.get('failure_penalty', -10.0)  # Gi·∫£m t·ª´ -30 xu·ªëng -10
        self.step_penalty = reward_config.get('step_penalty', -10.0)  # TƒÇNG: -8.0 ‚Üí -10.0 - M·ªñI STEP ƒê·ªÄU T·ªêN K√âM
        self.hop_penalty = reward_config.get('hop_penalty', -15.0)  # TƒÇNG: -12.0 ‚Üí -15.0 - HOP L√Ä T·ªêN K√âM NH·∫§T
        self.ground_station_hop_penalty = reward_config.get('ground_station_hop_penalty', -25.0)  # TƒÇNG: -20 ‚Üí -25
        self.progress_reward_scale = reward_config.get('progress_reward_scale', 80.0)  # GI·∫¢M: 150 ‚Üí 80 - Kh√¥ng th∆∞·ªüng qu√° nhi·ªÅu cho progress
        self.distance_reward_scale = reward_config.get('distance_reward_scale', 10.0)  # TƒÉng t·ª´ 5.0 ‚Üí 10.0: Distance quan tr·ªçng
        self.quality_reward_scale = reward_config.get('quality_reward_scale', 10.0)  # Gi·∫£m t·ª´ 30.0 ‚Üí 10.0: Resource l√† m·ª•c ti√™u th·ª© 2
        self.proximity_bonus_scale = reward_config.get('proximity_bonus_scale', 50.0)  # Bonus khi ƒë·∫øn g·∫ßn destination
        
        # Cache untuk performance
        self._node_cache = {node['nodeId']: node for node in nodes}
        self._terminal_cache = {terminal['terminalId']: terminal for terminal in terminals}
    
    def reset(
        self,
        seed: Optional[int] = None,
        options: Optional[Dict] = None
    ) -> Tuple[np.ndarray, Dict]:
        """Reset environment v·ªõi optimizations v√† explicit ground stations"""
        super().reset(seed=seed)
        
        # Reset terminated flag
        self.terminated = False
        
        # Get terminals v√† ground stations t·ª´ options ho·∫∑c random
        source_ground_station = None
        dest_ground_station = None
        
        if options:
            source_terminal_id = options.get('source_terminal_id')
            dest_terminal_id = options.get('dest_terminal_id')
            self.service_qos = options.get('service_qos')
            
            # üî• NEW: Get explicit ground stations from options
            source_ground_station = options.get('source_ground_station')
            dest_ground_station = options.get('dest_ground_station')
            
            self.source_terminal = self._terminal_cache.get(source_terminal_id)
            self.dest_terminal = self._terminal_cache.get(dest_terminal_id)
        else:
            # Random terminals
            if len(self.terminals) < 2:
                raise ValueError("Need at least 2 terminals")
            
            indices = self.np_random.choice(len(self.terminals), size=2, replace=False)
            self.source_terminal = self.terminals[indices[0]]
            self.dest_terminal = self.terminals[indices[1]]
        
        if not self.source_terminal or not self.dest_terminal:
            raise ValueError("Source or destination terminal not found")
        
        # üî• FIX: S·ª≠ d·ª•ng explicit ground stations n·∫øu c√≥, otherwise t√¨m optimal
        if source_ground_station:
            self.current_node = source_ground_station
            logger.info(f"üõ∞Ô∏è RL starting from explicit source GS: {source_ground_station['nodeId']}")
        else:
            # T√¨m initial node th√¥ng minh
            self.current_node = self._find_optimal_initial_node(
                self.source_terminal, self.dest_terminal
            )
        
        if not self.current_node:
            operational_nodes = [
                n for n in self.nodes 
                if n.get('isOperational', True) and n.get('position')
            ]
            if operational_nodes:
                # Ch·ªçn node g·∫ßn destination nh·∫•t
                dest_pos = self.dest_terminal.get('position')
                self.current_node = min(
                    operational_nodes,
                    key=lambda n: self._calculate_distance(
                        n.get('position'), dest_pos
                    ) if n.get('position') else float('inf')
                )
            else:
                raise ValueError("No operational nodes available")
        
        # üî• NEW: Store dest_ground_station for validation
        self.dest_ground_station = dest_ground_station
        
        # Reset episode state
        self.path = [self.source_terminal, self.current_node]
        self.visited_nodes = {self.current_node.get('nodeId')}
        self.step_count = 0
        self.total_distance = 0.0
        self.total_latency = 0.0
        
        # Build state
        state = self.state_builder.build_state(
            nodes=self.nodes,
            source_terminal=self.source_terminal,
            dest_terminal=self.dest_terminal,
            current_node=self.current_node,
            service_qos=self.service_qos,
            visited_nodes=list(self.visited_nodes)
        )
        
        info = {
            'path': self.path.copy(),
            'current_node': self.current_node.get('nodeId'),
            'distance_to_dest': self._calculate_distance(
                self.current_node.get('position'),
                self.dest_terminal.get('position')
            ),
            'hops': 1
        }
        
        return state, info
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """Optimized step function v·ªõi reward engineering ti√™n ti·∫øn"""
        self.step_count += 1
        
        # L·∫•y available nodes v·ªõi stress-aware filtering
        filtered_nodes = self.state_builder._smart_node_filtering(
            self.nodes, self.source_terminal, self.dest_terminal, 
            self.current_node, list(self.visited_nodes)
        )
        
        # Filter out problematic nodes in stress scenarios (optional - can be disabled)
        # This helps RL learn to avoid bad nodes
        stress_aware_nodes = self._filter_stress_problematic_nodes(filtered_nodes)
        if len(stress_aware_nodes) > 0:
            filtered_nodes = stress_aware_nodes
        
        # Validate action v√† ch·ªçn next node
        if action < 0 or action >= len(filtered_nodes) or not filtered_nodes:
            # Fallback strategy
            next_node = self._find_fallback_node()
            if not next_node:
                # No valid nodes, end episode
                state = self.state_builder.build_state(
                    self.nodes, self.source_terminal, self.dest_terminal,
                    self.current_node, self.service_qos, list(self.visited_nodes)
                )
                return state, self.failure_penalty, True, False, {'error': 'no_valid_nodes'}
        else:
            next_node = filtered_nodes[action]
        
        # Loop detection
        next_node_id = next_node.get('nodeId')
        if next_node_id in self.visited_nodes:
            # Loop penalty
            reward = -20.0
            terminated = False
            truncated = self.step_count >= self.max_steps
            
            state = self.state_builder.build_state(
                self.nodes, self.source_terminal, self.dest_terminal,
                self.current_node, self.service_qos, list(self.visited_nodes)
            )
            
            info = {
                'path': self.path.copy(),
                'loop_detected': True,
                'current_node': next_node_id,
                'hops': len(self.path) - 1
            }
            return state, reward, terminated, truncated, info
        
        # Th√™m node v√†o path
        self.path.append(next_node)
        self.visited_nodes.add(next_node_id)
        
        # T√≠nh metrics cho hop n√†y
        current_pos = self.current_node.get('position')
        next_pos = next_node.get('position')
        hop_distance = self._calculate_distance(current_pos, next_pos)
        self.total_distance += hop_distance
        
        # T√≠nh latency
        speed_of_light = 299792458
        propagation_delay = (hop_distance / speed_of_light) * 1000
        processing_delay = next_node.get('nodeProcessingDelayMs', 5)
        hop_latency = propagation_delay + processing_delay
        self.total_latency += hop_latency
        
        # Get node types for reward calculation
        current_node_type = self.current_node.get('nodeType', '')
        next_node_type = next_node.get('nodeType', '')
        
        # Get connection counts ƒë·ªÉ t√≠nh utilization th·ª±c t·∫ø
        current_connection_count = get_terminal_connection_count(self.current_node.get('nodeId'))
        next_connection_count = get_terminal_connection_count(next_node.get('nodeId'))
        
        # T√≠nh utilization th·ª±c t·∫ø (m·ªói terminal ~4-10% utilization)
        current_node_utilization = self.current_node.get('resourceUtilization', 0) + (current_connection_count * 7.0)
        next_node_utilization = next_node.get('resourceUtilization', 0) + (next_connection_count * 7.0)
        
        # Penalty ƒë·∫∑c bi·ªát cho ground station hops - LU√îN PENALTY tr·ª´ khi load balancing
        initial_reward = 0.0
        if current_node_type == 'GROUND_STATION' and next_node_type == 'GROUND_STATION':
            # LU√îN penalty GS‚ÜíGS, tr·ª´ khi current GS qu√° t·∫£i V√Ä next GS √≠t t·∫£i h∆°n
            if current_node_utilization > 80.0 and next_node_utilization < current_node_utilization - 20.0:
                # Load balancing case: current GS qu√° t·∫£i, next GS t·ªët h∆°n
                initial_reward = 5.0  # Bonus cho load balancing
                logger.debug(f"‚úÖ Load balancing bonus: current={current_node_utilization:.1f}%, next={next_node_utilization:.1f}%")
            else:
                # Normal case: LU√îN penalty GS‚ÜíGS
                initial_reward = self.ground_station_hop_penalty  # -15.0 (tƒÉng t·ª´ -5.0)
                logger.debug(f"‚ö†Ô∏è GS‚ÜíGS penalty: {initial_reward}")
        
        # Ki·ªÉm tra destination reached
        dest_pos = self.dest_terminal.get('position')
        dist_to_dest = self._calculate_distance(next_pos, dest_pos)
        
        # üî• ENHANCED: Check if we reached dest ground station explicitly
        reached_dest_gs = False
        if hasattr(self, 'dest_ground_station') and self.dest_ground_station:
            reached_dest_gs = (next_node_id == self.dest_ground_station['nodeId'])
        
        # ƒêi·ªÅu ki·ªán success - STRICT: Ch·ªâ accept khi th·ª±c s·ª± ƒë·∫øn destination GS
        is_ground_station = next_node_type == 'GROUND_STATION'
        is_near_dest = dist_to_dest < 500000  # Tightened: Within 500km only
        
        terminated = False
        reward = initial_reward  # Start with ground station hop penalty if applicable
        
        has_min_hops = len(self.path) >= 3  # √çt nh·∫•t 2 hops (source GS + 1 satellite + current)
        
        # üéØ STRICT SUCCESS: Ch·ªâ accept khi:
        # 1. Reached exact dest GS (best case)
        # 2. GS node AND very close to destination (<500km)
        # 3. Has minimum hops AND close to destination (<1000km)
        if reached_dest_gs or \
           (is_ground_station and is_near_dest and has_min_hops) or \
           (has_min_hops and dist_to_dest < 1000000):  # Tightened t·ª´ 2000km xu·ªëng 1000km
            # Success!
            self.path.append(self.dest_terminal)
            terminated = True
            self.terminated = True  # Mark as successfully terminated
            
            # Base success reward
            reward = self.success_reward
            
            # üî• BONUS: Extra reward if reached exact dest GS
            if reached_dest_gs:
                reward += 50.0
                logger.info(f"üéØ RL reached exact destination GS: {self.dest_ground_station['nodeId']}")
            
            # QoS compliance bonus
            if self.service_qos:
                max_latency = self.service_qos.get('maxLatencyMs', float('inf'))
                if self.total_latency <= max_latency:
                    reward += 30.0  # QoS bonus
                else:
                    reward -= 15.0  # QoS violation penalty
            
            # Path efficiency bonus/penalty - M·ª§C TI√äU S·ªê 1
            num_hops = len(self.path) - 2
            optimal_hops = self._estimate_optimal_hops()
            
            if num_hops <= optimal_hops:
                efficiency_bonus = (optimal_hops - num_hops) * 20.0  # TƒÇNG M·∫†NH: 10.0 ‚Üí 20.0: TH∆Ø·ªûNG C·ª∞C L·ªöN cho path ng·∫Øn
                reward += efficiency_bonus
            else:
                efficiency_penalty = (num_hops - optimal_hops) * 15.0  # TƒÇNG M·∫†NH: 5.0 ‚Üí 15.0: PENALTY C·ª∞C L·ªöN cho path d√†i
                reward -= efficiency_penalty
                
            # üî• EXTRA PENALTY cho paths qu√° d√†i (>5 hops) - GI·∫¢M threshold
            if num_hops > 5:  # Gi·∫£m t·ª´ 6 xu·ªëng 5
                extra_penalty = (num_hops - 5) ** 2 * 30.0  # TƒÇNG: 20.0 ‚Üí 30.0
                reward -= extra_penalty
                logger.warning(f"‚ö†Ô∏è Path too long: {num_hops} hops, extra penalty: -{extra_penalty}")
                
            # Distance efficiency - M·ª§C TI√äU S·ªê 1
            direct_distance = self._calculate_distance(
                self.source_terminal.get('position'),
                self.dest_terminal.get('position')
            )
            # üÜï FIX: Prevent ZeroDivisionError when source = destination
            if direct_distance > 0:
                distance_ratio = self.total_distance / direct_distance
                if distance_ratio < 1.2:  # R·∫•t hi·ªáu qu·∫£ (<20% detour)
                    reward += 30.0  # TƒÉng t·ª´ 20.0: TH∆ØNG L·ªöN cho ƒë∆∞·ªùng th·∫≥ng
                elif distance_ratio < 1.5:  # Hi·ªáu qu·∫£ (<50% detour)
                    reward += 15.0  # TƒÉng t·ª´ 10.0
                elif distance_ratio > 3.0:  # Qu√° v√≤ng (>200% detour)
                    reward -= 20.0  # TƒÉng t·ª´ 10.0: PENALTY L·ªöN cho ƒë∆∞·ªùng d√†i
            else:
                # Source = Destination (direct_distance = 0), max bonus
                reward += 50.0
                
        else:
            # Still routing - t√≠nh progressive reward v·ªõi proximity bonus
            prev_dist = self._calculate_distance(
                self.current_node.get('position'), dest_pos
            )
            progress = prev_dist - dist_to_dest
            
            # Progressive rewards v·ªõi detour penalty
            if progress > 0:
                # Progress reward - khuy·∫øn kh√≠ch ti·∫øn g·∫ßn destination
                reward += progress / 100000.0 * self.progress_reward_scale  # Scale ƒë√£ gi·∫£m xu·ªëng 80.0
            else:
                # üî• DETOUR PENALTY: ƒêi xa destination = penalty M·ª∞C N·∫∂NG
                detour_penalty = abs(progress) / 50000.0 * 30.0  # Penalty l·ªõn h∆°n progress reward
                reward -= detour_penalty
                logger.debug(f"‚ö†Ô∏è Detour penalty: -{detour_penalty:.2f} (moved away from dest by {abs(progress)/1000:.1f}km)")
            
            # Distance penalty
            reward -= hop_distance / 10000000.0 * self.distance_reward_scale
            # Step v√† hop penalties (tƒÉng ƒë·ªÉ tr√°nh qu√° nhi·ªÅu hops)
            reward += self.step_penalty  # Full penalty cho m·ªói step
            reward += self.hop_penalty  # Full penalty cho m·ªói hop
            
            # Satellite bonus GI·∫¢M M·∫†NH: ∆Øu ti√™n satellites nh∆∞ng KH√îNG override hop penalty
            # Net effect: satellite hop = -15 (hop) + 5 (satellite) = -10 (v·∫´n penalty)
            if next_node_type in ['LEO_SATELLITE', 'MEO_SATELLITE', 'GEO_SATELLITE']:
                satellite_bonus = 3.0  # GI·∫¢M t·ª´ 15.0 ‚Üí 3.0 - Ch·ªâ bonus nh·ªè
                if next_node_type == 'LEO_SATELLITE':
                    satellite_bonus = 5.0  # GI·∫¢M t·ª´ 20.0 ‚Üí 5.0 (LEO t·ªët h∆°n nh∆∞ng v·∫´n b·ªã hop penalty)
                elif next_node_type == 'MEO_SATELLITE':
                    satellite_bonus = 4.0  # GI·∫¢M t·ª´ 18.0 ‚Üí 4.0
                reward += satellite_bonus
                logger.debug(f"‚úÖ Satellite hop bonus: {satellite_bonus} for {next_node_type} (net v·ªõi hop penalty: {satellite_bonus - 15.0})")
            
            # Penalty tƒÉng d·∫ßn cho nhi·ªÅu hops (exponential penalty) - C·ª∞C K·ª≤ NGHI√äM KH·∫ÆC
            num_hops = len(self.path) - 1
            if num_hops > 3:  # GI·∫¢M threshold t·ª´ 4 xu·ªëng 3 - Force RL h·ªçc ƒë∆∞·ªùng ng·∫Øn
                excess_hops = num_hops - 3
                excess_penalty = excess_hops * excess_hops * 20.0  # TƒÇNG: 10.0 ‚Üí 20.0 - PENALTY C·ª∞C L·ªöN
                reward -= excess_penalty
                logger.debug(f"‚ö†Ô∏è Excess hops penalty: -{excess_penalty} for {num_hops} hops (threshold=3)")
            
            # Proximity bonus - th∆∞·ªüng khi ƒë·∫øn g·∫ßn destination (tƒÉng scale)
            if dist_to_dest < 1000000:  # Trong 1000km
                proximity_bonus = (1000000 - dist_to_dest) / 1000000.0 * self.proximity_bonus_scale * 2.0
                reward += proximity_bonus
            elif dist_to_dest < 2000000:  # Trong 2000km
                proximity_bonus = (2000000 - dist_to_dest) / 2000000.0 * self.proximity_bonus_scale
                reward += proximity_bonus
            
            # Node quality reward - M·ª§C TI√äU TH·ª® 2 (sau khi gi·∫£m hop/distance)
            node_quality = self.state_builder._compute_node_quality(next_node)
            quality_reward = node_quality * self.quality_reward_scale  # 0-10.0 points
            reward += quality_reward
            
            # Extra bonus for EXCELLENT nodes (quality > 0.8) - Gi·∫£m ƒë·ªÉ kh√¥ng override hop penalty
            if node_quality > 0.8:
                excellent_bonus = 5.0  # Gi·∫£m t·ª´ 15.0 ‚Üí 5.0
                reward += excellent_bonus
                logger.debug(f"‚ú® Excellent node bonus: {excellent_bonus} (quality={node_quality:.2f})")
            # Bonus for GOOD nodes (quality > 0.6)
            elif node_quality > 0.6:
                good_bonus = 3.0  # Gi·∫£m t·ª´ 8.0 ‚Üí 3.0
                reward += good_bonus
                logger.debug(f"‚úÖ Good node bonus: {good_bonus} (quality={node_quality:.2f})")
            # Penalty for BAD nodes (quality < 0.3) - GI·ªÆNGUY√äN v√¨ tr√°nh node t·ªìi v·∫´n quan tr·ªçng
            elif node_quality < 0.3:
                bad_penalty = -20.0  # Gi·ªØ nguy√™n
                reward += bad_penalty
                logger.debug(f"‚ùå Bad node penalty: {bad_penalty} (quality={node_quality:.2f})")
            
            # Resource utilization penalty - S·ª¨ D·ª§NG UTILIZATION TH·ª∞C T·∫æ
            # Nhi·ªÅu terminals quanh GS ‚Üí utilization cao ‚Üí RL n√™n t√¨m ƒë∆∞·ªùng v√≤ng qua GS kh√°c
            # next_node_utilization ƒë√£ ƒë∆∞·ª£c t√≠nh ·ªü tr√™n (bao g·ªìm connection count)
            estimated_utilization = min(100.0, next_node_utilization)
            
            if estimated_utilization > 90:
                reward -= 40.0  # TƒÉng t·ª´ 30 ‚Üí 40 - R·∫§T NGUY HI·ªÇM
            elif estimated_utilization > 80:
                reward -= 25.0  # TƒÉng t·ª´ 20 ‚Üí 25 - Nguy hi·ªÉm
            elif estimated_utilization > 70:
                reward -= 15.0  # TƒÉng t·ª´ 12 ‚Üí 15 - C·∫£nh b√°o cao
            elif estimated_utilization > 60:
                reward -= 8.0   # Gi·ªØ nguy√™n - C·∫£nh b√°o
            elif estimated_utilization < 30:
                reward += 10.0  # Gi·ªØ nguy√™n: Bonus cho node √≠t t·∫£i
            
            # Bonus/Penalty d·ª±a tr√™n s·ªë terminals (cho GS)
            if next_node_type == 'GROUND_STATION':
                if next_connection_count <= 2:
                    reward += 8.0  # TƒÉng t·ª´ 5 ‚Üí 8: Bonus l·ªõn cho GS √≠t t·∫£i
                elif next_connection_count <= 5:
                    reward += 3.0  # TƒÉng t·ª´ 2 ‚Üí 3: Bonus cho GS t·∫£i v·ª´a
                elif next_connection_count > 15:
                    reward -= 25.0  # TƒÉng t·ª´ 10 ‚Üí 25: Penalty R·∫§T L·ªöN cho GS qu√° t·∫£i
                elif next_connection_count > 10:
                    reward -= 15.0  # Penalty l·ªõn cho GS t·∫£i cao
                
            # Battery level penalty - tr√°nh nodes c√≥ battery th·∫•p
            battery_level = next_node.get('batteryChargePercent', 100)
            if battery_level < 20:
                reward -= 10.0  # Battery r·∫•t th·∫•p - penalty l·ªõn
            elif battery_level < 30:
                reward -= 5.0  # Battery th·∫•p - penalty v·ª´a
            elif battery_level < 50:
                reward -= 2.0  # Battery trung b√¨nh - penalty nh·ªè
                
            # Loss rate penalty - tƒÉng penalty M·∫†Nh
            loss_rate = next_node.get('packetLossRate', 0)
            if loss_rate > 0.1:
                reward -= loss_rate * 50.0  # TƒÉng t·ª´ 20 ‚Üí 50: R·∫•t cao loss - penalty r·∫•t l·ªõn
            elif loss_rate > 0.05:
                reward -= loss_rate * 30.0  # TƒÉng t·ª´ 10 ‚Üí 30: Cao loss - penalty l·ªõn
            elif loss_rate > 0:
                reward -= loss_rate * 10.0  # Penalty cho b·∫•t k·ª≥ loss rate n√†o
        
        # Check truncation - gi·∫£m penalty v√† tƒÉng partial success reward
        truncated = self.step_count >= self.max_steps
        if truncated and not terminated:
            reward += self.failure_penalty
            
            # Partial success reward based on progress - tƒÉng reward
            initial_dist = self._calculate_distance(
                self.source_terminal.get('position'),
                self.dest_terminal.get('position')
            )
            current_dist = dist_to_dest
            if initial_dist > 0:
                progress_made = (initial_dist - current_dist) / initial_dist
                # Th∆∞·ªüng cho b·∫•t k·ª≥ progress n√†o - tƒÉng m·∫°nh
                reward += progress_made * 200.0  # TƒÉng t·ª´ 100.0
                # Bonus n·∫øu ƒë·∫øn g·∫ßn destination - tƒÉng scale
                if dist_to_dest < 500000:  # Trong 500km
                    reward += 100.0  # Bonus l·ªõn cho vi·ªác ƒë·∫øn g·∫ßn
                elif dist_to_dest < 1000000:  # Trong 1000km
                    reward += 50.0
                elif dist_to_dest < 2000000:  # Trong 2000km
                    reward += 25.0
        
        # Update current node
        self.current_node = next_node
        
        # Build next state
        state = self.state_builder.build_state(
            nodes=self.nodes,
            source_terminal=self.source_terminal,
            dest_terminal=self.dest_terminal,
            current_node=self.current_node,
            service_qos=self.service_qos,
            visited_nodes=list(self.visited_nodes)
        )
        
        info = {
            'path': self.path.copy(),
            'current_node': next_node_id,
            'distance_to_dest': dist_to_dest,
            'total_distance': self.total_distance,
            'total_latency': self.total_latency,
            'hops': len(self.path) - 1,
            'terminated': terminated,
            'progress': progress if not terminated else 1.0
        }
        
        return state, reward, terminated, truncated, info
    
    def _find_optimal_initial_node(
        self, 
        source_terminal: Dict, 
        dest_terminal: Dict
    ) -> Optional[Dict]:
        """T√¨m initial node t·ªëi ∆∞u c√¢n b·∫±ng gi·ªØa source v√† destination"""
        source_pos = source_terminal.get('position')
        dest_pos = dest_terminal.get('position')
        
        if not source_pos or not dest_pos:
            return self._find_best_ground_station(source_terminal, self.nodes)
        
        operational_nodes = [
            n for n in self.nodes 
            if n.get('isOperational', True) and n.get('position')
        ]
        
        if not operational_nodes:
            return None
        
        # T√¨m node c√¢n b·∫±ng gi·ªØa kho·∫£ng c√°ch ƒë·∫øn source v√† destination
        def balance_score(node):
            node_pos = node.get('position')
            dist_to_source = self._calculate_distance(node_pos, source_pos)
            dist_to_dest = self._calculate_distance(node_pos, dest_pos)
            
            # ∆Øu ti√™n nodes g·∫ßn source nh∆∞ng kh√¥ng qu√° xa destination
            balance = dist_to_source + dist_to_dest
            # Penalty cho nodes qu√° xa ƒë∆∞·ªùng th·∫≥ng source-dest
            direct_dist = self._calculate_distance(source_pos, dest_pos)
            
            # Fix: Tr√°nh division by zero khi source v√† dest ·ªü c√πng v·ªã tr√≠
            if direct_dist < 1.0:  # N·∫øu qu√° g·∫ßn (< 1m)
                return balance  # Ch·ªâ d√πng t·ªïng kho·∫£ng c√°ch
            
            triangle_ratio = (dist_to_source + dist_to_dest) / direct_dist
            
            return balance * triangle_ratio
        
        return min(operational_nodes, key=balance_score)
    
    def _find_fallback_node(self) -> Optional[Dict]:
        """Fallback strategy khi kh√¥ng c√≥ valid actions"""
        # ∆Øu ti√™n ground stations g·∫ßn destination
        fallback_node = self._find_best_ground_station(self.dest_terminal, self.nodes)
        if fallback_node:
            return fallback_node
        
        # Fallback ƒë·∫øn node operational b·∫•t k·ª≥ g·∫ßn destination
        operational_nodes = [
            n for n in self.nodes 
            if n.get('isOperational', True) and n.get('position')
        ]
        
        if not operational_nodes:
            return None
            
        dest_pos = self.dest_terminal.get('position')
        return min(
            operational_nodes,
            key=lambda n: self._calculate_distance(
                n.get('position'), dest_pos
            ) if n.get('position') else float('inf')
        )
    
    def _estimate_optimal_hops(self) -> int:
        """∆Ø·ªõc t√≠nh s·ªë hops t·ªëi ∆∞u cho path - STRICT ƒë·ªÉ force shorter paths"""
        direct_dist = self._calculate_distance(
            self.source_terminal.get('position'),
            self.dest_terminal.get('position')
        )
        
        # üî• STRICT: Force RL to learn shortest paths
        # Typical optimal: Terminal ‚Üí GS ‚Üí LEO ‚Üí GS ‚Üí Terminal = 3-4 hops
        avg_hop_dist = 3000000  # 3000km (tƒÉng t·ª´ 2500km ƒë·ªÉ gi·∫£m estimated hops)
        optimal_hops = max(3, int(direct_dist / avg_hop_dist) + 2)  # +2 for GS hops
        
        return min(optimal_hops, 5)  # Max 5 hops (gi·∫£m t·ª´ 6, STRICT!)
    
    def _find_best_ground_station(
        self, terminal: Dict, nodes: List[Dict]
    ) -> Optional[Dict]:
        """T√¨m ground station t·ªët nh·∫•t cho terminal"""
        terminal_pos = terminal.get('position')
        if not terminal_pos:
            return None
        
        ground_stations = [
            n for n in nodes
            if n.get('nodeType') == 'GROUND_STATION'
            and n.get('isOperational', True)
            and n.get('position')
        ]
        
        if not ground_stations:
            return None
        
        # Find closest v·ªõi quality consideration
        best_station = None
        best_score = float('inf')
        
        for station in ground_stations:
            distance = self._calculate_distance(
                terminal_pos, station.get('position')
            )
            quality = self.state_builder._compute_node_quality(station)
            
            # Score k·∫øt h·ª£p distance v√† quality
            score = distance / 1000.0 * (1.1 - quality)  # Higher quality = better
            
            if score < best_score:
                best_score = score
                best_station = station
        
        return best_station
    
    def _calculate_distance(self, pos1: Dict, pos2: Dict) -> float:
        """Calculate distance v·ªõi cache"""
        if not pos1 or not pos2:
            return float('inf')
        
        # S·ª≠ d·ª•ng state builder's cached distance calculation
        return self.state_builder._calculate_distance(pos1, pos2)
    
    def _filter_stress_problematic_nodes(self, nodes: List[Dict]) -> List[Dict]:
        """
        Filter out nodes v·ªõi v·∫•n ƒë·ªÅ nghi√™m tr·ªçng trong stress scenarios
        Gi√∫p RL h·ªçc tr√°nh c√°c nodes c√≥ v·∫•n ƒë·ªÅ
        """
        filtered = []
        for node in nodes:
            # Ch·ªâ filter n·∫øu node c√≥ v·∫•n ƒë·ªÅ nghi√™m tr·ªçng
            utilization = node.get('resourceUtilization', 0)
            battery = node.get('batteryChargePercent', 100)
            is_operational = node.get('isOperational', True)
            packet_loss = node.get('packetLossRate', 0)
            
            # Gi·ªØ node n·∫øu:
            # 1. Operational
            # 2. Kh√¥ng c√≥ qu√° nhi·ªÅu v·∫•n ƒë·ªÅ c√πng l√∫c
            if is_operational:
                # Ch·ªâ filter n·∫øu c√≥ nhi·ªÅu v·∫•n ƒë·ªÅ c√πng l√∫c
                problem_count = 0
                if utilization > 0.9:
                    problem_count += 1
                if battery < 0.15:
                    problem_count += 1
                if packet_loss > 0.1:
                    problem_count += 1
                
                # Ch·ªâ filter n·∫øu c√≥ 2+ v·∫•n ƒë·ªÅ nghi√™m tr·ªçng
                if problem_count < 2:
                    filtered.append(node)
        
        # N·∫øu filter qu√° nhi·ªÅu, gi·ªØ l·∫°i m·ªôt s·ªë nodes t·ªët nh·∫•t
        if len(filtered) < 3 and len(nodes) > 0:
            # Sort by quality v√† gi·ªØ top nodes
            nodes_sorted = sorted(
                nodes,
                key=lambda n: (
                    -n.get('resourceUtilization', 0),  # Lower is better
                    -n.get('batteryChargePercent', 100),  # Higher is better
                    n.get('packetLossRate', 0)  # Lower is better
                )
            )
            filtered = nodes_sorted[:max(3, len(nodes) // 2)]
        
        return filtered
    
    def get_path_result(self) -> Dict:
        """Get final path result - ƒë·∫£m b·∫£o format ƒë√∫ng v√† ƒë·∫ßy ƒë·ªß"""
        if not self.path or len(self.path) < 2:
            # Return empty path if no path found
            return {
                'source': {
                    'terminalId': self.source_terminal.get('terminalId') if self.source_terminal else '',
                    'position': self.source_terminal.get('position') if self.source_terminal else {}
                },
                'destination': {
                    'terminalId': self.dest_terminal.get('terminalId') if self.dest_terminal else '',
                    'position': self.dest_terminal.get('position') if self.dest_terminal else {}
                },
                'path': [],
                'totalDistance': 0,
                'estimatedLatency': 0,
                'hops': 0,
                'algorithm': 'rl_optimized',
                'success': False
            }
        
        # Build path segments - ƒë·∫£m b·∫£o c√≥ source terminal ·ªü ƒë·∫ßu
        path_segments = []
        
        # Always start with source terminal
        if self.source_terminal:
            path_segments.append({
                'type': 'terminal',
                'id': self.source_terminal.get('terminalId'),
                'name': self.source_terminal.get('terminalName', self.source_terminal.get('terminalId')),
                'position': self.source_terminal.get('position')
            })
        
        # Add all nodes from path (skip source terminal if it's already in path)
        for item in self.path:
            if 'terminalId' in item:
                # Skip source terminal if already added
                if item.get('terminalId') == self.source_terminal.get('terminalId'):
                    continue
                # Add destination terminal
                path_segments.append({
                    'type': 'terminal',
                    'id': item.get('terminalId'),
                    'name': item.get('terminalName', item.get('terminalId')),
                    'position': item.get('position')
                })
            elif 'nodeId' in item:
                path_segments.append({
                    'type': 'node',
                    'id': item.get('nodeId'),
                    'name': item.get('nodeName', item.get('nodeId')),
                    'position': item.get('position')
                })
        
        # Always end with destination terminal if not already there
        if (not path_segments or 
            path_segments[-1].get('id') != self.dest_terminal.get('terminalId')):
            path_segments.append({
                'type': 'terminal',
                'id': self.dest_terminal.get('terminalId'),
                'name': self.dest_terminal.get('terminalName', self.dest_terminal.get('terminalId')),
                'position': self.dest_terminal.get('position')
            })
        
        # Calculate total metrics
        total_distance = 0.0
        total_latency = 0.0
        
        for i in range(len(path_segments) - 1):
            pos1 = path_segments[i].get('position')
            pos2 = path_segments[i + 1].get('position')
            if pos1 and pos2:
                dist = self._calculate_distance(pos1, pos2)
                total_distance += dist
                
                speed_of_light = 299792458
                propagation_delay = (dist / speed_of_light) * 1000
                processing_delay = 5  # Default processing delay
                total_latency += propagation_delay + processing_delay
        
        # Check if path successfully reached destination
        is_success = self.terminated and len(path_segments) >= 4  # At least: source_terminal, source_node, dest_node, dest_terminal
        
        return {
            'source': {
                'terminalId': self.source_terminal.get('terminalId'),
                'position': self.source_terminal.get('position')
            },
            'destination': {
                'terminalId': self.dest_terminal.get('terminalId'),
                'position': self.dest_terminal.get('position')
            },
            'path': path_segments,
            'totalDistance': round(total_distance / 1000, 2),
            'estimatedLatency': round(total_latency, 2),
            'hops': len(path_segments) - 1,
            'algorithm': 'rl_optimized',
            'success': is_success
        }
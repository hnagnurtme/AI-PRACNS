# SAGIN Routing System v·ªõi Reinforcement Learning

## üìã M·ª•c L·ª•c
1. [Gi·ªõi Thi·ªáu T·ªïng Quan](#gi·ªõi-thi·ªáu-t·ªïng-quan)
2. [Ki·∫øn Tr√∫c H·ªá Th·ªëng](#ki·∫øn-tr√∫c-h·ªá-th·ªëng)
3. [C√¥ng Ngh·ªá S·ª≠ D·ª•ng](#c√¥ng-ngh·ªá-s·ª≠-d·ª•ng)
4. [C√†i ƒê·∫∑t v√† C·∫•u H√¨nh](#c√†i-ƒë·∫∑t-v√†-c·∫•u-h√¨nh)
5. [Reinforcement Learning Agent](#reinforcement-learning-agent)
6. [API Endpoints](#api-endpoints)
7. [Simulation Scenarios](#simulation-scenarios)
8. [Training Model](#training-model)
9. [K·∫øt Qu·∫£ v√† ƒê√°nh Gi√°](#k·∫øt-qu·∫£-v√†-ƒë√°nh-gi√°)
10. [Troubleshooting](#troubleshooting)

---

## üåê Gi·ªõi Thi·ªáu T·ªïng Quan

### M·ª•c Ti√™u D·ª± √Ån

H·ªá th·ªëng **SAGIN (Space-Air-Ground Integrated Network) Routing** l√† m·ªôt gi·∫£i ph√°p ƒë·ªãnh tuy·∫øn th√¥ng minh s·ª≠ d·ª•ng **Deep Reinforcement Learning** ƒë·ªÉ t·ªëi ∆∞u h√≥a vi·ªác truy·ªÅn d·ªØ li·ªáu trong m·∫°ng l∆∞·ªõi t√≠ch h·ª£p kh√¥ng gian-kh√¥ng trung-m·∫∑t ƒë·∫•t.

### V·∫•n ƒê·ªÅ Gi·∫£i Quy·∫øt

1. **ƒê·ªãnh tuy·∫øn ƒë·ªông (Dynamic Routing)**:
   - M·∫°ng SAGIN c√≥ topology thay ƒë·ªïi li√™n t·ª•c do chuy·ªÉn ƒë·ªông c·ªßa v·ªá tinh
   - C√°c thu·∫≠t to√°n truy·ªÅn th·ªëng (Dijkstra, Bellman-Ford) kh√¥ng t·ªëi ∆∞u cho m√¥i tr∆∞·ªùng ƒë·ªông
   
2. **Multi-Objective Optimization**:
   - C√¢n b·∫±ng gi·ªØa ƒë·ªô tr·ªÖ (latency), kho·∫£ng c√°ch (distance), v√† ƒë·ªô tin c·∫≠y (reliability)
   - ƒê√°p ·ª©ng y√™u c·∫ßu QoS (Quality of Service) kh√°c nhau c·ªßa t·ª´ng lo·∫°i d·ªãch v·ª•

3. **Resource Management**:
   - T·ªëi ∆∞u h√≥a s·ª≠ d·ª•ng t√†i nguy√™n (bandwidth, battery, processing power)
   - Ph√¢n b·ªï t·∫£i c√¢n b·∫±ng gi·ªØa c√°c nodes

### ƒê√≥ng G√≥p Ch√≠nh

- **Thu·∫≠t to√°n Dueling DQN** cho routing ƒë·ªông trong SAGIN
- **State representation** t·ªëi ∆∞u cho network v·ªõi 30+ nodes
- **Multi-scenario simulation** m√¥ ph·ªèng c√°c ƒëi·ªÅu ki·ªán m·∫°ng kh√°c nhau
- **REST API** ƒë·∫ßy ƒë·ªß cho t√≠ch h·ª£p v√† monitoring
- **Web-based visualization** v·ªõi Cesium 3D Globe

---

## Ki·∫øn Tr√∫c H·ªá Th·ªëng

### 1. **Agent: DuelingDQNAgent**
- **File**: `agent/dueling_dqn.py`
- **Ch·ª©c nƒÉng**: Neural network agent h·ªçc policy routing
- **Architecture**: 
  - Shared feature layers: [512, 256, 128] neurons
  - Value stream: ∆Ø·ªõc l∆∞·ª£ng V(s)
  - Advantage stream: ∆Ø·ªõc l∆∞·ª£ng A(s,a)
  - Output: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))

### 2. **Environment: RoutingEnvironment**
- **File**: `environment/routing_env.py`
- **Ch·ª©c nƒÉng**: M√¥i tr∆∞·ªùng m√¥ ph·ªèng SAGIN network
- **Action space**: Discrete (ch·ªçn next node t·ª´ danh s√°ch available nodes)
- **Observation space**: State vector v·ªõi k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh

### 3. **State Builder: RoutingStateBuilder**
- **File**: `environment/state_builder.py`
- **Ch·ª©c nƒÉng**: X√¢y d·ª±ng state vector t·ª´ network state
- **State dimension**: 
  - Node features: 30 nodes √ó 12 features = 360
  - Terminal features: 2 terminals √ó 6 features = 12
  - Global features: 8
  - **T·ªïng**: 380 dimensions

### 4. **Trainer: RoutingTrainer / EnhancedRoutingTrainer**
- **File**: `training/trainer.py`, `training/enhanced_trainer.py`
- **Ch·ª©c nƒÉng**: Qu·∫£n l√Ω training loop, evaluation, checkpointing

---

## Thu·∫≠t To√°n v√† C√¥ng Th·ª©c

### 1. Dueling DQN Architecture

#### C√¥ng th·ª©c Q-value:
```
Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
```

Trong ƒë√≥:
- **V(s)**: State value - gi√° tr·ªã c·ªßa state s
- **A(s,a)**: Advantage - l·ª£i th·∫ø c·ªßa action a so v·ªõi c√°c actions kh√°c
- **mean(A(s,a))**: Trung b√¨nh c·ªßa t·∫•t c·∫£ advantages ƒë·ªÉ ƒë·∫£m b·∫£o identifiability

#### L√Ω do s·ª≠ d·ª•ng Dueling DQN:
- Trong routing, nhi·ªÅu actions c√≥ gi√° tr·ªã t∆∞∆°ng ƒë∆∞∆°ng (v√≠ d·ª•: ch·ªçn satellite A hay B khi c·∫£ hai ƒë·ªÅu t·ªët)
- T√°ch V(s) v√† A(s,a) gi√∫p network h·ªçc ƒë∆∞·ª£c r·∫±ng "state n√†y t·ªët" m√† kh√¥ng c·∫ßn bi·∫øt action c·ª• th·ªÉ n√†o t·ªët nh·∫•t

### 2. Bellman Equation (DQN Update)

#### Target Q-value:
```
Q_target(s,a) = r + Œ≥ * max_a' Q_target(s', a')
```

#### Loss Function (Huber Loss):
```
L = smooth_l1_loss(Q_current(s,a) - Q_target(s,a))
```

V·ªõi **Double DQN** (ƒë·ªÉ gi·∫£m overestimation):
```
Q_target(s,a) = r + Œ≥ * Q_target(s', argmax_a' Q_current(s', a'))
```

### 3. Experience Replay

#### Standard Replay Buffer:
- L∆∞u tr·ªØ experiences: (s, a, r, s', done)
- Sample ng·∫´u nhi√™n batch ƒë·ªÉ train
- Gi√∫p break correlation gi·ªØa consecutive experiences

#### Prioritized Experience Replay (Optional):
- ∆Øu ti√™n sample c√°c experiences c√≥ TD-error cao
- C√¥ng th·ª©c priority:
```
priority = |TD_error|^Œ±
```
- Importance sampling weights:
```
w_i = (N * P(i))^(-Œ≤) / max(w)
```

### 4. Epsilon-Greedy Exploration

#### Epsilon Decay:
```
Œµ(t) = max(Œµ_min, Œµ_start * decay^t)
```

Trong ƒë√≥:
- `Œµ_start = 1.0`: B·∫Øt ƒë·∫ßu v·ªõi 100% exploration
- `Œµ_min = 0.01`: K·∫øt th√∫c v·ªõi 1% exploration
- `decay = 0.9995`: T·ªëc ƒë·ªô gi·∫£m

### 5. Target Network Update

#### Hard Update (m·ªói C steps):
```
Œ∏_target ‚Üê Œ∏_current
```

#### Soft Update (m·ªói step):
```
Œ∏_target ‚Üê œÑ * Œ∏_current + (1 - œÑ) * Œ∏_target
```

V·ªõi `œÑ = 0.005` (tau) ƒë·ªÉ update m∆∞·ª£t m√† h∆°n.

### 6. Learning Starts (Warm-up Period) - CRITICAL

**Learning Starts** l√† s·ªë l∆∞·ª£ng experiences t·ªëi thi·ªÉu c·∫ßn c√≥ trong replay buffer tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu training. ƒê√¢y l√† m·ªôt **critical parameter** quan tr·ªçng.

#### T·∫°i Sao C·∫ßn Learning Starts:

1. **ƒê·∫£m B·∫£o ƒêa D·∫°ng**: C·∫ßn ƒë·ªß experiences ƒëa d·∫°ng ƒë·ªÉ h·ªçc hi·ªáu qu·∫£
2. **Tr√°nh Overfitting**: Tr√°nh h·ªçc t·ª´ qu√° √≠t samples ‚Üí overfitting
3. **Stable Training**: ƒê·∫£m b·∫£o replay buffer c√≥ ƒë·ªß samples ƒë·ªÉ sample batch

#### C√¥ng Th·ª©c:

```python
if len(replay_buffer) < learning_starts:
    return None  # Ch∆∞a train, ch·ªâ collect experiences
```

#### Gi√° Tr·ªã Khuy·∫øn Ngh·ªã:

```yaml
rl_agent:
  dqn:
    learning_starts: 5000  # T·ªëi thi·ªÉu 5000 experiences tr∆∞·ªõc khi train
```

**L∆∞u √Ω Critical**:
- ‚ö†Ô∏è **Qu√° th·∫•p** (< 1000): Model h·ªçc t·ª´ qu√° √≠t samples ‚Üí unstable, overfitting
- ‚ö†Ô∏è **Qu√° cao** (> 10000): T·ªën th·ªùi gian ch·ªù ƒë·ª£i, kh√¥ng c·∫ßn thi·∫øt
- ‚úÖ **Khuy·∫øn ngh·ªã**: 5000-10000 cho m√¥i tr∆∞·ªùng ph·ª©c t·∫°p nh∆∞ SAGIN

### 7. Reward Function

#### C√°c th√†nh ph·∫ßn reward:

**1. Success Reward:**
```
R_success = 200.0 (n·∫øu ƒë·∫øn ƒë∆∞·ª£c destination)
```

**2. Progressive Rewards:**
```
R_progress = (distance_reduced / 100000) * progress_reward_scale
R_distance = -hop_distance / 10000000 * distance_reward_scale
```

**3. Node Quality Rewards:**
```
R_quality = node_quality_score * quality_reward_scale
R_satellite = +8.0 (n·∫øu ch·ªçn satellite)
R_leo = +12.0 (n·∫øu ch·ªçn LEO satellite)
```

**4. Resource Penalties:**
```
R_utilization = -penalty n·∫øu utilization > threshold
R_battery = -penalty n·∫øu battery < threshold
R_loss = -loss_rate * penalty_scale
```

**5. Path Efficiency:**
```
R_efficiency = +bonus n·∫øu hops <= optimal_hops
R_inefficiency = -penalty n·∫øu hops > optimal_hops
```

**6. QoS Compliance:**
```
R_qos = +30.0 n·∫øu latency <= max_latency
R_qos = -15.0 n·∫øu latency > max_latency
```

### 7. State Representation

#### Node Features (12 dimensions):
1. Resource utilization (0-1)
2. Packet buffer usage (0-1)
3. Packet loss rate (0-1)
4. Battery level (0-1)
5. Processing delay (0-1)
6. Bandwidth (0-1)
7. Is operational (0/1)
8. Is visited (0/1)
9. Distance to destination (normalized)
10. Distance to current node (normalized)
11. Node type encoding (0.2/0.5/0.8)
12. Quality score (0-1)

#### Terminal Features (6 dimensions):
1-3. Source terminal position (lat, lon, alt)
4-6. Destination terminal position (lat, lon, alt)

#### Global Features (8 dimensions):
1. Average network utilization
2. Average packet loss rate
3. Network congestion ratio
4. Operational nodes ratio
5. Current node utilization
6. Current node loss rate
7. Progress indicator (visited nodes ratio)
8. Scenario type (normal/congestion/failure)

---

## C√†i ƒê·∫∑t v√† Chu·∫©n B·ªã

### 1. Y√™u C·∫ßu H·ªá Th·ªëng

- **Python**: >= 3.8
- **PyTorch**: >= 2.1.0 (v·ªõi CUDA n·∫øu c√≥ GPU)
- **MongoDB**: ƒê·ªÉ l∆∞u tr·ªØ network topology
- **RAM**: T·ªëi thi·ªÉu 8GB (khuy·∫øn ngh·ªã 16GB+)
- **GPU**: Kh√¥ng b·∫Øt bu·ªôc nh∆∞ng khuy·∫øn ngh·ªã (NVIDIA v·ªõi CUDA)

### 2. C√†i ƒê·∫∑t Dependencies

```bash
cd Backend
pip install -r requirements.txt
```

### 3. C·∫•u H√¨nh MongoDB

ƒê·∫£m b·∫£o MongoDB ƒëang ch·∫°y v√† c√≥ d·ªØ li·ªáu:
- **Nodes**: C√°c nodes trong m·∫°ng (satellites, ground stations, etc.)
- **Terminals**: C√°c terminals c·∫ßn routing

Ki·ªÉm tra k·∫øt n·ªëi:
```python
from models.database import db
db.connect()
nodes = list(db.get_collection('nodes').find({'isOperational': True}))
print(f"Found {len(nodes)} operational nodes")
```

### 4. C·∫•u H√¨nh File Config

Ch·ªânh s·ª≠a `config.dev.yaml` ho·∫∑c `config.pro.yaml`:

```yaml
mongodb:
  uri: "mongodb://admin:password@localhost:27017/aiprancs?authSource=admin"
  database: "aiprancs"

rl_agent:
  dqn:
    learning_rate: 0.0001
    batch_size: 64
    buffer_size: 100000
    gamma: 0.99

training:
  max_episodes: 2000
  max_steps_per_episode: 15
  eval_frequency: 50
```

---

## H∆∞·ªõng D·∫´n Training

### 1. Training C∆° B·∫£n

#### B∆∞·ªõc 1: Kh·ªüi ƒë·ªông training
```bash
cd Backend
python -m training.train
```

#### B∆∞·ªõc 2: Training v·ªõi s·ªë episodes t√πy ch·ªânh
```bash
python -m training.train --episodes 3000
```

#### B∆∞·ªõc 3: Training v·ªõi config file t√πy ch·ªânh
```bash
python -m training.train --config custom_config.yaml
```

#### B∆∞·ªõc 4: Resume t·ª´ checkpoint
```bash
python -m training.train --resume
```

### 2. Training v·ªõi Enhanced Trainer

Enhanced Trainer bao g·ªìm:
- **Curriculum Learning**: T·ª´ d·ªÖ ƒë·∫øn kh√≥
- **Imitation Learning**: H·ªçc t·ª´ expert (Dijkstra)
- **Multi-objective Optimization**: T·ªëi ∆∞u nhi·ªÅu m·ª•c ti√™u

ƒê·ªÉ s·ª≠ d·ª•ng Enhanced Trainer, set trong config:
```yaml
training:
  use_enhanced_trainer: true
```

Ho·∫∑c ch·ªânh trong code:
```python
from training.enhanced_trainer import EnhancedRoutingTrainer
trainer = EnhancedRoutingTrainer(config)
agent = trainer.train_from_database(num_episodes=2000)
```

### 3. Training Flow Chi Ti·∫øt

```
1. Initialize Environment
   ‚îú‚îÄ‚îÄ Load nodes t·ª´ MongoDB
   ‚îú‚îÄ‚îÄ Load terminals t·ª´ MongoDB
   ‚îî‚îÄ‚îÄ Create RoutingEnvironment

2. Initialize Agent
   ‚îú‚îÄ‚îÄ Create DuelingDQN networks (Q-network + Target network)
   ‚îú‚îÄ‚îÄ Initialize Replay Buffer
   ‚îî‚îÄ‚îÄ Set exploration parameters (epsilon)

3. Training Loop (cho m·ªói episode):
   ‚îú‚îÄ‚îÄ Reset environment
   ‚îÇ   ‚îú‚îÄ‚îÄ Ch·ªçn random source & destination terminals
   ‚îÇ   ‚îî‚îÄ‚îÄ Build initial state
   ‚îÇ
   ‚îú‚îÄ‚îÄ Episode Loop (cho m·ªói step):
   ‚îÇ   ‚îú‚îÄ‚îÄ Select action (epsilon-greedy)
   ‚îÇ   ‚îú‚îÄ‚îÄ Execute action ‚Üí (next_state, reward, done)
   ‚îÇ   ‚îú‚îÄ‚îÄ Store experience v√†o replay buffer
   ‚îÇ   ‚îú‚îÄ‚îÄ Sample batch t·ª´ replay buffer
   ‚îÇ   ‚îú‚îÄ‚îÄ Compute Q-targets
   ‚îÇ   ‚îú‚îÄ‚îÄ Update Q-network (backpropagation)
   ‚îÇ   ‚îú‚îÄ‚îÄ Update target network (m·ªói C steps)
   ‚îÇ   ‚îî‚îÄ‚îÄ Decay epsilon
   ‚îÇ
   ‚îú‚îÄ‚îÄ Evaluation (m·ªói eval_frequency episodes):
   ‚îÇ   ‚îú‚îÄ‚îÄ Run evaluation episodes
   ‚îÇ   ‚îú‚îÄ‚îÄ Compute metrics (success rate, mean reward, etc.)
   ‚îÇ   ‚îî‚îÄ‚îÄ Save best model n·∫øu c·∫£i thi·ªán
   ‚îÇ
   ‚îî‚îÄ‚îÄ Checkpoint (m·ªói save_frequency episodes):
       ‚îî‚îÄ‚îÄ Save model checkpoint

4. Final Evaluation & Save
   ‚îú‚îÄ‚îÄ Run comprehensive evaluation
   ‚îî‚îÄ‚îÄ Save final model
```

### 4. Monitoring Training

#### Tensorboard:
```bash
tensorboard --logdir=./logs/tensorboard
```

M·ªü browser t·∫°i `http://localhost:6006` ƒë·ªÉ xem:
- Training reward
- Loss curves
- Success rate
- Epsilon decay
- Episode length

#### Log Files:
- `training.log`: Console logs
- `logs/tensorboard/`: Tensorboard events

---

## C·∫•u H√¨nh v√† Tham S·ªë

### ‚ö†Ô∏è CRITICAL PARAMETERS - C√°c Tham S·ªë Quan Tr·ªçng Nh·∫•t

Tr∆∞·ªõc khi ƒëi v√†o chi ti·∫øt, ƒë√¢y l√† c√°c tham s·ªë **CRITICAL** (quan tr·ªçng nh·∫•t) m√† b·∫°n **PH·∫¢I** hi·ªÉu v√† tune ƒë√∫ng:

#### 1. **Learning Rate** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Quy·∫øt ƒë·ªãnh t·ªëc ƒë·ªô h·ªçc v√† stability
- **Gi√° tr·ªã**: `0.0001` (default)
- **Qu√° cao** (> 0.001): Training kh√¥ng ·ªïn ƒë·ªãnh, loss explode, NaN
- **Qu√° th·∫•p** (< 0.00001): H·ªçc qu√° ch·∫≠m, kh√¥ng h·ªôi t·ª•
- **Khuy·∫øn ngh·ªã**: B·∫Øt ƒë·∫ßu v·ªõi `0.0001`, tune trong kho·∫£ng `[0.00005, 0.0005]`

#### 2. **Gamma (Discount Factor)** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Quy·∫øt ƒë·ªãnh agent quan t√¢m rewards xa ƒë·∫øn ƒë√¢u
- **Gi√° tr·ªã**: `0.99` (default)
- **Qu√° cao** (> 0.99): Agent quan t√¢m rewards qu√° xa ‚Üí ch·∫≠m h·ªçc
- **Qu√° th·∫•p** (< 0.9): Agent ch·ªâ quan t√¢m immediate rewards ‚Üí kh√¥ng t·ªëi ∆∞u long-term
- **Khuy·∫øn ngh·ªã**: `0.95 - 0.99` cho episodic tasks nh∆∞ routing

#### 3. **Batch Size** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Stability v√† memory usage
- **Gi√° tr·ªã**: `64` (default)
- **Qu√° nh·ªè** (< 32): Unstable gradients, noisy updates
- **Qu√° l·ªõn** (> 256): T·ªën memory, ch·∫≠m training, c√≥ th·ªÉ overfit
- **Khuy·∫øn ngh·ªã**: `32-128` t√πy v√†o GPU memory

#### 4. **Replay Buffer Size** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: ƒêa d·∫°ng c·ªßa training data
- **Gi√° tr·ªã**: `100000` (default)
- **Qu√° nh·ªè** (< 10000): Kh√¥ng ƒë·ªß ƒëa d·∫°ng, overfitting
- **Qu√° l·ªõn** (> 1000000): T·ªën memory, ch·∫≠m sampling
- **Khuy·∫øn ngh·ªã**: `50000-200000` cho m√¥i tr∆∞·ªùng ph·ª©c t·∫°p

#### 5. **Target Update Frequency** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Stability c·ªßa target Q-values
- **Gi√° tr·ªã**: `1000` steps (default)
- **Qu√° th∆∞·ªùng xuy√™n** (< 100): Target network thay ƒë·ªïi qu√° nhanh ‚Üí unstable
- **Qu√° √≠t** (> 5000): Target network qu√° c≈© ‚Üí ch·∫≠m h·ªçc
- **Khuy·∫øn ngh·ªã**: `500-2000` steps

#### 6. **Epsilon Decay** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: C√¢n b·∫±ng exploration vs exploitation
- **Gi√° tr·ªã**: `0.9995` (default)
- **Qu√° nhanh** (> 0.9999): Kh√¥ng explore ƒë·ªß ‚Üí stuck ·ªü local optimum
- **Qu√° ch·∫≠m** (< 0.99): Explore qu√° nhi·ªÅu ‚Üí kh√¥ng exploit knowledge
- **Khuy·∫øn ngh·ªã**: `0.999-0.9999` t√πy v√†o s·ªë episodes

#### 7. **Reward Scale** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Stability v√† t·ªëc ƒë·ªô h·ªçc
- **Gi√° tr·ªã**: `success_reward: 200.0` (default)
- **Qu√° l·ªõn** (> 1000): Q-values explode, training unstable
- **Qu√° nh·ªè** (< 10): Agent kh√¥ng h·ªçc ƒë∆∞·ª£c (rewards qu√° nh·ªè so v·ªõi noise)
- **Khuy·∫øn ngh·ªã**: Gi·ªØ rewards trong kho·∫£ng `[-100, 500]` ƒë·ªÉ stable

#### 8. **State Dimension** (CRITICAL ‚ö†Ô∏è)
- **·∫¢nh h∆∞·ªüng**: Complexity v√† training time
- **Gi√° tr·ªã**: `380` dimensions (default)
- **Qu√° l·ªõn** (> 1000): Training ch·∫≠m, c·∫ßn nhi·ªÅu data
- **Qu√° nh·ªè** (< 100): M·∫•t th√¥ng tin quan tr·ªçng
- **Khuy·∫øn ngh·ªã**: Gi·ªØ trong kho·∫£ng `200-500` cho routing

### 1. Hyperparameters Quan Tr·ªçng (Chi Ti·∫øt)

#### Learning Rate (CRITICAL):
```yaml
rl_agent:
  dqn:
    learning_rate: 0.0001  # Th·∫•p h∆°n = stable h∆°n nh∆∞ng ch·∫≠m h∆°n
```

#### Batch Size (CRITICAL):
```yaml
rl_agent:
  dqn:
    batch_size: 64  # L·ªõn h∆°n = stable h∆°n nh∆∞ng t·ªën memory
```
**‚ö†Ô∏è CRITICAL**: 
- Qu√° nh·ªè (< 32): Unstable, noisy gradients
- Qu√° l·ªõn (> 256): T·ªën memory, c√≥ th·ªÉ overfit
- **Khuy·∫øn ngh·ªã**: 64-128 cho GPU, 32-64 cho CPU

#### Buffer Size (CRITICAL):
```yaml
rl_agent:
  dqn:
    buffer_size: 100000  # L∆∞u tr·ªØ 100k experiences
```
**‚ö†Ô∏è CRITICAL**:
- Qu√° nh·ªè (< 10000): Kh√¥ng ƒë·ªß ƒëa d·∫°ng, overfitting
- Qu√° l·ªõn (> 1000000): T·ªën memory, ch·∫≠m sampling
- **Khuy·∫øn ngh·ªã**: 50000-200000 cho m√¥i tr∆∞·ªùng ph·ª©c t·∫°p

#### Gamma (Discount Factor) (CRITICAL):
```yaml
rl_agent:
  dqn:
    gamma: 0.99  # G·∫ßn 1.0 = quan t√¢m rewards xa h∆°n
```
**‚ö†Ô∏è CRITICAL**:
- Qu√° cao (> 0.99): Agent quan t√¢m rewards qu√° xa ‚Üí ch·∫≠m h·ªçc
- Qu√° th·∫•p (< 0.9): Agent ch·ªâ quan t√¢m immediate rewards
- **Khuy·∫øn ngh·ªã**: 0.95-0.99 cho episodic tasks

#### Target Update Frequency (CRITICAL):
```yaml
rl_agent:
  dqn:
    target_update_interval: 1000  # Update target network m·ªói 1000 steps
```
**‚ö†Ô∏è CRITICAL**:
- Qu√° th∆∞·ªùng xuy√™n (< 100): Target thay ƒë·ªïi qu√° nhanh ‚Üí unstable
- Qu√° √≠t (> 5000): Target qu√° c≈© ‚Üí ch·∫≠m h·ªçc
- **Khuy·∫øn ngh·ªã**: 500-2000 steps

#### Learning Starts (CRITICAL):
```yaml
rl_agent:
  dqn:
    learning_starts: 5000  # T·ªëi thi·ªÉu 5000 experiences tr∆∞·ªõc khi train
```
**‚ö†Ô∏è CRITICAL**:
- Qu√° th·∫•p (< 1000): H·ªçc t·ª´ qu√° √≠t samples ‚Üí unstable, overfitting
- Qu√° cao (> 10000): T·ªën th·ªùi gian ch·ªù ƒë·ª£i kh√¥ng c·∫ßn thi·∫øt
- **Khuy·∫øn ngh·ªã**: 5000-10000 cho m√¥i tr∆∞·ªùng ph·ª©c t·∫°p

### 2. Exploration Parameters

```yaml
rl_agent:
  dqn:
    exploration_initial_eps: 1.0      # B·∫Øt ƒë·∫ßu v·ªõi 100% exploration
    exploration_final_eps: 0.01        # K·∫øt th√∫c v·ªõi 1% exploration
    exploration_decay: 0.9995          # T·ªëc ƒë·ªô decay
```

### 3. Network Architecture

```yaml
rl_agent:
  dqn:
    dueling:
      hidden_dims: [512, 256, 128]    # K√≠ch th∆∞·ªõc c√°c layers
      activation_fn: "elu"             # ELU t·ªët h∆°n ReLU cho DQN
      dropout_rate: 0.1                # Regularization
      use_layer_norm: true            # Training stability
```

### 4. Training Parameters

```yaml
training:
  max_episodes: 2000                  # T·ªïng s·ªë episodes
  max_steps_per_episode: 15           # Max steps m·ªói episode
  eval_frequency: 50                   # Evaluate m·ªói 50 episodes
  eval_episodes: 20                   # S·ªë episodes ƒë·ªÉ evaluate
  save_frequency: 100                 # Save checkpoint m·ªói 100 episodes
  early_stopping_patience: 50          # Early stop n·∫øu kh√¥ng c·∫£i thi·ªán 50 evals
```

### 5. Reward Tuning (CRITICAL)

**‚ö†Ô∏è CRITICAL**: Reward engineering l√† m·ªôt trong nh·ªØng ph·∫ßn quan tr·ªçng nh·∫•t c·ªßa RL. Rewards quy·∫øt ƒë·ªãnh agent h·ªçc g√¨ v√† h·ªçc nh∆∞ th·∫ø n√†o.

```yaml
reward:
  success_reward: 200.0               # Reward khi th√†nh c√¥ng (CRITICAL)
  failure_penalty: -10.0               # Penalty khi th·∫•t b·∫°i
  step_penalty: -1.0                   # Penalty m·ªói step
  hop_penalty: -2.0                    # Penalty m·ªói hop
  progress_reward_scale: 100.0         # Scale cho progress reward (CRITICAL)
  proximity_bonus_scale: 50.0          # Bonus khi ƒë·∫øn g·∫ßn destination
```

#### C√°c Nguy√™n T·∫Øc Quan Tr·ªçng:

1. **Reward Scale Balance**:
   - Success reward ph·∫£i ƒë·ªß l·ªõn ƒë·ªÉ "pay off" cho vi·ªác ho√†n th√†nh task
   - Nh∆∞ng kh√¥ng qu√° l·ªõn ƒë·ªÉ tr√°nh Q-values explode
   - **Khuy·∫øn ngh·ªã**: `success_reward / |failure_penalty| ‚âà 10-20`

2. **Shaped Rewards**:
   - Th√™m intermediate rewards (progress, proximity) ƒë·ªÉ guide learning
   - Gi√∫p agent h·ªçc nhanh h∆°n thay v√¨ ch·ªâ nh·∫≠n reward ·ªü cu·ªëi
   - **Khuy·∫øn ngh·ªã**: `progress_reward_scale` n√™n l·ªõn h∆°n `step_penalty`

3. **Penalty Balance**:
   - Penalties kh√¥ng n√™n qu√° l·ªõn ƒë·ªÉ agent kh√¥ng s·ª£ explore
   - Nh∆∞ng ƒë·ªß l·ªõn ƒë·ªÉ discourage bad behaviors
   - **Khuy·∫øn ngh·ªã**: `|penalty| < success_reward / 10`

4. **Reward Normalization**:
   - N·∫øu rewards qu√° l·ªõn (> 1000), normalize v·ªÅ kho·∫£ng [-100, 500]
   - N·∫øu rewards qu√° nh·ªè (< 1), scale l√™n ƒë·ªÉ agent c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c

---

## T√≠nh NƒÉng N√¢ng Cao

### 1. Curriculum Learning

**M·ª•c ƒë√≠ch**: Train t·ª´ scenarios ƒë∆°n gi·∫£n ƒë·∫øn ph·ª©c t·∫°p

**C√°c Levels:**
- **Level 0 (Beginner)**: G·∫ßn (<1000km), √≠t nodes (5-30)
- **Level 1 (Easy)**: G·∫ßn (<2000km), nhi·ªÅu nodes h∆°n (10-40)
- **Level 2 (Medium)**: Xa (<5000km), nhi·ªÅu nodes (20-60)
- **Level 3 (Hard)**: R·∫•t xa (<10000km), nhi·ªÅu nodes (40-77), c√≥ QoS
- **Level 4 (Expert)**: To√†n c·∫ßu (<20000km), t·∫•t c·∫£ nodes (60-81)
- **Level 5 (Master)**: Kh√¥ng gi·ªõi h·∫°n

**C·∫•u h√¨nh:**
```yaml
curriculum:
  enabled: true
  min_success_rate: 0.7              # Advance khi success rate >= 70%
  min_episodes_at_level: 100         # T·ªëi thi·ªÉu 100 episodes m·ªói level
  adaptive: true                      # Adaptive difficulty
```

### 2. Imitation Learning

**M·ª•c ƒë√≠ch**: H·ªçc t·ª´ expert demonstrations (Dijkstra algorithm)

**Ph∆∞∆°ng ph√°p**: DAGGER (Dataset Aggregation)
- B·∫Øt ƒë·∫ßu v·ªõi 100% expert actions
- Gradually gi·∫£m expert ratio khi agent c·∫£i thi·ªán
- Mix expert actions v·ªõi agent actions

**C·∫•u h√¨nh:**
```yaml
imitation_learning:
  enabled: true
  use_dagger: true
  expert_ratio: 0.3                   # 30% expert actions ban ƒë·∫ßu
  bc_loss_weight: 0.5                # Behavior Cloning loss weight
```

### 3. Multi-Objective Optimization

**M·ª•c ƒë√≠ch**: T·ªëi ∆∞u ƒë·ªìng th·ªùi nhi·ªÅu m·ª•c ti√™u (latency, reliability, energy)

**Ph∆∞∆°ng ph√°p**: Pareto Front
- T√¨m c√°c solutions kh√¥ng b·ªã dominate b·ªüi solutions kh√°c
- User c√≥ th·ªÉ ch·ªçn solution d·ª±a tr√™n preference

**C·∫•u h√¨nh:**
```yaml
multi_objective:
  enabled: true
  use_pareto: true
  pareto_front_size: 10
  latency_weight: 0.4
  reliability_weight: 0.3
  energy_weight: 0.3
  adaptive_weights: true              # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh weights
```

### 4. Prioritized Experience Replay

**M·ª•c ƒë√≠ch**: ∆Øu ti√™n h·ªçc t·ª´ c√°c experiences quan tr·ªçng (TD-error cao)

**C·∫•u h√¨nh:**
```yaml
rl_agent:
  dqn:
    use_prioritized_replay: true
```

**C√¥ng th·ª©c:**
```
priority = |TD_error|^Œ±
P(i) = priority_i / Œ£ priority_j
w_i = (N * P(i))^(-Œ≤)
```

### 5. Double DQN

**M·ª•c ƒë√≠ch**: Gi·∫£m overestimation c·ªßa Q-values

**C·∫•u h√¨nh:**
```yaml
rl_agent:
  dqn:
    use_double_dqn: true
```

**C√¥ng th·ª©c:**
```
Q_target = r + Œ≥ * Q_target(s', argmax_a' Q_current(s', a'))
```

---

## Monitoring v√† ƒê√°nh Gi√°

### 1. Metrics ƒê∆∞·ª£c Track

#### Training Metrics:
- **Episode Reward**: T·ªïng reward m·ªói episode
- **Episode Length**: S·ªë steps m·ªói episode
- **Loss**: Training loss (Huber loss)
- **Q-values**: Mean Q-values
- **Epsilon**: Exploration rate
- **Success Rate**: T·ª∑ l·ªá episodes th√†nh c√¥ng

#### Evaluation Metrics:
- **Mean Reward**: Reward trung b√¨nh
- **Success Rate**: T·ª∑ l·ªá th√†nh c√¥ng
- **Mean Hops**: S·ªë hops trung b√¨nh
- **Mean Latency**: Latency trung b√¨nh (ms)
- **Mean Distance**: Kho·∫£ng c√°ch trung b√¨nh (km)

### 2. Model Checkpoints

#### Best Model:
- **Path**: `models/best_models/best_model.pt`
- **Saved khi**: Evaluation reward c·∫£i thi·ªán

#### Checkpoints:
- **Path**: `models/checkpoints/checkpoint_ep{episode}.pt`
- **Saved m·ªói**: `save_frequency` episodes

#### Final Model:
- **Path**: `models/rl_agent/final_model.pt`
- **Saved khi**: Training ho√†n th√†nh

### 3. Evaluation Script

```python
from training.trainer import RoutingTrainer
from models.database import db

# Load model
trainer = RoutingTrainer(config)
metrics = trainer.load_and_evaluate(
    model_path='./models/best_models/best_model.pt',
    nodes=nodes,
    terminals=terminals,
    num_episodes=50
)

print(f"Success Rate: {metrics['success_rate']:.2%}")
print(f"Mean Hops: {metrics['mean_hops']:.1f}")
print(f"Mean Latency: {metrics['mean_latency']:.2f}ms")
```

### 4. So S√°nh v·ªõi Baseline

H·ªá th·ªëng c√≥ th·ªÉ so s√°nh v·ªõi:
- **Dijkstra**: Shortest path algorithm
- **Heuristic**: Rule-based routing

```python
# Trong evaluation, so s√°nh v·ªõi Dijkstra
enable_dijkstra_comparison: true
```

---

## Troubleshooting

### 1. Training Kh√¥ng H·ªôi T·ª•

**Tri·ªáu ch·ª©ng**: Reward kh√¥ng tƒÉng, loss kh√¥ng gi·∫£m

**Gi·∫£i ph√°p**:
- Gi·∫£m learning rate: `0.0001 ‚Üí 0.00005`
- TƒÉng batch size: `64 ‚Üí 128`
- Ki·ªÉm tra reward scale (c√≥ th·ªÉ qu√° l·ªõn/nh·ªè)
- TƒÉng exploration: `exploration_final_eps: 0.05`
- Ki·ªÉm tra state normalization

### 2. Out of Memory

**Tri·ªáu ch·ª©ng**: CUDA out of memory

**Gi·∫£i ph√°p**:
- Gi·∫£m batch size: `64 ‚Üí 32`
- Gi·∫£m buffer size: `100000 ‚Üí 50000`
- Gi·∫£m max_nodes trong state: `30 ‚Üí 20`
- S·ª≠ d·ª•ng CPU thay v√¨ GPU

### 3. Success Rate Th·∫•p

**Tri·ªáu ch·ª©ng**: Agent kh√¥ng t√¨m ƒë∆∞·ª£c ƒë∆∞·ªùng ƒëi

**Gi·∫£i ph√°p**:
- TƒÉng success reward: `200.0 ‚Üí 500.0`
- Gi·∫£m failure penalty: `-10.0 ‚Üí -5.0`
- TƒÉng progress reward scale: `100.0 ‚Üí 200.0`
- S·ª≠ d·ª•ng Curriculum Learning
- S·ª≠ d·ª•ng Imitation Learning ƒë·ªÉ bootstrap

### 4. Training Qu√° Ch·∫≠m

**Tri·ªáu ch·ª©ng**: M·ªói episode m·∫•t qu√° nhi·ªÅu th·ªùi gian

**Gi·∫£i ph√°p**:
- Gi·∫£m max_steps_per_episode: `15 ‚Üí 10`
- Gi·∫£m max_nodes: `30 ‚Üí 20`
- T·∫Øt m·ªôt s·ªë features kh√¥ng c·∫ßn thi·∫øt
- S·ª≠ d·ª•ng GPU n·∫øu c√≥
- TƒÉng batch size ƒë·ªÉ train hi·ªáu qu·∫£ h∆°n

### 5. Model Kh√¥ng Load ƒê∆∞·ª£c

**Tri·ªáu ch·ª©ng**: L·ªói khi load checkpoint

**Gi·∫£i ph√°p**:
- Ki·ªÉm tra action_dim c√≥ kh·ªõp kh√¥ng
- Ki·ªÉm tra state_dim c√≥ kh·ªõp kh√¥ng
- Load v·ªõi `strict=False` n·∫øu architecture thay ƒë·ªïi
- Ki·ªÉm tra PyTorch version compatibility

### 6. NaN Loss

**Tri·ªáu ch·ª©ng**: Loss = NaN

**Gi·∫£i ph√°p**:
- Ki·ªÉm tra reward scale (c√≥ th·ªÉ qu√° l·ªõn)
- Th√™m gradient clipping: `gradient_clip: 10.0`
- Ki·ªÉm tra state c√≥ NaN/Inf kh√¥ng
- Normalize rewards: `reward = reward / 100.0`

---

## Best Practices

### 1. Hyperparameter Tuning

- **B·∫Øt ƒë·∫ßu v·ªõi default values** trong config
- **Tune t·ª´ng tham s·ªë m·ªôt** ƒë·ªÉ hi·ªÉu ·∫£nh h∆∞·ªüng
- **S·ª≠ d·ª•ng Tensorboard** ƒë·ªÉ visualize
- **Early stopping** ƒë·ªÉ tr√°nh overfitting

### 2. Reward Engineering

- **Reward scale quan tr·ªçng**: Qu√° l·ªõn ‚Üí unstable, qu√° nh·ªè ‚Üí ch·∫≠m h·ªçc
- **Shaped rewards**: Th√™m intermediate rewards ƒë·ªÉ guide learning
- **Penalty balance**: Kh√¥ng penalty qu√° m·∫°nh ‚Üí agent kh√¥ng d√°m explore

### 3. State Design

- **Normalize features**: T·∫•t c·∫£ features v·ªÅ [0, 1] ho·∫∑c [-1, 1]
- **Feature selection**: Ch·ªâ gi·ªØ features quan tr·ªçng
- **Caching**: Cache c√°c t√≠nh to√°n t·ªën k√©m (distance, quality)

### 4. Training Strategy

- **Warm-up**: Cho agent explore nhi·ªÅu tr∆∞·ªõc khi train (`learning_starts: 5000`)
- **Curriculum**: B·∫Øt ƒë·∫ßu t·ª´ scenarios ƒë∆°n gi·∫£n
- **Evaluation**: Evaluate th∆∞·ªùng xuy√™n nh∆∞ng kh√¥ng qu√° nhi·ªÅu (t·ªën th·ªùi gian)

### 5. Model Selection

- **Best model**: Ch·ªçn model c√≥ evaluation reward cao nh·∫•t
- **Ensemble**: C√≥ th·ªÉ ensemble nhi·ªÅu models ƒë·ªÉ tƒÉng robustness
- **Transfer learning**: Fine-tune t·ª´ pretrained model

---

## T√†i Li·ªáu Tham Kh·∫£o

### Papers:
1. **Dueling DQN**: "Dueling Network Architectures for Deep Reinforcement Learning" (Wang et al., 2016)
2. **Double DQN**: "Deep Reinforcement Learning with Double Q-learning" (van Hasselt et al., 2016)
3. **Prioritized Replay**: "Prioritized Experience Replay" (Schaul et al., 2016)
4. **Curriculum Learning**: "Curriculum Learning" (Bengio et al., 2009)
5. **DAGGER**: "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning" (Ross et al., 2011)

### Code Structure:
- `agent/dueling_dqn.py`: Dueling DQN implementation
- `environment/routing_env.py`: Environment implementation
- `environment/state_builder.py`: State representation
- `training/trainer.py`: Standard trainer
- `training/enhanced_trainer.py`: Enhanced trainer v·ªõi advanced features
- `training/curriculum_learning.py`: Curriculum learning
- `training/imitation_learning.py`: Imitation learning
- `training/multi_objective.py`: Multi-objective optimization

---

## Li√™n H·ªá v√† H·ªó Tr·ª£

N·∫øu c√≥ v·∫•n ƒë·ªÅ ho·∫∑c c√¢u h·ªèi, vui l√≤ng:
1. Ki·ªÉm tra logs trong `training.log`
2. Xem Tensorboard ƒë·ªÉ visualize training
3. Ki·ªÉm tra config file c√≥ ƒë√∫ng kh√¥ng
4. ƒê·ªçc troubleshooting section ·ªü tr√™n

---

**Ch√∫c b·∫°n training th√†nh c√¥ng! üöÄ**


{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3 Testing: Optimization Features\n",
        "\n",
        "This notebook tests Phase 3 improvements:\n",
        "1. **Deterministic Action Selection**: Q-value validation warning for low Q-values\n",
        "2. **Performance Tuning**: Double DQN, Prioritized Replay, Gradient Clipping, Soft Target Updates\n",
        "3. **Model Optimization**: DuelingDQN architecture with LayerNorm, Dropout, ELU activation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Setup complete\n"
          ]
        }
      ],
      "source": [
        "# Setup paths and imports\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add backend to path\n",
        "backend_path = os.path.abspath('..')\n",
        "if backend_path not in sys.path:\n",
        "    sys.path.insert(0, backend_path)\n",
        "\n",
        "# Plotting configuration\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Logging configuration\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"âœ“ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Modules loaded\n",
            "âœ“ Database connected: aiprancs\n",
            "âœ“ Config loaded\n"
          ]
        }
      ],
      "source": [
        "# Import project modules\n",
        "from config import Config\n",
        "from models.database import Database\n",
        "from agent.dueling_dqn import DuelingDQNAgent\n",
        "from environment.routing_env import RoutingEnvironment\n",
        "from environment.state_builder import RoutingStateBuilder\n",
        "\n",
        "# Initialize\n",
        "config = Config.get_yaml_config()\n",
        "db = Database()\n",
        "db.connect()\n",
        "\n",
        "print(\"âœ“ Modules loaded\")\n",
        "print(f\"âœ“ Database connected: {db.db.name}\")\n",
        "print(f\"âœ“ Config loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Deterministic Action Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:============================================================\n",
            "INFO:__main__:TEST 1: Deterministic Action Selection\n",
            "INFO:__main__:============================================================\n",
            "INFO:environment.routing_env:Dynamic max_steps: 25 (network_size=50, base=15)\n",
            "INFO:agent.dueling_dqn:Initializing DuelingDQN Agent on device: cpu\n",
            "INFO:agent.dueling_dqn:DuelingDQN Agent initialized: state_dim=560, action_dim=30\n",
            "INFO:__main__:Test 1: Deterministic consistency\n",
            "INFO:__main__:  Deterministic actions consistent: True\n",
            "INFO:__main__:  Deterministic action: 10\n",
            "INFO:__main__:  Non-deterministic variations: 7\n",
            "INFO:__main__:Test 2: Action mask support\n",
            "INFO:__main__:  All actions respect mask: True\n",
            "INFO:__main__:  Masked actions (sample): [2, 2, 2, 2, 2]\n",
            "INFO:__main__:Test 3: Q-value validation warning\n",
            "INFO:__main__:  Max Q-value: 1.00\n",
            "INFO:__main__:  Min Q-value: -6.05\n",
            "INFO:__main__:  Would trigger warning: False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Deterministic Action Selection Results:\n",
            "                     test deterministic_consistent  deterministic_action  non_deterministic_variations all_actions_masked  masked_actions  max_q_value  min_q_value q_value_low would_trigger_warning\n",
            "Deterministic Consistency                     True                  10.0                           7.0                NaN             NaN          NaN          NaN         NaN                   NaN\n",
            "      Action Mask Support                      NaN                   NaN                           NaN               True [2, 2, 2, 2, 2]          NaN          NaN         NaN                   NaN\n",
            "       Q-value Validation                      NaN                   NaN                           NaN                NaN             NaN     0.999353    -6.052233       False                 False\n"
          ]
        }
      ],
      "source": [
        "def test_deterministic_action_selection():\n",
        "    \"\"\"Test deterministic action selection with Q-value validation\"\"\"\n",
        "    logger.info(\"=\" * 60)\n",
        "    logger.info(\"TEST 1: Deterministic Action Selection\")\n",
        "    logger.info(\"=\" * 60)\n",
        "    \n",
        "    # Load nodes and terminals\n",
        "    nodes_collection = db.get_collection('nodes')\n",
        "    terminals_collection = db.get_collection('terminals')\n",
        "    \n",
        "    nodes = list(nodes_collection.find({'isOperational': True}, {'_id': 0}).limit(50))\n",
        "    terminals = list(terminals_collection.find({}, {'_id': 0}).limit(10))\n",
        "    \n",
        "    if len(nodes) < 5 or len(terminals) < 2:\n",
        "        logger.error(\"Not enough nodes or terminals for testing\")\n",
        "        return None\n",
        "    \n",
        "    # Create environment\n",
        "    env = RoutingEnvironment(\n",
        "        nodes=nodes,\n",
        "        terminals=terminals[:2],\n",
        "        config=config,\n",
        "        max_steps=15\n",
        "    )\n",
        "    \n",
        "    # Initialize state builder\n",
        "    state_builder = RoutingStateBuilder(config)\n",
        "    state_dim = state_builder.state_dimension\n",
        "    action_dim = env.action_space.n\n",
        "    \n",
        "    # Create agent\n",
        "    agent = DuelingDQNAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        config=config\n",
        "    )\n",
        "    \n",
        "    # Set to eval mode for deterministic behavior (disable dropout)\n",
        "    agent.eval()\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Test 1: Deterministic vs Non-deterministic consistency\n",
        "    logger.info(\"Test 1: Deterministic consistency\")\n",
        "    state, info = env.reset()\n",
        "    \n",
        "    deterministic_actions = []\n",
        "    non_deterministic_actions = []\n",
        "    \n",
        "    for _ in range(10):\n",
        "        det_action = agent.select_action(state, deterministic=True)\n",
        "        non_det_action = agent.select_action(state, deterministic=False)\n",
        "        deterministic_actions.append(det_action)\n",
        "        non_deterministic_actions.append(non_det_action)\n",
        "    \n",
        "    det_consistent = len(set(deterministic_actions)) == 1\n",
        "    results.append({\n",
        "        'test': 'Deterministic Consistency',\n",
        "        'deterministic_consistent': det_consistent,\n",
        "        'deterministic_action': deterministic_actions[0],\n",
        "        'non_deterministic_variations': len(set(non_deterministic_actions))\n",
        "    })\n",
        "    \n",
        "    logger.info(f\"  Deterministic actions consistent: {det_consistent}\")\n",
        "    logger.info(f\"  Deterministic action: {deterministic_actions[0]}\")\n",
        "    logger.info(f\"  Non-deterministic variations: {len(set(non_deterministic_actions))}\")\n",
        "    \n",
        "    # Test 2: Action mask support\n",
        "    logger.info(\"Test 2: Action mask support\")\n",
        "    state, info = env.reset()\n",
        "    \n",
        "    # Create action mask (allow only first 3 actions)\n",
        "    action_mask = np.zeros(action_dim)\n",
        "    action_mask[:3] = 1\n",
        "    \n",
        "    masked_actions = []\n",
        "    for _ in range(10):\n",
        "        action = agent.select_action(state, deterministic=True, action_mask=action_mask)\n",
        "        masked_actions.append(action)\n",
        "    \n",
        "    all_masked = all(a < 3 for a in masked_actions)\n",
        "    results.append({\n",
        "        'test': 'Action Mask Support',\n",
        "        'all_actions_masked': all_masked,\n",
        "        'masked_actions': masked_actions[:5]\n",
        "    })\n",
        "    \n",
        "    logger.info(f\"  All actions respect mask: {all_masked}\")\n",
        "    logger.info(f\"  Masked actions (sample): {masked_actions[:5]}\")\n",
        "    \n",
        "    # Test 3: Q-value validation (simulate low Q-values)\n",
        "    logger.info(\"Test 3: Q-value validation warning\")\n",
        "    \n",
        "    # Create a state that might produce low Q-values (untrained model)\n",
        "    state, info = env.reset()\n",
        "    \n",
        "    # Check if warning is triggered (would need very low Q-values)\n",
        "    # This is tested by checking the agent's behavior with untrained model\n",
        "    with torch.no_grad():\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
        "        q_values = agent.q_network(state_tensor)\n",
        "        max_q = q_values.max().item()\n",
        "        min_q = q_values.min().item()\n",
        "    \n",
        "    q_value_low = max_q < -100\n",
        "    results.append({\n",
        "        'test': 'Q-value Validation',\n",
        "        'max_q_value': max_q,\n",
        "        'min_q_value': min_q,\n",
        "        'q_value_low': q_value_low,\n",
        "        'would_trigger_warning': q_value_low\n",
        "    })\n",
        "    \n",
        "    logger.info(f\"  Max Q-value: {max_q:.2f}\")\n",
        "    logger.info(f\"  Min Q-value: {min_q:.2f}\")\n",
        "    logger.info(f\"  Would trigger warning: {q_value_low}\")\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    return df, agent\n",
        "\n",
        "df_deterministic, test_agent = test_deterministic_action_selection()\n",
        "if df_deterministic is not None:\n",
        "    print(\"\\nðŸ“Š Deterministic Action Selection Results:\")\n",
        "    print(df_deterministic.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Performance Tuning Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:============================================================\n",
            "INFO:__main__:TEST 2: Performance Tuning Features\n",
            "INFO:__main__:============================================================\n",
            "INFO:environment.routing_env:Dynamic max_steps: 25 (network_size=50, base=15)\n",
            "INFO:agent.dueling_dqn:Initializing DuelingDQN Agent on device: cpu\n",
            "INFO:agent.dueling_dqn:DuelingDQN Agent initialized: state_dim=560, action_dim=30\n",
            "INFO:__main__:Test 1: Double DQN\n",
            "INFO:__main__:  Double DQN enabled: True\n",
            "INFO:__main__:Test 2: Prioritized Replay\n",
            "INFO:__main__:  Prioritized Replay enabled: True\n",
            "INFO:__main__:Test 3: Gradient Clipping\n",
            "INFO:__main__:  Gradient clipping: 10.0\n",
            "INFO:__main__:Test 4: Soft Target Updates\n",
            "INFO:__main__:  Tau (soft update): 0.005\n",
            "INFO:__main__:Test 5: Learning Rate Scheduler\n",
            "INFO:__main__:  LR Scheduler enabled: True\n",
            "INFO:__main__:  Initial learning rate: 0.0001\n",
            "INFO:__main__:Test 6: Exploration Parameters\n",
            "INFO:__main__:  Epsilon: 1.0 -> 0.01 (decay: 0.9995)\n",
            "INFO:__main__:Test 7: Training Step Integration\n",
            "INFO:__main__:  Not enough samples for training: 100/5000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Performance Tuning Results:\n",
            "\n",
            "DOUBLE DQN:\n",
            "  enabled: True\n",
            "  description: Reduces overestimation bias\n",
            "\n",
            "PRIORITIZED REPLAY:\n",
            "  enabled: True\n",
            "  description: Samples important transitions more frequently\n",
            "\n",
            "GRADIENT CLIPPING:\n",
            "  enabled: True\n",
            "  clip_value: 10.0\n",
            "  description: Prevents exploding gradients\n",
            "\n",
            "SOFT TARGET UPDATES:\n",
            "  enabled: True\n",
            "  tau_value: 0.005\n",
            "  description: Smooth target network updates\n",
            "\n",
            "LR SCHEDULER:\n",
            "  enabled: True\n",
            "  initial_lr: 0.0001\n",
            "  description: Adaptive learning rate scheduling\n",
            "\n",
            "EXPLORATION:\n",
            "  epsilon_start: 1.0\n",
            "  epsilon_end: 0.01\n",
            "  epsilon_decay: 0.9995\n",
            "  description: Optimized exploration strategy\n",
            "\n",
            "TRAINING STEP:\n",
            "  success: False\n",
            "  reason: Buffer size (100) < learning_starts (5000)\n"
          ]
        }
      ],
      "source": [
        "def test_performance_tuning():\n",
        "    \"\"\"Test performance tuning features: Double DQN, Prioritized Replay, Gradient Clipping, etc.\"\"\"\n",
        "    logger.info(\"=\" * 60)\n",
        "    logger.info(\"TEST 2: Performance Tuning Features\")\n",
        "    logger.info(\"=\" * 60)\n",
        "    \n",
        "    # Load nodes and terminals\n",
        "    nodes_collection = db.get_collection('nodes')\n",
        "    terminals_collection = db.get_collection('terminals')\n",
        "    \n",
        "    nodes = list(nodes_collection.find({'isOperational': True}, {'_id': 0}).limit(50))\n",
        "    terminals = list(terminals_collection.find({}, {'_id': 0}).limit(10))\n",
        "    \n",
        "    if len(nodes) < 5 or len(terminals) < 2:\n",
        "        logger.error(\"Not enough nodes or terminals for testing\")\n",
        "        return None\n",
        "    \n",
        "    # Create environment\n",
        "    env = RoutingEnvironment(\n",
        "        nodes=nodes,\n",
        "        terminals=terminals[:2],\n",
        "        config=config,\n",
        "        max_steps=15\n",
        "    )\n",
        "    \n",
        "    # Initialize state builder\n",
        "    state_builder = RoutingStateBuilder(config)\n",
        "    state_dim = state_builder.state_dimension\n",
        "    action_dim = env.action_space.n\n",
        "    \n",
        "    # Create agent\n",
        "    agent = DuelingDQNAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        config=config\n",
        "    )\n",
        "    \n",
        "    # Set to eval mode for testing\n",
        "    agent.eval()\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Test 1: Double DQN\n",
        "    logger.info(\"Test 1: Double DQN\")\n",
        "    use_double_dqn = agent.use_double_dqn\n",
        "    results['double_dqn'] = {\n",
        "        'enabled': use_double_dqn,\n",
        "        'description': 'Reduces overestimation bias'\n",
        "    }\n",
        "    logger.info(f\"  Double DQN enabled: {use_double_dqn}\")\n",
        "    \n",
        "    # Test 2: Prioritized Replay\n",
        "    logger.info(\"Test 2: Prioritized Replay\")\n",
        "    use_prioritized = agent.use_prioritized_replay\n",
        "    results['prioritized_replay'] = {\n",
        "        'enabled': use_prioritized,\n",
        "        'description': 'Samples important transitions more frequently'\n",
        "    }\n",
        "    logger.info(f\"  Prioritized Replay enabled: {use_prioritized}\")\n",
        "    \n",
        "    # Test 3: Gradient Clipping\n",
        "    logger.info(\"Test 3: Gradient Clipping\")\n",
        "    gradient_clip = agent.gradient_clip\n",
        "    results['gradient_clipping'] = {\n",
        "        'enabled': gradient_clip > 0,\n",
        "        'clip_value': gradient_clip,\n",
        "        'description': 'Prevents exploding gradients'\n",
        "    }\n",
        "    logger.info(f\"  Gradient clipping: {gradient_clip}\")\n",
        "    \n",
        "    # Test 4: Soft Target Updates\n",
        "    logger.info(\"Test 4: Soft Target Updates\")\n",
        "    tau = agent.tau\n",
        "    results['soft_target_updates'] = {\n",
        "        'enabled': tau < 1.0,\n",
        "        'tau_value': tau,\n",
        "        'description': 'Smooth target network updates'\n",
        "    }\n",
        "    logger.info(f\"  Tau (soft update): {tau}\")\n",
        "    \n",
        "    # Test 5: Learning Rate Scheduler\n",
        "    logger.info(\"Test 5: Learning Rate Scheduler\")\n",
        "    has_lr_scheduler = hasattr(agent, 'lr_scheduler') and agent.lr_scheduler is not None\n",
        "    initial_lr = agent.optimizer.param_groups[0]['lr']\n",
        "    results['lr_scheduler'] = {\n",
        "        'enabled': has_lr_scheduler,\n",
        "        'initial_lr': initial_lr,\n",
        "        'description': 'Adaptive learning rate scheduling'\n",
        "    }\n",
        "    logger.info(f\"  LR Scheduler enabled: {has_lr_scheduler}\")\n",
        "    logger.info(f\"  Initial learning rate: {initial_lr}\")\n",
        "    \n",
        "    # Test 6: Exploration parameters\n",
        "    logger.info(\"Test 6: Exploration Parameters\")\n",
        "    epsilon_start = agent.epsilon_start\n",
        "    epsilon_end = agent.epsilon_end\n",
        "    epsilon_decay = agent.epsilon_decay\n",
        "    results['exploration'] = {\n",
        "        'epsilon_start': epsilon_start,\n",
        "        'epsilon_end': epsilon_end,\n",
        "        'epsilon_decay': epsilon_decay,\n",
        "        'description': 'Optimized exploration strategy'\n",
        "    }\n",
        "    logger.info(f\"  Epsilon: {epsilon_start} -> {epsilon_end} (decay: {epsilon_decay})\")\n",
        "    \n",
        "    # Test 7: Training step with all features\n",
        "    logger.info(\"Test 7: Training Step Integration\")\n",
        "    \n",
        "    # Set to training mode for training step test\n",
        "    agent.train_mode()\n",
        "    \n",
        "    # Add some experiences to replay buffer\n",
        "    state, info = env.reset()\n",
        "    for _ in range(100):\n",
        "        action = agent.select_action(state, deterministic=False)\n",
        "        next_state, reward, terminated, truncated, step_info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        agent.replay_buffer.push(\n",
        "            state=state,\n",
        "            action=action,\n",
        "            reward=reward,\n",
        "            next_state=next_state,\n",
        "            done=done\n",
        "        )\n",
        "        \n",
        "        if done:\n",
        "            state, info = env.reset()\n",
        "        else:\n",
        "            state = next_state\n",
        "    \n",
        "    # Try training step\n",
        "    if len(agent.replay_buffer) >= agent.learning_starts:\n",
        "        train_metrics = agent.train_step()\n",
        "        if train_metrics:\n",
        "            results['training_step'] = {\n",
        "                'success': True,\n",
        "                'loss': train_metrics.get('loss', 0),\n",
        "                'grad_norm': train_metrics.get('grad_norm', 0),\n",
        "                'q_value': train_metrics.get('q_value', 0)\n",
        "            }\n",
        "            logger.info(f\"  Training step successful\")\n",
        "            logger.info(f\"  Loss: {train_metrics.get('loss', 0):.4f}\")\n",
        "            logger.info(f\"  Grad norm: {train_metrics.get('grad_norm', 0):.4f}\")\n",
        "        else:\n",
        "            results['training_step'] = {'success': False}\n",
        "            logger.warning(\"  Training step returned None\")\n",
        "    else:\n",
        "        results['training_step'] = {\n",
        "            'success': False,\n",
        "            'reason': f'Buffer size ({len(agent.replay_buffer)}) < learning_starts ({agent.learning_starts})'\n",
        "        }\n",
        "        logger.info(f\"  Not enough samples for training: {len(agent.replay_buffer)}/{agent.learning_starts}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "performance_results = test_performance_tuning()\n",
        "if performance_results:\n",
        "    print(\"\\nðŸ“Š Performance Tuning Results:\")\n",
        "    for key, value in performance_results.items():\n",
        "        print(f\"\\n{key.upper().replace('_', ' ')}:\")\n",
        "        for k, v in value.items():\n",
        "            print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Model Optimization Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:============================================================\n",
            "INFO:__main__:TEST 3: Model Optimization Features\n",
            "INFO:__main__:============================================================\n",
            "INFO:environment.routing_env:Dynamic max_steps: 25 (network_size=50, base=15)\n",
            "INFO:agent.dueling_dqn:Initializing DuelingDQN Agent on device: cpu\n",
            "INFO:agent.dueling_dqn:DuelingDQN Agent initialized: state_dim=560, action_dim=30\n",
            "INFO:__main__:Test 1: Architecture\n",
            "INFO:__main__:  Architecture: DuelingDQN\n",
            "INFO:__main__:Test 2: LayerNorm\n",
            "INFO:__main__:  LayerNorm enabled: True\n",
            "INFO:__main__:Test 3: Dropout\n",
            "INFO:__main__:  Dropout enabled: True\n",
            "INFO:__main__:  Dropout rate: 0.1\n",
            "INFO:__main__:Test 4: Activation Function\n",
            "INFO:__main__:  Activation function: ELU\n",
            "INFO:__main__:Test 5: Network Depth\n",
            "INFO:__main__:  Hidden dimensions: [512, 256, 128]\n",
            "INFO:__main__:Test 6: Loss Function\n",
            "INFO:__main__:  Loss function: Huber Loss (Smooth L1)\n",
            "INFO:__main__:Test 7: Model Size\n",
            "INFO:__main__:  Total parameters: 506,431\n",
            "INFO:__main__:  Trainable parameters: 506,431\n",
            "INFO:__main__:  Model size: 1.93 MB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Model Optimization Results:\n",
            "\n",
            "ARCHITECTURE:\n",
            "  type: DuelingDQN\n",
            "  is_dueling: True\n",
            "  description: Separate value and advantage streams\n",
            "\n",
            "LAYER NORM:\n",
            "  enabled: True\n",
            "  description: Training stability\n",
            "\n",
            "DROPOUT:\n",
            "  enabled: True\n",
            "  rate: 0.1\n",
            "  description: Prevents overfitting\n",
            "\n",
            "ACTIVATION:\n",
            "  type: ELU\n",
            "  is_elu: True\n",
            "  description: Better than ReLU for DQN\n",
            "\n",
            "NETWORK DEPTH:\n",
            "  hidden_dims: [512, 256, 128]\n",
            "  total_layers: 3\n",
            "  description: Deep network for better representation\n",
            "\n",
            "LOSS FUNCTION:\n",
            "  type: Huber Loss (Smooth L1)\n",
            "  description: Stable training\n",
            "\n",
            "MODEL SIZE:\n",
            "  total_params: 506431\n",
            "  trainable_params: 506431\n",
            "  size_mb: 1.9318809509277344\n",
            "  description: Model complexity\n"
          ]
        }
      ],
      "source": [
        "def test_model_optimization():\n",
        "    \"\"\"Test model optimization features: Architecture, LayerNorm, Dropout, ELU activation\"\"\"\n",
        "    logger.info(\"=\" * 60)\n",
        "    logger.info(\"TEST 3: Model Optimization Features\")\n",
        "    logger.info(\"=\" * 60)\n",
        "    \n",
        "    # Load nodes and terminals\n",
        "    nodes_collection = db.get_collection('nodes')\n",
        "    terminals_collection = db.get_collection('terminals')\n",
        "    \n",
        "    nodes = list(nodes_collection.find({'isOperational': True}, {'_id': 0}).limit(50))\n",
        "    terminals = list(terminals_collection.find({}, {'_id': 0}).limit(10))\n",
        "    \n",
        "    if len(nodes) < 5 or len(terminals) < 2:\n",
        "        logger.error(\"Not enough nodes or terminals for testing\")\n",
        "        return None\n",
        "    \n",
        "    # Create environment\n",
        "    env = RoutingEnvironment(\n",
        "        nodes=nodes,\n",
        "        terminals=terminals[:2],\n",
        "        config=config,\n",
        "        max_steps=15\n",
        "    )\n",
        "    \n",
        "    # Initialize state builder\n",
        "    state_builder = RoutingStateBuilder(config)\n",
        "    state_dim = state_builder.state_dimension\n",
        "    action_dim = env.action_space.n\n",
        "    \n",
        "    # Create agent\n",
        "    agent = DuelingDQNAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        config=config\n",
        "    )\n",
        "    \n",
        "    # Set to eval mode for testing\n",
        "    agent.eval()\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Test 1: Architecture type\n",
        "    logger.info(\"Test 1: Architecture\")\n",
        "    is_dueling = hasattr(agent.q_network, 'value_stream') and hasattr(agent.q_network, 'advantage_stream')\n",
        "    results['architecture'] = {\n",
        "        'type': 'DuelingDQN' if is_dueling else 'Standard DQN',\n",
        "        'is_dueling': is_dueling,\n",
        "        'description': 'Separate value and advantage streams'\n",
        "    }\n",
        "    logger.info(f\"  Architecture: {'DuelingDQN' if is_dueling else 'Standard DQN'}\")\n",
        "    \n",
        "    # Test 2: LayerNorm\n",
        "    logger.info(\"Test 2: LayerNorm\")\n",
        "    has_layernorm = False\n",
        "    for module in agent.q_network.modules():\n",
        "        if isinstance(module, torch.nn.LayerNorm):\n",
        "            has_layernorm = True\n",
        "            break\n",
        "    \n",
        "    results['layer_norm'] = {\n",
        "        'enabled': has_layernorm,\n",
        "        'description': 'Training stability'\n",
        "    }\n",
        "    logger.info(f\"  LayerNorm enabled: {has_layernorm}\")\n",
        "    \n",
        "    # Test 3: Dropout\n",
        "    logger.info(\"Test 3: Dropout\")\n",
        "    has_dropout = False\n",
        "    dropout_rate = 0.0\n",
        "    for module in agent.q_network.modules():\n",
        "        if isinstance(module, torch.nn.Dropout):\n",
        "            has_dropout = True\n",
        "            dropout_rate = module.p\n",
        "            break\n",
        "    \n",
        "    results['dropout'] = {\n",
        "        'enabled': has_dropout,\n",
        "        'rate': dropout_rate if has_dropout else 0.0,\n",
        "        'description': 'Prevents overfitting'\n",
        "    }\n",
        "    logger.info(f\"  Dropout enabled: {has_dropout}\")\n",
        "    if has_dropout:\n",
        "        logger.info(f\"  Dropout rate: {dropout_rate}\")\n",
        "    \n",
        "    # Test 4: Activation function\n",
        "    logger.info(\"Test 4: Activation Function\")\n",
        "    activation_type = 'Unknown'\n",
        "    for module in agent.q_network.modules():\n",
        "        if isinstance(module, torch.nn.ELU):\n",
        "            activation_type = 'ELU'\n",
        "            break\n",
        "        elif isinstance(module, torch.nn.ReLU):\n",
        "            activation_type = 'ReLU'\n",
        "            break\n",
        "        elif isinstance(module, torch.nn.SELU):\n",
        "            activation_type = 'SELU'\n",
        "            break\n",
        "    \n",
        "    results['activation'] = {\n",
        "        'type': activation_type,\n",
        "        'is_elu': activation_type == 'ELU',\n",
        "        'description': 'Better than ReLU for DQN'\n",
        "    }\n",
        "    logger.info(f\"  Activation function: {activation_type}\")\n",
        "    \n",
        "    # Test 5: Network depth\n",
        "    logger.info(\"Test 5: Network Depth\")\n",
        "    hidden_layers = []\n",
        "    for name, module in agent.q_network.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear) and 'shared' in name.lower():\n",
        "            hidden_layers.append(module.out_features)\n",
        "    \n",
        "    results['network_depth'] = {\n",
        "        'hidden_dims': hidden_layers,\n",
        "        'total_layers': len(hidden_layers),\n",
        "        'description': 'Deep network for better representation'\n",
        "    }\n",
        "    logger.info(f\"  Hidden dimensions: {hidden_layers}\")\n",
        "    \n",
        "    # Test 6: Loss function\n",
        "    logger.info(\"Test 6: Loss Function\")\n",
        "    # Check training step to see loss type\n",
        "    results['loss_function'] = {\n",
        "        'type': 'Huber Loss (Smooth L1)',\n",
        "        'description': 'Stable training'\n",
        "    }\n",
        "    logger.info(\"  Loss function: Huber Loss (Smooth L1)\")\n",
        "    \n",
        "    # Test 7: Model size\n",
        "    logger.info(\"Test 7: Model Size\")\n",
        "    total_params = sum(p.numel() for p in agent.q_network.parameters())\n",
        "    trainable_params = sum(p.numel() for p in agent.q_network.parameters() if p.requires_grad)\n",
        "    model_size_mb = total_params * 4 / (1024 * 1024)  # Assuming float32\n",
        "    \n",
        "    results['model_size'] = {\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'size_mb': model_size_mb,\n",
        "        'description': 'Model complexity'\n",
        "    }\n",
        "    logger.info(f\"  Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    logger.info(f\"  Model size: {model_size_mb:.2f} MB\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "model_results = test_model_optimization()\n",
        "if model_results:\n",
        "    print(\"\\nðŸ“Š Model Optimization Results:\")\n",
        "    for key, value in model_results.items():\n",
        "        print(f\"\\n{key.upper().replace('_', ' ')}:\")\n",
        "        for k, v in value.items():\n",
        "            print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Integration Test: All Phase 3 Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:============================================================\n",
            "INFO:__main__:TEST 4: Integration Test - All Phase 3 Features\n",
            "INFO:__main__:============================================================\n",
            "INFO:environment.routing_env:Dynamic max_steps: 25 (network_size=50, base=15)\n",
            "INFO:agent.dueling_dqn:Initializing DuelingDQN Agent on device: cpu\n",
            "INFO:agent.dueling_dqn:DuelingDQN Agent initialized: state_dim=560, action_dim=30\n",
            "INFO:__main__:Running 5 episodes with deterministic actions...\n",
            "INFO:__main__:Episode 1: Steps=6, Reward=-65267.58, Success=False\n",
            "INFO:__main__:Episode 2: Steps=6, Reward=-65267.58, Success=False\n",
            "INFO:__main__:Episode 3: Steps=4, Reward=-197492.79, Success=False\n",
            "INFO:__main__:Episode 4: Steps=4, Reward=-197492.79, Success=False\n",
            "INFO:__main__:Episode 5: Steps=6, Reward=-65267.58, Success=False\n",
            "INFO:__main__:\n",
            "Integration Test Results:\n",
            "INFO:__main__:  Average steps: 5.2\n",
            "INFO:__main__:  Success rate: 0.0%\n",
            "INFO:__main__:  Average reward: -118157.67\n",
            "INFO:__main__:  Deterministic consistency: True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Integration Test Results:\n",
            " episode  steps         reward  success  truncated\n",
            "       1      6  -65267.582752    False       True\n",
            "       2      6  -65267.582752    False       True\n",
            "       3      4 -197492.788740    False       True\n",
            "       4      4 -197492.788740    False       True\n",
            "       5      6  -65267.582752    False       True\n"
          ]
        }
      ],
      "source": [
        "def test_integration():\n",
        "    \"\"\"Integration test with all Phase 3 features\"\"\"\n",
        "    logger.info(\"=\" * 60)\n",
        "    logger.info(\"TEST 4: Integration Test - All Phase 3 Features\")\n",
        "    logger.info(\"=\" * 60)\n",
        "    \n",
        "    # Load nodes and terminals\n",
        "    nodes_collection = db.get_collection('nodes')\n",
        "    terminals_collection = db.get_collection('terminals')\n",
        "    \n",
        "    nodes = list(nodes_collection.find({'isOperational': True}, {'_id': 0}).limit(50))\n",
        "    terminals = list(terminals_collection.find({}, {'_id': 0}).limit(10))\n",
        "    \n",
        "    if len(nodes) < 5 or len(terminals) < 2:\n",
        "        logger.error(\"Not enough nodes or terminals for testing\")\n",
        "        return None\n",
        "    \n",
        "    # Create environment\n",
        "    env = RoutingEnvironment(\n",
        "        nodes=nodes,\n",
        "        terminals=terminals[:2],\n",
        "        config=config,\n",
        "        max_steps=15\n",
        "    )\n",
        "    \n",
        "    # Initialize state builder\n",
        "    state_builder = RoutingStateBuilder(config)\n",
        "    state_dim = state_builder.state_dimension\n",
        "    action_dim = env.action_space.n\n",
        "    \n",
        "    # Create agent with all Phase 3 optimizations\n",
        "    agent = DuelingDQNAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        config=config\n",
        "    )\n",
        "    \n",
        "    agent.eval()  # Set to evaluation mode\n",
        "    \n",
        "    results = []\n",
        "    num_episodes = 5\n",
        "    \n",
        "    logger.info(f\"Running {num_episodes} episodes with deterministic actions...\")\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        episode_steps = 0\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        actions_taken = []\n",
        "        \n",
        "        while not done and episode_steps < env.max_steps:\n",
        "            # Use deterministic action selection (Phase 3 feature)\n",
        "            action = agent.select_action(state, deterministic=True)\n",
        "            actions_taken.append(action)\n",
        "            \n",
        "            next_state, reward, terminated, truncated, step_info = env.step(action)\n",
        "            \n",
        "            done = terminated or truncated\n",
        "            episode_steps += 1\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "        \n",
        "        results.append({\n",
        "            'episode': episode + 1,\n",
        "            'steps': episode_steps,\n",
        "            'reward': episode_reward,\n",
        "            'success': terminated,\n",
        "            'truncated': truncated,\n",
        "            'actions_taken': actions_taken[:5]  # Sample\n",
        "        })\n",
        "        \n",
        "        logger.info(f\"Episode {episode + 1}: Steps={episode_steps}, Reward={episode_reward:.2f}, Success={terminated}\")\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    logger.info(f\"\\nIntegration Test Results:\")\n",
        "    logger.info(f\"  Average steps: {df['steps'].mean():.1f}\")\n",
        "    logger.info(f\"  Success rate: {(df['success'].sum() / len(df)) * 100:.1f}%\")\n",
        "    logger.info(f\"  Average reward: {df['reward'].mean():.2f}\")\n",
        "    \n",
        "    # Verify deterministic behavior\n",
        "    if len(results) > 1:\n",
        "        first_episode_actions = results[0]['actions_taken']\n",
        "        second_episode_actions = results[1]['actions_taken']\n",
        "        \n",
        "        # Reset and test deterministic consistency\n",
        "        state1, _ = env.reset()\n",
        "        state2, _ = env.reset()\n",
        "        \n",
        "        action1 = agent.select_action(state1, deterministic=True)\n",
        "        action2 = agent.select_action(state2, deterministic=True)\n",
        "        \n",
        "        # Same state should give same action\n",
        "        same_state_same_action = action1 == agent.select_action(state1, deterministic=True)\n",
        "        \n",
        "        logger.info(f\"  Deterministic consistency: {same_state_same_action}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "integration_results = test_integration()\n",
        "if integration_results is not None:\n",
        "    print(\"\\nðŸ“Š Integration Test Results:\")\n",
        "    print(integration_results[['episode', 'steps', 'reward', 'success', 'truncated']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary & Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PHASE 3 TEST SUMMARY\n",
            "============================================================\n",
            "\n",
            "Test Date: 2025-12-20 18:14:04\n",
            "\n",
            "Features Tested:\n",
            "  1. Deterministic Action Selection\n",
            "  2. Performance Tuning\n",
            "  3. Model Optimization\n",
            "  4. Integration Test\n",
            "\n",
            "âœ“ Deterministic Action Selection: Tested 3 scenarios\n",
            "  - Deterministic consistency: True\n",
            "\n",
            "âœ“ Performance Tuning:\n",
            "  - Double DQN: True\n",
            "  - Prioritized Replay: True\n",
            "  - Gradient Clipping: 10.0\n",
            "  - Soft Target Updates: True\n",
            "\n",
            "âœ“ Model Optimization:\n",
            "  - Architecture: DuelingDQN\n",
            "  - LayerNorm: True\n",
            "  - Dropout: True\n",
            "  - Activation: ELU\n",
            "  - Model size: 1.93 MB\n",
            "\n",
            "âœ“ Integration Test: Completed 5 episodes\n",
            "  - Average steps: 5.2\n",
            "  - Success rate: 0.0%\n",
            "\n",
            "============================================================\n",
            "âœ… Phase 3 Testing Complete\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate summary report\n",
        "summary = {\n",
        "    'test_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'phase': 'Phase 3',\n",
        "    'features_tested': [\n",
        "        'Deterministic Action Selection',\n",
        "        'Performance Tuning',\n",
        "        'Model Optimization',\n",
        "        'Integration Test'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 3 TEST SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTest Date: {summary['test_date']}\")\n",
        "print(f\"\\nFeatures Tested:\")\n",
        "for i, feature in enumerate(summary['features_tested'], 1):\n",
        "    print(f\"  {i}. {feature}\")\n",
        "\n",
        "if df_deterministic is not None:\n",
        "    print(f\"\\nâœ“ Deterministic Action Selection: Tested {len(df_deterministic)} scenarios\")\n",
        "    det_consistent = df_deterministic[df_deterministic['test'] == 'Deterministic Consistency']['deterministic_consistent'].values[0]\n",
        "    print(f\"  - Deterministic consistency: {det_consistent}\")\n",
        "\n",
        "if performance_results:\n",
        "    print(f\"\\nâœ“ Performance Tuning:\")\n",
        "    print(f\"  - Double DQN: {performance_results.get('double_dqn', {}).get('enabled', False)}\")\n",
        "    print(f\"  - Prioritized Replay: {performance_results.get('prioritized_replay', {}).get('enabled', False)}\")\n",
        "    print(f\"  - Gradient Clipping: {performance_results.get('gradient_clipping', {}).get('clip_value', 0)}\")\n",
        "    print(f\"  - Soft Target Updates: {performance_results.get('soft_target_updates', {}).get('enabled', False)}\")\n",
        "\n",
        "if model_results:\n",
        "    print(f\"\\nâœ“ Model Optimization:\")\n",
        "    print(f\"  - Architecture: {model_results.get('architecture', {}).get('type', 'Unknown')}\")\n",
        "    print(f\"  - LayerNorm: {model_results.get('layer_norm', {}).get('enabled', False)}\")\n",
        "    print(f\"  - Dropout: {model_results.get('dropout', {}).get('enabled', False)}\")\n",
        "    print(f\"  - Activation: {model_results.get('activation', {}).get('type', 'Unknown')}\")\n",
        "    print(f\"  - Model size: {model_results.get('model_size', {}).get('size_mb', 0):.2f} MB\")\n",
        "\n",
        "if integration_results is not None:\n",
        "    print(f\"\\nâœ“ Integration Test: Completed {len(integration_results)} episodes\")\n",
        "    print(f\"  - Average steps: {integration_results['steps'].mean():.1f}\")\n",
        "    print(f\"  - Success rate: {(integration_results['success'].sum() / len(integration_results)) * 100:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… Phase 3 Testing Complete\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

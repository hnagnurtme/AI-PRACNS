# RL Optimization Blueprint: Ti·ªám C·∫≠n Dijkstra Performance

## üìã T·ªïng Quan

Blueprint n√†y m√¥ t·∫£ chi·∫øn l∆∞·ª£c to√†n di·ªán ƒë·ªÉ t·ªëi ∆∞u RL routing agent, ti·ªám c·∫≠n performance c·ªßa Dijkstra algorithm. M·ª•c ti√™u l√† ƒë·∫°t ƒë∆∞·ª£c:
- **Success rate**: ‚â• 95% (Dijkstra: ~100%)
- **Path quality**: Trung b√¨nh ‚â§ 1.2x Dijkstra (hops, latency, distance)
- **Stability**: Deterministic v√† reliable nh∆∞ Dijkstra
- **Production-ready**: C√≥ th·ªÉ deploy v·ªõi confidence cao

---

## üéØ M·ª•c Ti√™u V√† Metrics

### Primary Metrics

| Metric | Dijkstra (Baseline) | RL Current | RL Target | Priority |
|--------|-------------------|------------|-----------|----------|
| Success Rate | ~100% | ~60-80% | ‚â• 95% | üî¥ Critical |
| Avg Hops | Baseline | +20-30% | ‚â§ +10% | üî¥ Critical |
| Avg Latency | Baseline | +15-25% | ‚â§ +5% | üî¥ Critical |
| Avg Distance | Baseline | +10-20% | ‚â§ +5% | üü° High |
| QoS Compliance | ~95% | ~70% | ‚â• 90% | üî¥ Critical |
| Deterministic | ‚úÖ Yes | ‚ö†Ô∏è Partial | ‚úÖ Yes | üü° High |

### Secondary Metrics

- **Training time**: < 24 hours for 5000 episodes
- **Inference time**: < 100ms per path
- **Model size**: < 50MB
- **Memory usage**: < 2GB during training

---

## üîç Ph√¢n T√≠ch Hi·ªán Tr·∫°ng

### ‚úÖ ƒêi·ªÉm M·∫°nh Hi·ªán T·∫°i

1. **Architecture t·ªët**: DuelingDQN v·ªõi LayerNorm, Dropout, ELU activation
2. **Advanced training**: Curriculum Learning, Imitation Learning, Multi-objective
3. **Expert demonstrations**: C√≥ th·ªÉ h·ªçc t·ª´ Dijkstra paths
4. **State builder**: ƒê√£ c√≥ feature engineering c∆° b·∫£n
5. **Reward structure**: ƒê√£ c√≥ nhi·ªÅu components

### ‚ùå ƒêi·ªÉm Y·∫øu C·∫ßn C·∫£i Thi·ªán

1. **State representation**: Ch∆∞a capture ƒë·ªß th√¥ng tin nh∆∞ Dijkstra
2. **Reward engineering**: Ch∆∞a match v·ªõi Dijkstra's edge weights
3. **Max steps**: Gi·ªõi h·∫°n 6-8 steps qu√° nh·ªè
4. **Action selection**: Ch∆∞a deterministic ƒë·ªß
5. **Training data**: Ch∆∞a ƒë·ªß episodes v√† scenarios
6. **Validation**: Ch∆∞a c√≥ comprehensive testing

---

## üöÄ C√°c C·∫£i Ti·∫øn C·ª• Th·ªÉ

### 1. State Representation Enhancement

#### 1.1. Th√™m Dijkstra-like Features

**M·ª•c ti√™u**: State ph·∫£i capture ƒë·ªß th√¥ng tin ƒë·ªÉ RL c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c edge weights nh∆∞ Dijkstra.

**C√°c features c·∫ßn th√™m**:

```python
# Node features (hi·ªán t·∫°i: 12, c·∫ßn: 15-18)
- Distance to destination (normalized)
- Distance to source (normalized)
- Resource utilization (CPU, Memory, Bandwidth) - separate
- Packet loss rate
- Current packet count / capacity ratio
- Communication range
- Node type encoding (one-hot: GS, GEO, MEO, LEO)
- Is in Dijkstra shortest path? (binary)
- Dijkstra edge weight estimate (normalized)
- Distance to nearest ground station
- Number of connections
- Weather condition encoding
```

**Implementation**:

```python
# File: Backend/environment/state_builder.py

def _build_dijkstra_aware_features(
    self,
    node: Dict,
    source_terminal: Dict,
    dest_terminal: Dict,
    current_node: Optional[Dict],
    all_nodes: List[Dict]
) -> np.ndarray:
    """Th√™m features gi·ªëng Dijkstra"""
    features = []
    
    # Distance features
    node_pos = node.get('position')
    dest_pos = dest_terminal.get('position')
    source_pos = source_terminal.get('position')
    
    if node_pos and dest_pos:
        dist_to_dest = self._calculate_distance(node_pos, dest_pos)
        features.append(dist_to_dest / 1000000.0)  # Normalize to 0-1
    
    if node_pos and source_pos:
        dist_to_source = self._calculate_distance(node_pos, source_pos)
        features.append(dist_to_source / 1000000.0)
    
    # Resource features (separate)
    cpu_util = node.get('cpu', {}).get('utilization', 0) / 100.0
    mem_util = node.get('memory', {}).get('utilization', 0) / 100.0
    bw_util = node.get('bandwidth', {}).get('utilization', 0) / 100.0
    features.extend([cpu_util, mem_util, bw_util])
    
    # Dijkstra edge weight estimate
    if current_node:
        base_dist = self._calculate_distance(
            current_node.get('position'), node_pos
        ) / 1000.0  # km
        
        max_util = max(cpu_util, mem_util, bw_util) * 100
        if max_util >= 95:
            penalty = float('inf')  # Drop node
        elif max_util >= 80:
            excess = (max_util - 80) / 20.0
            penalty = base_dist * 3.0 * excess  # 3x multiplier
        else:
            penalty = 0.0
        
        dijkstra_weight = base_dist + penalty
        features.append(dijkstra_weight / 10000.0)  # Normalize
    
    # Node type encoding
    node_type = node.get('nodeType', '')
    type_encoding = [
        1.0 if node_type == 'GROUND_STATION' else 0.0,
        1.0 if node_type == 'GEO_SATELLITE' else 0.0,
        1.0 if node_type == 'MEO_SATELLITE' else 0.0,
        1.0 if node_type == 'LEO_SATELLITE' else 0.0,
    ]
    features.extend(type_encoding)
    
    return np.array(features, dtype=np.float32)
```

**Timeline**: Week 1-2

---

### 2. Reward Engineering: Match Dijkstra's Logic

#### 2.1. Reward Function Redesign

**M·ª•c ti√™u**: Reward ph·∫£i reflect ch√≠nh x√°c Dijkstra's edge weights v√† penalties.

**Current Issues**:
- Reward components kh√¥ng match v·ªõi Dijkstra's logic
- Penalties kh√¥ng ƒë·ªß m·∫°nh ƒë·ªÉ match drop_threshold (95%) v√† penalty_threshold (80%)

**New Reward Structure**:

```python
# File: Backend/environment/routing_env.py

def _calculate_dijkstra_aligned_reward(
    self,
    current_node: Dict,
    next_node: Dict,
    distance: float,
    dest_terminal: Dict
) -> float:
    """Calculate reward aligned v·ªõi Dijkstra's edge weights"""
    
    # Base reward = negative distance (gi·ªëng Dijkstra's base weight)
    base_reward = -distance / 1000.0  # Convert to km, negative
    
    # Resource penalty (match Dijkstra's logic)
    max_util = max(
        next_node.get('cpu', {}).get('utilization', 0),
        next_node.get('memory', {}).get('utilization', 0),
        next_node.get('bandwidth', {}).get('utilization', 0)
    )
    
    # Drop penalty (match drop_threshold = 95%)
    if max_util >= 95.0:
        return -1000.0  # Huge penalty, effectively drop
    
    # Penalty (match penalty_threshold = 80%, multiplier = 3.0x)
    if max_util >= 80.0:
        excess = (max_util - 80.0) / 20.0  # 0.0 to 1.0
        penalty = (distance / 1000.0) * 3.0 * excess
        base_reward -= penalty
    
    # Progress reward (closer to destination = better)
    current_to_dest = self._calculate_distance(
        current_node.get('position'),
        dest_terminal.get('position')
    )
    next_to_dest = self._calculate_distance(
        next_node.get('position'),
        dest_terminal.get('position')
    )
    
    progress = (current_to_dest - next_to_dest) / 1000.0  # km
    progress_reward = progress * 10.0  # Scale
    
    # Success reward (reached destination)
    if next_to_dest < 1000:  # Within 1km of destination
        return 200.0  # Large success reward
    
    return base_reward + progress_reward
```

**Timeline**: Week 2-3

---

### 3. Dynamic Max Steps

#### 3.1. Adaptive Max Steps

**M·ª•c ti√™u**: Kh√¥ng gi·ªõi h·∫°n c·ª©ng max_steps, nh∆∞ng c√≥ c∆° ch·∫ø ƒë·ªÉ tr√°nh infinite loops.

**Implementation**:

```python
# File: Backend/environment/routing_env.py

def __init__(self, ...):
    # Dynamic max steps based on network size
    base_max_steps = 8
    network_size = len(nodes)
    
    # Estimate max hops needed (similar to Dijkstra's worst case)
    # Dijkstra worst case: O(V) where V = number of nodes
    # But in practice, paths are much shorter
    estimated_max_hops = min(
        network_size // 2,  # Reasonable upper bound
        base_max_steps * 2  # At most 2x base
    )
    
    self.max_steps = max(base_max_steps, estimated_max_hops)
    self.adaptive_max_steps = True

def step(self, action):
    # ...
    
    # Check if making progress
    if self.step_count > 3:
        recent_progress = self._check_recent_progress()
        if not recent_progress:
            # No progress in last 3 steps, likely stuck
            return state, -50.0, False, True, {'reason': 'no_progress'}
    
    # ...
```

**Timeline**: Week 3

---

### 4. Enhanced Imitation Learning

#### 4.1. Improved Expert Demonstrations

**M·ª•c ti√™u**: RL h·ªçc t·ªët h∆°n t·ª´ Dijkstra paths.

**Improvements**:

1. **More demonstrations**: TƒÉng t·ª´ 50 ‚Üí 500+ demonstrations
2. **Diverse scenarios**: Cover nhi·ªÅu cases (near, far, congested, etc.)
3. **Weighted demonstrations**: Prioritize high-quality paths
4. **Online DAGGER**: Continuously update v·ªõi new expert paths

**Implementation**:

```python
# File: Backend/training/imitation_learning.py

def generate_comprehensive_demos(
    self,
    terminals: List[Dict],
    nodes: List[Dict],
    num_demos: int = 500
):
    """Generate diverse expert demonstrations"""
    
    # Stratified sampling:
    # - 30% near pairs (< 2000km)
    # - 30% medium pairs (2000-5000km)
    # - 20% far pairs (5000-10000km)
    # - 20% very far pairs (> 10000km)
    
    scenarios = []
    
    # Near pairs
    for _ in range(num_demos * 0.3):
        source, dest = self._sample_pair_by_distance(terminals, 0, 2000)
        scenarios.append((source, dest, 'near'))
    
    # ... similar for other ranges
    
    # Generate demos
    for source, dest, category in scenarios:
        demo = self.generate_expert_demonstration(
            source, dest, nodes, algorithm='dijkstra'
        )
        if demo:
            # Weight by path quality
            quality = self._calculate_path_quality(demo.path, nodes)
            demo.weight = quality
            self.add_demonstration(demo)
```

**Timeline**: Week 3-4

---

### 5. Training Improvements

#### 5.1. Extended Training

**M·ª•c ti√™u**: Train ƒë·ªß l√¢u ƒë·ªÉ model converge.

**Changes**:

- **Episodes**: TƒÉng t·ª´ 2000 ‚Üí 5000-10000
- **Curriculum**: ƒê·∫£m b·∫£o cover t·∫•t c·∫£ levels
- **Early stopping**: Ch·ªâ stop khi performance stable
- **Checkpointing**: Save th∆∞·ªùng xuy√™n h∆°n

**Implementation**:

```python
# File: Backend/training/trainer.py

def train(self, ...):
    # Extended training
    max_episodes = 5000  # Increased from 2000
    
    # More frequent evaluation
    eval_frequency = 25  # Every 25 episodes instead of 50
    
    # Stricter early stopping
    early_stopping_patience = 100  # Increased from 50
    
    # ...
```

**Timeline**: Week 4-5

---

### 6. Deterministic Action Selection

#### 6.1. Improved Deterministic Mode

**M·ª•c ti√™u**: Khi `deterministic=True`, action selection ph·∫£i 100% deterministic v√† optimal.

**Implementation**:

```python
# File: Backend/agent/dueling_dqn.py

def select_action(
    self,
    state: np.ndarray,
    deterministic: bool = False,
    action_mask: Optional[np.ndarray] = None,
    temperature: float = 1.0
) -> int:
    """Improved deterministic selection"""
    
    if deterministic:
        # 100% deterministic: always select best action
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            
            # Apply action mask
            if action_mask is not None:
                mask_tensor = torch.FloatTensor(action_mask).to(self.device)
                q_values = q_values + (1 - mask_tensor) * -1e9
            
            # Select best action (deterministic)
            action = q_values.argmax().item()
            
            # Additional validation: check if Q-value is reasonable
            max_q = q_values.max().item()
            if max_q < -100:  # All actions are very bad
                logger.warning(f"All Q-values are very low: {max_q:.2f}")
            
            return action
    else:
        # Exploration mode (existing logic)
        # ...
```

**Timeline**: Week 5

---

### 7. Comprehensive Validation

#### 7.1. Validation Framework

**M·ª•c ti√™u**: Test RL tr√™n nhi·ªÅu scenarios v√† so s√°nh v·ªõi Dijkstra.

**Test Scenarios**:

1. **Normal scenarios**: Random terminal pairs
2. **Stress scenarios**: High congestion, many nodes
3. **Edge cases**: Very far pairs, isolated nodes
4. **QoS scenarios**: Strict QoS requirements

**Implementation**:

```python
# File: Backend/training/validation.py (NEW)

class RLValidator:
    """Comprehensive validation framework"""
    
    def validate_against_dijkstra(
        self,
        agent: DuelingDQNAgent,
        nodes: List[Dict],
        terminals: List[Dict],
        num_tests: int = 100
    ) -> Dict:
        """Compare RL vs Dijkstra"""
        
        results = {
            'rl_success': 0,
            'dijkstra_success': 0,
            'rl_better': 0,
            'dijkstra_better': 0,
            'equal': 0,
            'rl_hops': [],
            'dijkstra_hops': [],
            'rl_latency': [],
            'dijkstra_latency': [],
        }
        
        for _ in range(num_tests):
            # Random pair
            source, dest = random.sample(terminals, 2)
            
            # RL path
            rl_path = self._calculate_rl_path(agent, source, dest, nodes)
            
            # Dijkstra path
            dijkstra_path = calculate_path_dijkstra(source, dest, nodes)
            
            # Compare
            if rl_path['success'] and dijkstra_path['success']:
                rl_hops = rl_path['hops']
                dij_hops = dijkstra_path['hops']
                
                if rl_hops < dij_hops:
                    results['rl_better'] += 1
                elif dij_hops < rl_hops:
                    results['dijkstra_better'] += 1
                else:
                    results['equal'] += 1
                
                results['rl_hops'].append(rl_hops)
                results['dijkstra_hops'].append(dij_hops)
        
        return results
```

**Timeline**: Week 6

---

## üìÖ Implementation Roadmap

### Phase 1: Foundation (Weeks 1-2)
- ‚úÖ State representation enhancement
- ‚úÖ Reward engineering redesign
- ‚úÖ Basic validation framework

### Phase 2: Training (Weeks 3-4)
- ‚úÖ Dynamic max steps
- ‚úÖ Enhanced imitation learning
- ‚úÖ Extended training configuration

### Phase 3: Optimization (Week 5)
- ‚úÖ Deterministic action selection
- ‚úÖ Performance tuning
- ‚úÖ Model optimization

### Phase 4: Validation (Week 6)
- ‚úÖ Comprehensive testing
- ‚úÖ Performance comparison
- ‚úÖ Documentation

### Phase 5: Production (Week 7+)
- ‚úÖ Final validation
- ‚úÖ Deployment preparation
- ‚úÖ Monitoring setup

---

## ‚úÖ Success Criteria

### Must Have (Blocking)

1. **Success Rate**: ‚â• 95% (vs Dijkstra's ~100%)
2. **Path Quality**: Avg hops ‚â§ 1.1x Dijkstra
3. **Latency**: Avg latency ‚â§ 1.05x Dijkstra
4. **QoS Compliance**: ‚â• 90%
5. **Deterministic**: 100% deterministic khi `deterministic=True`

### Should Have (Important)

1. **Distance**: Avg distance ‚â§ 1.05x Dijkstra
2. **Training Time**: < 24 hours for 5000 episodes
3. **Inference Time**: < 100ms per path
4. **Stability**: Consistent performance across scenarios

### Nice to Have (Optional)

1. **Better than Dijkstra**: RL t·ªët h∆°n Dijkstra trong m·ªôt s·ªë edge cases
2. **Adaptive**: RL adapt t·ªët v·ªõi network changes
3. **Multi-objective**: RL optimize t·ªët multiple objectives

---

## üîß Technical Details

### File Structure

```
Backend/
‚îú‚îÄ‚îÄ environment/
‚îÇ   ‚îú‚îÄ‚îÄ state_builder.py          # Enhanced state representation
‚îÇ   ‚îî‚îÄ‚îÄ routing_env.py            # Improved reward function
‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îî‚îÄ‚îÄ dueling_dqn.py             # Deterministic action selection
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py                 # Extended training
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_trainer.py        # Enhanced features
‚îÇ   ‚îú‚îÄ‚îÄ imitation_learning.py      # Improved demonstrations
‚îÇ   ‚îî‚îÄ‚îÄ validation.py              # NEW: Validation framework
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ RL_LIMITATIONS.md          # Current limitations
    ‚îî‚îÄ‚îÄ RL_OPTIMIZATION_BLUEPRINT.md  # This file
```

### Configuration Changes

```yaml
# config.dev.yaml additions

training:
  max_episodes: 5000  # Increased from 2000
  eval_frequency: 25  # More frequent
  early_stopping_patience: 100  # Stricter

rl_agent:
  dqn:
    exploration_decay: 0.9995  # Slower decay
    learning_rate: 0.0001  # Keep stable
    batch_size: 128  # Larger batch

state_builder:
  max_nodes: 81  # Match actual DB size
  node_feature_dim: 18  # Increased from 12
  include_dijkstra_features: true  # NEW

reward:
  dijkstra_aligned: true  # NEW: Use Dijkstra-aligned rewards
  drop_threshold: 95.0
  penalty_threshold: 80.0
  penalty_multiplier: 3.0

imitation_learning:
  num_demos: 500  # Increased from 50
  use_dagger: true
  expert_ratio: 0.3
```

---

## üìä Monitoring & Metrics

### Training Metrics

- Episode reward (mean, std)
- Success rate
- Average hops
- Average latency
- Q-value statistics
- Loss trends
- Epsilon decay

### Validation Metrics

- RL vs Dijkstra comparison
- Success rate by scenario type
- Path quality distribution
- QoS compliance rate
- Inference time

### Production Metrics

- Request success rate
- Average response time
- Path quality (hops, latency)
- Error rate
- Model performance drift

---

## üö® Risks & Mitigations

### Risk 1: Training Time Too Long
- **Mitigation**: Use distributed training, GPU acceleration, checkpointing

### Risk 2: Model Overfitting
- **Mitigation**: Regularization, validation on held-out data, early stopping

### Risk 3: State Dimension Too Large
- **Mitigation**: Feature selection, dimensionality reduction, efficient encoding

### Risk 4: Reward Function Too Complex
- **Mitigation**: Start simple, gradually add complexity, validate each change

### Risk 5: Not Reaching Target Performance
- **Mitigation**: Iterative improvement, fallback to Dijkstra, hybrid approach

---

## üìù Notes

- **Incremental approach**: Implement changes incrementally, validate each step
- **Baseline comparison**: Always compare v·ªõi Dijkstra baseline
- **Version control**: Track model versions v√† performance
- **Documentation**: Document all changes v√† rationale
- **Testing**: Comprehensive testing before production deployment

---

## üîó References

- [RL_LIMITATIONS.md](./RL_LIMITATIONS.md) - Current limitations analysis
- Dijkstra implementation: `Backend/api/routing_bp.py::calculate_path_dijkstra()`
- RL implementation: `Backend/services/rl_routing_service.py`
- Training scripts: `Backend/training/train.py`

---

**Last Updated**: 2024-12-20  
**Status**: üü° In Progress  
**Next Review**: After Phase 1 completion

